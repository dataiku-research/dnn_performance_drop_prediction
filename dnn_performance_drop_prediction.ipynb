{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Performance Drop Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose** Train a DNN-based meta-model to predict a primary model accuracy drop (on various shifted datasets) and beat the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Primary Task** RandomForestClassifier to predict low/high sales of video games records. Accuracy on clean validation set 0.798."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data for the Performance Drop Regressor**\n",
    "- training: 500 datasets (X1), their accuracy drop (y), their meta-features (X2)\n",
    "- validation: take a random split of the previous, if needed.\n",
    "- test data:\n",
    "   1. test: 500 datasets (X1) with same shifts as in the training, but different severity (and their X2 and y).\n",
    "   2. test_unseen: 900 datasets (X1) with other types of shifts, not seen at training time (and their X2 and y).\n",
    "   3. test_natural: 10 datasets (X1) coming from different domains, but same primary task (and their X2 and y).\n",
    "   \n",
    "Each dataset has 475 rows and 9 features (preprocessed already).\n",
    "\n",
    "Each meta-feature vector contains 114 features (will be preprocessed in this notebook to 110 final features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline**\n",
    "\n",
    "Baseline-Meta-Features: RandomForestRegressor trained on meta features only (prediction_percentiles, PAD, RCA, confidence drop, BBSDs KS and BBSDh X2 statistics, KS statistics on individual preprocessed features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/smaggio/workspace/dnn_performance_drop_prediction_under_drift/env_dnn_performance_drop_prediction/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "tf.experimental.output_all_intermediates(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here import of the class I'm using to train DNNs in keras, where you can choose among 3 types of encoders 'mlp', 'lstm' or 'odt' (oblivious decision tree). \n",
    "\n",
    "Have a look at `_make_model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, Layer, Concatenate\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model as KerasModel\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.random import set_seed as set_random_seed\n",
    "from node.networks.layer import ObliviousDecisionTree\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "class RandomSamplePermutation(Layer):\n",
    "    def __init__(self, n_samples=500, **kwargs):\n",
    "        super(RandomSamplePermutation, self).__init__(**kwargs)\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def call(self, datasets, training=None):\n",
    "        if not training:\n",
    "            return datasets\n",
    "\n",
    "        permuted_indices = np.random.permutation(self.n_samples)\n",
    "\n",
    "        return datasets[:, permuted_indices, :]\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"n_samples\": self.n_samples,\n",
    "                \"name\": 'rnd_permutation'}\n",
    "\n",
    "\n",
    "class VectorizedODT(Layer):\n",
    "    def __init__(self, n_trees=1,\n",
    "                 depth=1,\n",
    "                 units=100,\n",
    "                 threshold_init_beta=1.,\n",
    "                 name='vec_odt',\n",
    "                 **kwargs):\n",
    "        super(VectorizedODT, self).__init__(name=name, **kwargs)\n",
    "        self.odt_layer = ObliviousDecisionTree(n_trees=n_trees,\n",
    "                                               depth=depth,\n",
    "                                               units=units,\n",
    "                                               threshold_init_beta=threshold_init_beta)\n",
    "\n",
    "    def call(self, input_numeric, training=None):\n",
    "        dataset_encoder = tf.vectorized_map(self.odt_layer, input_numeric)\n",
    "\n",
    "        return dataset_encoder\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"n_trees\": self.odt_layer.n_trees,\n",
    "                \"depth\": self.odt_layer.depth,\n",
    "                \"units\": self.odt_layer.units,\n",
    "                \"threshold_init_beta\": self.odt_layer.threshold_init_beta,\n",
    "                \"name\": self.name}\n",
    "\n",
    "\n",
    "class MultiDomainPerformancePredictorDNN():\n",
    "    def __init__(self, n_samples, n_features, encoded_ds_size=10, hidden_size=5,\n",
    "                 encoder_type='mlp', n_trees=3, depth=3, threshold_init_beta=1., lr=0.01):\n",
    "        self.n_features = n_features\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "        self.n_samples_per_dataset = n_samples\n",
    "        self.n_features = n_features\n",
    "        self.encoded_ds_size = encoded_ds_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.n_trees = n_trees\n",
    "        self.depth = depth\n",
    "        self.threshold_init_beta = threshold_init_beta\n",
    "\n",
    "        self.history = None\n",
    "        self.lr = lr\n",
    "\n",
    "        valid_encoder_types = ['mlp', 'lstm', 'odt']\n",
    "        if encoder_type not in valid_encoder_types:\n",
    "            raise ValueError('Encoder type must be one of: %s.' % str(valid_encoder_types))\n",
    "        self.encoder_type = encoder_type\n",
    "\n",
    "        self.model = self._make_model()\n",
    "        self._compile(self.lr)\n",
    "\n",
    "    def _make_encoder(self, input_numeric):\n",
    "\n",
    "        input_shape = (self.n_samples_per_dataset, self.n_features)\n",
    "\n",
    "        if self.encoder_type == 'lstm':\n",
    "            dataset_encoder = Bidirectional(\n",
    "                LSTM(units=self.encoded_ds_size, return_sequences=False,\n",
    "                     input_shape=input_shape), name='encoded_dataset')(input_numeric)\n",
    "\n",
    "        elif self.encoder_type == 'mlp':\n",
    "\n",
    "            dataset_encoder = Dense(units=self.encoded_ds_size, kernel_initializer='normal',\n",
    "                                    activation='relu', name='ds_dense')(input_numeric)\n",
    "            dataset_encoder = GlobalAveragePooling1D()(dataset_encoder)\n",
    "            dataset_encoder = Dense(self.hidden_size, kernel_initializer='normal',\n",
    "                                    activation='relu', name='ds_avg_dense')(dataset_encoder)\n",
    "\n",
    "        elif self.encoder_type == 'odt':\n",
    "\n",
    "            dataset_encoder = VectorizedODT(n_trees=self.n_trees, depth=self.depth, units=self.encoded_ds_size,\n",
    "                                            threshold_init_beta=self.threshold_init_beta)(input_numeric)\n",
    "            dataset_encoder = GlobalAveragePooling1D()(dataset_encoder)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Only lstm, mlp and odt are supported.')\n",
    "\n",
    "        return dataset_encoder\n",
    "\n",
    "    def _make_branches(self, encoded_data):\n",
    "\n",
    "        # single branch for drop prediction\n",
    "        encoded_data = Dropout(0.2, name='perf_drop_dropout')(encoded_data)\n",
    "        performance_drop = Dense(1, kernel_initializer='normal', name='perf_drop')(encoded_data)\n",
    "        output_model = performance_drop\n",
    "\n",
    "        return output_model\n",
    "\n",
    "    def _compile(self, lr=0.01):\n",
    "        self.model.compile(\n",
    "            loss='mean_absolute_error',\n",
    "            optimizer=Adam(lr),\n",
    "            metrics=['mean_absolute_error']\n",
    "        )\n",
    "        \n",
    "    def _make_model(self) -> KerasModel:\n",
    "        \"\"\"\n",
    "        This method is used to generate a Keras Model containing the DNN performance predictor\n",
    "        :return: a compiled Keras Model object\n",
    "        \"\"\"\n",
    "\n",
    "        input_numeric = Input(shape=(self.n_samples_per_dataset, self.n_features), name='dataset')\n",
    "\n",
    "        input_numeric_permuted = RandomSamplePermutation(n_samples=self.n_samples_per_dataset, name='permuted_dataset')(\n",
    "            input_numeric)\n",
    "\n",
    "        encoded_data = self._make_encoder(input_numeric_permuted)\n",
    "\n",
    "        output_model = self._make_branches(encoded_data)\n",
    "\n",
    "        model = KerasModel(inputs=input_numeric, outputs=output_model)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _step_decay(self, epoch):\n",
    "        initial_lrate = self.lr\n",
    "        drop = 0.5\n",
    "        epochs_drop = 10.0\n",
    "        lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "        return lrate\n",
    "\n",
    "    def _fixed_lr(self, epoch):\n",
    "        return self.lr\n",
    "\n",
    "    def fit(self, list_X, list_y_drop, random_state=1234, epochs=100, batch_size=256,\n",
    "            validation_split=0.2, verbose=0, early_stop_patience=10, lr=None,\n",
    "            reduce_lr_plateau=True, validation_data=None):\n",
    "\n",
    "        np.random.seed(random_state)\n",
    "        set_random_seed(random_state)\n",
    "\n",
    "        if os.path.exists('./mdc_net.h5'):\n",
    "            os.remove('./mdc_net.h5')\n",
    "\n",
    "        if lr is not None:\n",
    "            self.lr = lr\n",
    "            self._compile(self.lr)\n",
    "\n",
    "        if validation_split != 0 or (validation_data is not None):\n",
    "            callbacks = [\n",
    "                ModelCheckpoint(filepath='./mdc_net.h5', monitor='val_loss', verbose=verbose,\n",
    "                                save_best_only=True, mode='min'),\n",
    "                EarlyStopping(monitor='val_loss', mode='min', verbose=verbose, patience=early_stop_patience)]\n",
    "\n",
    "            if reduce_lr_plateau:\n",
    "                callbacks += [\n",
    "                    ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=5, min_lr=self.lr * 0.01,\n",
    "                                      verbose=verbose)]\n",
    "            else:\n",
    "                callbacks += [LearningRateScheduler(self._fixed_lr)]\n",
    "        else:\n",
    "            callbacks = [LearningRateScheduler(self._fixed_lr)]\n",
    "\n",
    "        input_X = list_X\n",
    "\n",
    "        input_y = list_y_drop\n",
    "\n",
    "        self.history = self.model.fit(\n",
    "            input_X,\n",
    "            input_y,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            validation_data=validation_data,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        if validation_split != 0 or (validation_data is not None):\n",
    "            self.model = load_model('./mdc_net.h5', custom_objects={'RandomSamplePermutation': RandomSamplePermutation,\n",
    "                                                                    'VectorizedODT': VectorizedODT})\n",
    "\n",
    "    def predict(self, list_X):\n",
    "\n",
    "        drop_pred = self.model.predict(list_X)\n",
    "\n",
    "        return np.clip(drop_pred, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data and some simple preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fld = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(fld, 'data.pkl'), 'rb') as f:\n",
    "    out = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, test_unseen, test_natural, ref_task, result_df = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = train.datasets\n",
    "y_train = train.drops\n",
    "train_meta_features_orig = train.meta_features\n",
    "train_drift_types = train.drift_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_1 = test.datasets\n",
    "y_test = test.drops\n",
    "test_meta_features_orig = test.meta_features\n",
    "test_drift_types = test.drift_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_unseen_1 = test_unseen.datasets\n",
    "y_test_unseen = test_unseen.drops\n",
    "test_unseen_meta_features_orig = test_unseen.meta_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_natural_1 = test_natural.datasets\n",
    "y_test_natural = test_natural.drops\n",
    "test_natural_meta_features_orig = test_natural.meta_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 114)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_natural_meta_features_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = Pipeline([('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "                    ('scaler', MinMaxScaler())])\n",
    "\n",
    "X_train_2 = imp.fit_transform(train_meta_features_orig)\n",
    "X_test_2 = imp.transform(test_meta_features_orig)\n",
    "X_test_unseen_2 = imp.transform(test_unseen_meta_features_orig)\n",
    "X_test_natural_2 = imp.transform(test_natural_meta_features_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 475, 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 110)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 475, 9)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 475, 9)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_unseen_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 475, 9)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_natural_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the reference task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ref_task.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7978947368421052"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source validation dataset\n",
    "X_src = ref_task.X_orig\n",
    "y_src = ref_task.y\n",
    "\n",
    "# reference accuracy\n",
    "model.score(ref_task.preprocess.transform(X_src), y_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can extract primary predictions for all datasets, and possibly use them as additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 475, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pred = np.array([model.predict_proba(X) for X in X_train_1])\n",
    "X_train_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 475, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_pred = np.array([model.predict_proba(X) for X in X_test_1])\n",
    "X_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 475, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_unseen_pred = np.array([model.predict_proba(X) for X in X_test_unseen_1])\n",
    "X_test_unseen_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 475, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_natural_pred = np.array([model.predict_proba(X) for X in X_test_natural_1])\n",
    "X_test_natural_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Random Forest Regresson on Meta-Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Drift Features Baseline Performance Drop Predictor...\n",
      "Evaluate Drift Features Baseline and save results...\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Drift Features Baseline Performance Drop Predictor...\")\n",
    "\n",
    "regr = RandomForestRegressor().fit(X_train_2, y_train)\n",
    "\n",
    "print(\"Evaluate Drift Features Baseline and save results...\")\n",
    "\n",
    "y_train_drops_pred = regr.predict(X_train_2)\n",
    "\n",
    "y_test_drops_pred = regr.predict(X_test_2)\n",
    "\n",
    "y_test_unseen_drops_pred = regr.predict(X_test_unseen_2)\n",
    "\n",
    "y_test_natural_drops_pred = regr.predict(X_test_natural_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015181221052631578"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_train, y_train_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046871199999999995"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_test_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.062211532163742686"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test_unseen, y_test_unseen_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05895368421052629"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test_natural, y_test_natural_drops_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiDomainPredictor on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Baseline Performance Drop Predictor...\n",
      "Evaluate baseline and save results...\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Baseline Performance Drop Predictor...\")\n",
    "\n",
    "baseline_drop_predictor = MultiDomainPerformancePredictor(n_features=X_train_1.shape[2],\n",
    "                                                          n_domains=len(np.unique(train_drift_types)),\n",
    "                                                          multi_task=True)\n",
    "\n",
    "baseline_drop_predictor.fit(X_train_1, y_train, train_drift_types)\n",
    "\n",
    "print(\"Evaluate baseline and save results...\")\n",
    "\n",
    "y_train_drops_pred, _ = baseline_drop_predictor.predict(X_train_1)\n",
    "\n",
    "y_test_drops_pred, _ = baseline_drop_predictor.predict(X_test_1)\n",
    "\n",
    "y_test_unseen_drops_pred, _ = baseline_drop_predictor.predict(X_test_unseen_1)\n",
    "\n",
    "y_test_natural_drops_pred, _ = baseline_drop_predictor.predict(X_test_natural_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030972423737471784"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_train, y_train_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022175162336610748"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_test_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.049388417864048254"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test_unseen, y_test_unseen_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05432391256617969"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test_natural, y_test_natural_drops_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training DNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using the MultiDomainPerformancePredictorDNN, do not change the default multi_task, drop_branch, n_domains: with these defaults the DNN is a simple DNN regressors.\n",
    "\n",
    "Instead play with the network (mlp, lstm, odt), the sizes of the various layer, regularizations, augmentations.\n",
    "\n",
    "Make sure on multiple runs of training, that results are stable.\n",
    "\n",
    "Can you make the **training error** go lower than 4% as a start ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using datasets only, with no meta-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define DNN Performance Drop Predictor...\n"
     ]
    }
   ],
   "source": [
    "print(\"Define DNN Performance Drop Predictor...\")\n",
    "\n",
    "drop_predictor = MultiDomainPerformancePredictorDNN(n_samples=X_train_1.shape[1],\n",
    "                                                    n_features=X_train_1.shape[2],\n",
    "                                                    encoded_ds_size=10, \n",
    "                                                    hidden_size=5,  \n",
    "                                                    encoder_type='lstm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dataset (InputLayer)         [(None, 475, 9)]          0         \n",
      "_________________________________________________________________\n",
      "permuted_dataset (RandomSamp (None, 475, 9)            0         \n",
      "_________________________________________________________________\n",
      "encoded_dataset (Bidirection (None, 20)                1600      \n",
      "_________________________________________________________________\n",
      "perf_drop_dropout (Dropout)  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "perf_drop (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,621\n",
      "Trainable params: 1,621\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "drop_predictor.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DNN Performance Drop Predictor...\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.1268 - mean_absolute_error: 0.1268\n",
      "Epoch 00001: val_loss improved from inf to 0.10812, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 1s 2ms/sample - loss: 0.1268 - mean_absolute_error: 0.1268 - val_loss: 0.1081 - val_mean_absolute_error: 0.1081 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.1176 - mean_absolute_error: 0.1176\n",
      "Epoch 00002: val_loss improved from 0.10812 to 0.10250, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 574us/sample - loss: 0.1176 - mean_absolute_error: 0.1176 - val_loss: 0.1025 - val_mean_absolute_error: 0.1025 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.1123 - mean_absolute_error: 0.1123\n",
      "Epoch 00003: val_loss improved from 0.10250 to 0.09730, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 556us/sample - loss: 0.1123 - mean_absolute_error: 0.1123 - val_loss: 0.0973 - val_mean_absolute_error: 0.0973 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.1056 - mean_absolute_error: 0.1056\n",
      "Epoch 00004: val_loss improved from 0.09730 to 0.09225, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 568us/sample - loss: 0.1056 - mean_absolute_error: 0.1056 - val_loss: 0.0922 - val_mean_absolute_error: 0.0922 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.1025 - mean_absolute_error: 0.1025\n",
      "Epoch 00005: val_loss improved from 0.09225 to 0.08776, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 547us/sample - loss: 0.1025 - mean_absolute_error: 0.1025 - val_loss: 0.0878 - val_mean_absolute_error: 0.0878 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0925 - mean_absolute_error: 0.0925\n",
      "Epoch 00006: val_loss improved from 0.08776 to 0.08414, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 553us/sample - loss: 0.0925 - mean_absolute_error: 0.0925 - val_loss: 0.0841 - val_mean_absolute_error: 0.0841 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0892 - mean_absolute_error: 0.0892\n",
      "Epoch 00007: val_loss improved from 0.08414 to 0.08046, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 558us/sample - loss: 0.0892 - mean_absolute_error: 0.0892 - val_loss: 0.0805 - val_mean_absolute_error: 0.0805 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0881 - mean_absolute_error: 0.0881\n",
      "Epoch 00008: val_loss improved from 0.08046 to 0.07729, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 575us/sample - loss: 0.0881 - mean_absolute_error: 0.0881 - val_loss: 0.0773 - val_mean_absolute_error: 0.0773 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0831 - mean_absolute_error: 0.0831\n",
      "Epoch 00009: val_loss improved from 0.07729 to 0.07400, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 579us/sample - loss: 0.0831 - mean_absolute_error: 0.0831 - val_loss: 0.0740 - val_mean_absolute_error: 0.0740 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0796 - mean_absolute_error: 0.0796\n",
      "Epoch 00010: val_loss improved from 0.07400 to 0.07088, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 602us/sample - loss: 0.0796 - mean_absolute_error: 0.0796 - val_loss: 0.0709 - val_mean_absolute_error: 0.0709 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0794 - mean_absolute_error: 0.0794\n",
      "Epoch 00011: val_loss improved from 0.07088 to 0.06816, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 590us/sample - loss: 0.0794 - mean_absolute_error: 0.0794 - val_loss: 0.0682 - val_mean_absolute_error: 0.0682 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0748 - mean_absolute_error: 0.0748\n",
      "Epoch 00012: val_loss improved from 0.06816 to 0.06588, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 577us/sample - loss: 0.0748 - mean_absolute_error: 0.0748 - val_loss: 0.0659 - val_mean_absolute_error: 0.0659 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0741 - mean_absolute_error: 0.0741\n",
      "Epoch 00013: val_loss improved from 0.06588 to 0.06444, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 598us/sample - loss: 0.0741 - mean_absolute_error: 0.0741 - val_loss: 0.0644 - val_mean_absolute_error: 0.0644 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0698 - mean_absolute_error: 0.0698\n",
      "Epoch 00014: val_loss improved from 0.06444 to 0.06315, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 630us/sample - loss: 0.0698 - mean_absolute_error: 0.0698 - val_loss: 0.0632 - val_mean_absolute_error: 0.0632 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0683 - mean_absolute_error: 0.0683\n",
      "Epoch 00015: val_loss improved from 0.06315 to 0.06219, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 627us/sample - loss: 0.0683 - mean_absolute_error: 0.0683 - val_loss: 0.0622 - val_mean_absolute_error: 0.0622 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0687 - mean_absolute_error: 0.0687\n",
      "Epoch 00016: val_loss improved from 0.06219 to 0.06163, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 701us/sample - loss: 0.0687 - mean_absolute_error: 0.0687 - val_loss: 0.0616 - val_mean_absolute_error: 0.0616 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0681 - mean_absolute_error: 0.0681\n",
      "Epoch 00017: val_loss improved from 0.06163 to 0.06105, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 758us/sample - loss: 0.0681 - mean_absolute_error: 0.0681 - val_loss: 0.0610 - val_mean_absolute_error: 0.0610 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0672 - mean_absolute_error: 0.0672\n",
      "Epoch 00018: val_loss improved from 0.06105 to 0.06052, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 846us/sample - loss: 0.0672 - mean_absolute_error: 0.0672 - val_loss: 0.0605 - val_mean_absolute_error: 0.0605 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0662 - mean_absolute_error: 0.0662\n",
      "Epoch 00019: val_loss improved from 0.06052 to 0.06031, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 719us/sample - loss: 0.0662 - mean_absolute_error: 0.0662 - val_loss: 0.0603 - val_mean_absolute_error: 0.0603 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0642 - mean_absolute_error: 0.0642\n",
      "Epoch 00020: val_loss improved from 0.06031 to 0.06014, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 711us/sample - loss: 0.0642 - mean_absolute_error: 0.0642 - val_loss: 0.0601 - val_mean_absolute_error: 0.0601 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0669 - mean_absolute_error: 0.0669\n",
      "Epoch 00021: val_loss improved from 0.06014 to 0.05993, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 714us/sample - loss: 0.0669 - mean_absolute_error: 0.0669 - val_loss: 0.0599 - val_mean_absolute_error: 0.0599 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0658 - mean_absolute_error: 0.0658\n",
      "Epoch 00022: val_loss improved from 0.05993 to 0.05966, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 733us/sample - loss: 0.0658 - mean_absolute_error: 0.0658 - val_loss: 0.0597 - val_mean_absolute_error: 0.0597 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0634 - mean_absolute_error: 0.0634\n",
      "Epoch 00023: val_loss improved from 0.05966 to 0.05928, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 669us/sample - loss: 0.0634 - mean_absolute_error: 0.0634 - val_loss: 0.0593 - val_mean_absolute_error: 0.0593 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0653 - mean_absolute_error: 0.0653\n",
      "Epoch 00024: val_loss improved from 0.05928 to 0.05880, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 703us/sample - loss: 0.0653 - mean_absolute_error: 0.0653 - val_loss: 0.0588 - val_mean_absolute_error: 0.0588 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0638 - mean_absolute_error: 0.0638\n",
      "Epoch 00025: val_loss improved from 0.05880 to 0.05824, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 673us/sample - loss: 0.0638 - mean_absolute_error: 0.0638 - val_loss: 0.0582 - val_mean_absolute_error: 0.0582 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0594 - mean_absolute_error: 0.0594\n",
      "Epoch 00026: val_loss improved from 0.05824 to 0.05760, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 637us/sample - loss: 0.0594 - mean_absolute_error: 0.0594 - val_loss: 0.0576 - val_mean_absolute_error: 0.0576 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0605 - mean_absolute_error: 0.0605\n",
      "Epoch 00027: val_loss improved from 0.05760 to 0.05699, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 634us/sample - loss: 0.0605 - mean_absolute_error: 0.0605 - val_loss: 0.0570 - val_mean_absolute_error: 0.0570 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0597 - mean_absolute_error: 0.0597\n",
      "Epoch 00028: val_loss improved from 0.05699 to 0.05631, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 647us/sample - loss: 0.0597 - mean_absolute_error: 0.0597 - val_loss: 0.0563 - val_mean_absolute_error: 0.0563 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0581 - mean_absolute_error: 0.0581\n",
      "Epoch 00029: val_loss improved from 0.05631 to 0.05566, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 685us/sample - loss: 0.0581 - mean_absolute_error: 0.0581 - val_loss: 0.0557 - val_mean_absolute_error: 0.0557 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0581 - mean_absolute_error: 0.0581\n",
      "Epoch 00030: val_loss improved from 0.05566 to 0.05499, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 668us/sample - loss: 0.0581 - mean_absolute_error: 0.0581 - val_loss: 0.0550 - val_mean_absolute_error: 0.0550 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0536 - mean_absolute_error: 0.0536\n",
      "Epoch 00031: val_loss improved from 0.05499 to 0.05429, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 647us/sample - loss: 0.0536 - mean_absolute_error: 0.0536 - val_loss: 0.0543 - val_mean_absolute_error: 0.0543 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0557 - mean_absolute_error: 0.0557\n",
      "Epoch 00032: val_loss improved from 0.05429 to 0.05359, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 642us/sample - loss: 0.0557 - mean_absolute_error: 0.0557 - val_loss: 0.0536 - val_mean_absolute_error: 0.0536 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0556 - mean_absolute_error: 0.0556\n",
      "Epoch 00033: val_loss improved from 0.05359 to 0.05304, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 618us/sample - loss: 0.0556 - mean_absolute_error: 0.0556 - val_loss: 0.0530 - val_mean_absolute_error: 0.0530 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0564 - mean_absolute_error: 0.0564\n",
      "Epoch 00034: val_loss improved from 0.05304 to 0.05257, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 602us/sample - loss: 0.0564 - mean_absolute_error: 0.0564 - val_loss: 0.0526 - val_mean_absolute_error: 0.0526 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0519 - mean_absolute_error: 0.0519\n",
      "Epoch 00035: val_loss improved from 0.05257 to 0.05217, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 632us/sample - loss: 0.0519 - mean_absolute_error: 0.0519 - val_loss: 0.0522 - val_mean_absolute_error: 0.0522 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0523 - mean_absolute_error: 0.0523\n",
      "Epoch 00036: val_loss improved from 0.05217 to 0.05176, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 650us/sample - loss: 0.0523 - mean_absolute_error: 0.0523 - val_loss: 0.0518 - val_mean_absolute_error: 0.0518 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0523 - mean_absolute_error: 0.0523\n",
      "Epoch 00037: val_loss improved from 0.05176 to 0.05138, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 651us/sample - loss: 0.0523 - mean_absolute_error: 0.0523 - val_loss: 0.0514 - val_mean_absolute_error: 0.0514 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0499 - mean_absolute_error: 0.0499\n",
      "Epoch 00038: val_loss improved from 0.05138 to 0.05106, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 656us/sample - loss: 0.0499 - mean_absolute_error: 0.0499 - val_loss: 0.0511 - val_mean_absolute_error: 0.0511 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0518 - mean_absolute_error: 0.0518\n",
      "Epoch 00039: val_loss improved from 0.05106 to 0.05076, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 653us/sample - loss: 0.0518 - mean_absolute_error: 0.0518 - val_loss: 0.0508 - val_mean_absolute_error: 0.0508 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0502 - mean_absolute_error: 0.0502\n",
      "Epoch 00040: val_loss improved from 0.05076 to 0.05048, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 618us/sample - loss: 0.0502 - mean_absolute_error: 0.0502 - val_loss: 0.0505 - val_mean_absolute_error: 0.0505 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0515 - mean_absolute_error: 0.0515\n",
      "Epoch 00041: val_loss improved from 0.05048 to 0.05019, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 648us/sample - loss: 0.0515 - mean_absolute_error: 0.0515 - val_loss: 0.0502 - val_mean_absolute_error: 0.0502 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0515 - mean_absolute_error: 0.0515\n",
      "Epoch 00042: val_loss improved from 0.05019 to 0.04989, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 608us/sample - loss: 0.0515 - mean_absolute_error: 0.0515 - val_loss: 0.0499 - val_mean_absolute_error: 0.0499 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0491 - mean_absolute_error: 0.0491\n",
      "Epoch 00043: val_loss improved from 0.04989 to 0.04959, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 642us/sample - loss: 0.0491 - mean_absolute_error: 0.0491 - val_loss: 0.0496 - val_mean_absolute_error: 0.0496 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0497 - mean_absolute_error: 0.0497\n",
      "Epoch 00044: val_loss improved from 0.04959 to 0.04931, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 653us/sample - loss: 0.0497 - mean_absolute_error: 0.0497 - val_loss: 0.0493 - val_mean_absolute_error: 0.0493 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0509 - mean_absolute_error: 0.0509\n",
      "Epoch 00045: val_loss improved from 0.04931 to 0.04902, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 656us/sample - loss: 0.0509 - mean_absolute_error: 0.0509 - val_loss: 0.0490 - val_mean_absolute_error: 0.0490 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0496 - mean_absolute_error: 0.0496\n",
      "Epoch 00046: val_loss improved from 0.04902 to 0.04875, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 675us/sample - loss: 0.0496 - mean_absolute_error: 0.0496 - val_loss: 0.0487 - val_mean_absolute_error: 0.0487 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0459 - mean_absolute_error: 0.0459\n",
      "Epoch 00047: val_loss improved from 0.04875 to 0.04849, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 647us/sample - loss: 0.0459 - mean_absolute_error: 0.0459 - val_loss: 0.0485 - val_mean_absolute_error: 0.0485 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0496 - mean_absolute_error: 0.0496\n",
      "Epoch 00048: val_loss improved from 0.04849 to 0.04823, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 645us/sample - loss: 0.0496 - mean_absolute_error: 0.0496 - val_loss: 0.0482 - val_mean_absolute_error: 0.0482 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0451 - mean_absolute_error: 0.0451\n",
      "Epoch 00049: val_loss improved from 0.04823 to 0.04797, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 651us/sample - loss: 0.0451 - mean_absolute_error: 0.0451 - val_loss: 0.0480 - val_mean_absolute_error: 0.0480 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0482 - mean_absolute_error: 0.0482\n",
      "Epoch 00050: val_loss improved from 0.04797 to 0.04772, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 631us/sample - loss: 0.0482 - mean_absolute_error: 0.0482 - val_loss: 0.0477 - val_mean_absolute_error: 0.0477 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0443 - mean_absolute_error: 0.0443\n",
      "Epoch 00051: val_loss improved from 0.04772 to 0.04747, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 647us/sample - loss: 0.0443 - mean_absolute_error: 0.0443 - val_loss: 0.0475 - val_mean_absolute_error: 0.0475 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0450 - mean_absolute_error: 0.0450\n",
      "Epoch 00052: val_loss improved from 0.04747 to 0.04718, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 690us/sample - loss: 0.0450 - mean_absolute_error: 0.0450 - val_loss: 0.0472 - val_mean_absolute_error: 0.0472 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0476 - mean_absolute_error: 0.0476\n",
      "Epoch 00053: val_loss improved from 0.04718 to 0.04688, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 651us/sample - loss: 0.0476 - mean_absolute_error: 0.0476 - val_loss: 0.0469 - val_mean_absolute_error: 0.0469 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0445 - mean_absolute_error: 0.0445\n",
      "Epoch 00054: val_loss improved from 0.04688 to 0.04658, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 630us/sample - loss: 0.0445 - mean_absolute_error: 0.0445 - val_loss: 0.0466 - val_mean_absolute_error: 0.0466 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0445 - mean_absolute_error: 0.0445\n",
      "Epoch 00055: val_loss improved from 0.04658 to 0.04627, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 677us/sample - loss: 0.0445 - mean_absolute_error: 0.0445 - val_loss: 0.0463 - val_mean_absolute_error: 0.0463 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0437 - mean_absolute_error: 0.0437\n",
      "Epoch 00056: val_loss improved from 0.04627 to 0.04596, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 719us/sample - loss: 0.0437 - mean_absolute_error: 0.0437 - val_loss: 0.0460 - val_mean_absolute_error: 0.0460 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0436 - mean_absolute_error: 0.0436\n",
      "Epoch 00057: val_loss improved from 0.04596 to 0.04563, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 696us/sample - loss: 0.0436 - mean_absolute_error: 0.0436 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0433 - mean_absolute_error: 0.0433\n",
      "Epoch 00058: val_loss improved from 0.04563 to 0.04531, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 678us/sample - loss: 0.0433 - mean_absolute_error: 0.0433 - val_loss: 0.0453 - val_mean_absolute_error: 0.0453 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0444 - mean_absolute_error: 0.0444\n",
      "Epoch 00059: val_loss improved from 0.04531 to 0.04500, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 664us/sample - loss: 0.0444 - mean_absolute_error: 0.0444 - val_loss: 0.0450 - val_mean_absolute_error: 0.0450 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0429 - mean_absolute_error: 0.0429\n",
      "Epoch 00060: val_loss improved from 0.04500 to 0.04470, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 673us/sample - loss: 0.0429 - mean_absolute_error: 0.0429 - val_loss: 0.0447 - val_mean_absolute_error: 0.0447 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0407 - mean_absolute_error: 0.0407\n",
      "Epoch 00061: val_loss improved from 0.04470 to 0.04443, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 651us/sample - loss: 0.0407 - mean_absolute_error: 0.0407 - val_loss: 0.0444 - val_mean_absolute_error: 0.0444 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0439 - mean_absolute_error: 0.0439\n",
      "Epoch 00062: val_loss improved from 0.04443 to 0.04419, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 666us/sample - loss: 0.0439 - mean_absolute_error: 0.0439 - val_loss: 0.0442 - val_mean_absolute_error: 0.0442 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0422 - mean_absolute_error: 0.0422\n",
      "Epoch 00063: val_loss improved from 0.04419 to 0.04396, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 658us/sample - loss: 0.0422 - mean_absolute_error: 0.0422 - val_loss: 0.0440 - val_mean_absolute_error: 0.0440 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0419 - mean_absolute_error: 0.0419\n",
      "Epoch 00064: val_loss improved from 0.04396 to 0.04373, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 642us/sample - loss: 0.0419 - mean_absolute_error: 0.0419 - val_loss: 0.0437 - val_mean_absolute_error: 0.0437 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0422 - mean_absolute_error: 0.0422\n",
      "Epoch 00065: val_loss improved from 0.04373 to 0.04349, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 639us/sample - loss: 0.0422 - mean_absolute_error: 0.0422 - val_loss: 0.0435 - val_mean_absolute_error: 0.0435 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0434 - mean_absolute_error: 0.0434\n",
      "Epoch 00066: val_loss improved from 0.04349 to 0.04327, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 659us/sample - loss: 0.0434 - mean_absolute_error: 0.0434 - val_loss: 0.0433 - val_mean_absolute_error: 0.0433 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0410 - mean_absolute_error: 0.0410\n",
      "Epoch 00067: val_loss improved from 0.04327 to 0.04304, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 737us/sample - loss: 0.0410 - mean_absolute_error: 0.0410 - val_loss: 0.0430 - val_mean_absolute_error: 0.0430 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0389 - mean_absolute_error: 0.0389\n",
      "Epoch 00068: val_loss improved from 0.04304 to 0.04284, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 657us/sample - loss: 0.0389 - mean_absolute_error: 0.0389 - val_loss: 0.0428 - val_mean_absolute_error: 0.0428 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0429 - mean_absolute_error: 0.0429\n",
      "Epoch 00069: val_loss improved from 0.04284 to 0.04263, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 700us/sample - loss: 0.0429 - mean_absolute_error: 0.0429 - val_loss: 0.0426 - val_mean_absolute_error: 0.0426 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0388 - mean_absolute_error: 0.0388\n",
      "Epoch 00070: val_loss improved from 0.04263 to 0.04243, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 686us/sample - loss: 0.0388 - mean_absolute_error: 0.0388 - val_loss: 0.0424 - val_mean_absolute_error: 0.0424 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0408 - mean_absolute_error: 0.0408\n",
      "Epoch 00071: val_loss improved from 0.04243 to 0.04223, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 682us/sample - loss: 0.0408 - mean_absolute_error: 0.0408 - val_loss: 0.0422 - val_mean_absolute_error: 0.0422 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0375 - mean_absolute_error: 0.0375\n",
      "Epoch 00072: val_loss improved from 0.04223 to 0.04204, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 731us/sample - loss: 0.0375 - mean_absolute_error: 0.0375 - val_loss: 0.0420 - val_mean_absolute_error: 0.0420 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0404 - mean_absolute_error: 0.0404\n",
      "Epoch 00073: val_loss improved from 0.04204 to 0.04185, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 678us/sample - loss: 0.0404 - mean_absolute_error: 0.0404 - val_loss: 0.0418 - val_mean_absolute_error: 0.0418 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0384 - mean_absolute_error: 0.0384\n",
      "Epoch 00074: val_loss improved from 0.04185 to 0.04167, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 663us/sample - loss: 0.0384 - mean_absolute_error: 0.0384 - val_loss: 0.0417 - val_mean_absolute_error: 0.0417 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0394 - mean_absolute_error: 0.0394\n",
      "Epoch 00075: val_loss improved from 0.04167 to 0.04151, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 699us/sample - loss: 0.0394 - mean_absolute_error: 0.0394 - val_loss: 0.0415 - val_mean_absolute_error: 0.0415 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0379 - mean_absolute_error: 0.0379\n",
      "Epoch 00076: val_loss improved from 0.04151 to 0.04134, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 702us/sample - loss: 0.0379 - mean_absolute_error: 0.0379 - val_loss: 0.0413 - val_mean_absolute_error: 0.0413 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0390 - mean_absolute_error: 0.0390\n",
      "Epoch 00077: val_loss improved from 0.04134 to 0.04120, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 695us/sample - loss: 0.0390 - mean_absolute_error: 0.0390 - val_loss: 0.0412 - val_mean_absolute_error: 0.0412 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0389 - mean_absolute_error: 0.0389\n",
      "Epoch 00078: val_loss improved from 0.04120 to 0.04107, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 651us/sample - loss: 0.0389 - mean_absolute_error: 0.0389 - val_loss: 0.0411 - val_mean_absolute_error: 0.0411 - lr: 0.0010\n",
      "Epoch 79/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0374 - mean_absolute_error: 0.0374\n",
      "Epoch 00079: val_loss improved from 0.04107 to 0.04094, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 675us/sample - loss: 0.0374 - mean_absolute_error: 0.0374 - val_loss: 0.0409 - val_mean_absolute_error: 0.0409 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0385 - mean_absolute_error: 0.0385\n",
      "Epoch 00080: val_loss improved from 0.04094 to 0.04082, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 656us/sample - loss: 0.0385 - mean_absolute_error: 0.0385 - val_loss: 0.0408 - val_mean_absolute_error: 0.0408 - lr: 0.0010\n",
      "Epoch 81/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0366 - mean_absolute_error: 0.0366\n",
      "Epoch 00081: val_loss improved from 0.04082 to 0.04070, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 648us/sample - loss: 0.0366 - mean_absolute_error: 0.0366 - val_loss: 0.0407 - val_mean_absolute_error: 0.0407 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0370 - mean_absolute_error: 0.0370\n",
      "Epoch 00082: val_loss improved from 0.04070 to 0.04059, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 635us/sample - loss: 0.0370 - mean_absolute_error: 0.0370 - val_loss: 0.0406 - val_mean_absolute_error: 0.0406 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0378 - mean_absolute_error: 0.0378\n",
      "Epoch 00083: val_loss improved from 0.04059 to 0.04048, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 585us/sample - loss: 0.0378 - mean_absolute_error: 0.0378 - val_loss: 0.0405 - val_mean_absolute_error: 0.0405 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00084: val_loss improved from 0.04048 to 0.04040, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 609us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0404 - val_mean_absolute_error: 0.0404 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0365 - mean_absolute_error: 0.0365\n",
      "Epoch 00085: val_loss improved from 0.04040 to 0.04034, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 631us/sample - loss: 0.0365 - mean_absolute_error: 0.0365 - val_loss: 0.0403 - val_mean_absolute_error: 0.0403 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0369 - mean_absolute_error: 0.0369\n",
      "Epoch 00086: val_loss improved from 0.04034 to 0.04026, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 623us/sample - loss: 0.0369 - mean_absolute_error: 0.0369 - val_loss: 0.0403 - val_mean_absolute_error: 0.0403 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0377 - mean_absolute_error: 0.0377\n",
      "Epoch 00087: val_loss improved from 0.04026 to 0.04019, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 636us/sample - loss: 0.0377 - mean_absolute_error: 0.0377 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0366 - mean_absolute_error: 0.0366\n",
      "Epoch 00088: val_loss improved from 0.04019 to 0.04013, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 644us/sample - loss: 0.0366 - mean_absolute_error: 0.0366 - val_loss: 0.0401 - val_mean_absolute_error: 0.0401 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00089: val_loss improved from 0.04013 to 0.04007, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 649us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0401 - val_mean_absolute_error: 0.0401 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00090: val_loss improved from 0.04007 to 0.04003, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 621us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0400 - val_mean_absolute_error: 0.0400 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0357 - mean_absolute_error: 0.0357\n",
      "Epoch 00091: val_loss improved from 0.04003 to 0.03999, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 629us/sample - loss: 0.0357 - mean_absolute_error: 0.0357 - val_loss: 0.0400 - val_mean_absolute_error: 0.0400 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00092: val_loss improved from 0.03999 to 0.03996, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 659us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0400 - val_mean_absolute_error: 0.0400 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0335 - mean_absolute_error: 0.0335\n",
      "Epoch 00093: val_loss improved from 0.03996 to 0.03991, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 705us/sample - loss: 0.0335 - mean_absolute_error: 0.0335 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399 - lr: 0.0010\n",
      "Epoch 94/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0339 - mean_absolute_error: 0.0339\n",
      "Epoch 00094: val_loss improved from 0.03991 to 0.03986, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 719us/sample - loss: 0.0339 - mean_absolute_error: 0.0339 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399 - lr: 0.0010\n",
      "Epoch 95/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0346 - mean_absolute_error: 0.0346\n",
      "Epoch 00095: val_loss improved from 0.03986 to 0.03978, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 736us/sample - loss: 0.0346 - mean_absolute_error: 0.0346 - val_loss: 0.0398 - val_mean_absolute_error: 0.0398 - lr: 0.0010\n",
      "Epoch 96/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0363 - mean_absolute_error: 0.0363\n",
      "Epoch 00096: val_loss improved from 0.03978 to 0.03971, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 711us/sample - loss: 0.0363 - mean_absolute_error: 0.0363 - val_loss: 0.0397 - val_mean_absolute_error: 0.0397 - lr: 0.0010\n",
      "Epoch 97/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00097: val_loss improved from 0.03971 to 0.03963, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 782us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0396 - val_mean_absolute_error: 0.0396 - lr: 0.0010\n",
      "Epoch 98/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0328 - mean_absolute_error: 0.0328\n",
      "Epoch 00098: val_loss improved from 0.03963 to 0.03955, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 944us/sample - loss: 0.0328 - mean_absolute_error: 0.0328 - val_loss: 0.0396 - val_mean_absolute_error: 0.0396 - lr: 0.0010\n",
      "Epoch 99/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00099: val_loss improved from 0.03955 to 0.03952, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 862us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0395 - val_mean_absolute_error: 0.0395 - lr: 0.0010\n",
      "Epoch 100/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0349 - mean_absolute_error: 0.0349\n",
      "Epoch 00100: val_loss improved from 0.03952 to 0.03945, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 731us/sample - loss: 0.0349 - mean_absolute_error: 0.0349 - val_loss: 0.0395 - val_mean_absolute_error: 0.0395 - lr: 0.0010\n",
      "Epoch 101/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0348 - mean_absolute_error: 0.0348\n",
      "Epoch 00101: val_loss improved from 0.03945 to 0.03940, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 717us/sample - loss: 0.0348 - mean_absolute_error: 0.0348 - val_loss: 0.0394 - val_mean_absolute_error: 0.0394 - lr: 0.0010\n",
      "Epoch 102/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00102: val_loss improved from 0.03940 to 0.03935, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 703us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0394 - val_mean_absolute_error: 0.0394 - lr: 0.0010\n",
      "Epoch 103/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0328 - mean_absolute_error: 0.0328\n",
      "Epoch 00103: val_loss improved from 0.03935 to 0.03930, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 704us/sample - loss: 0.0328 - mean_absolute_error: 0.0328 - val_loss: 0.0393 - val_mean_absolute_error: 0.0393 - lr: 0.0010\n",
      "Epoch 104/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0335 - mean_absolute_error: 0.0335\n",
      "Epoch 00104: val_loss improved from 0.03930 to 0.03923, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 720us/sample - loss: 0.0335 - mean_absolute_error: 0.0335 - val_loss: 0.0392 - val_mean_absolute_error: 0.0392 - lr: 0.0010\n",
      "Epoch 105/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0345 - mean_absolute_error: 0.0345\n",
      "Epoch 00105: val_loss improved from 0.03923 to 0.03916, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 694us/sample - loss: 0.0345 - mean_absolute_error: 0.0345 - val_loss: 0.0392 - val_mean_absolute_error: 0.0392 - lr: 0.0010\n",
      "Epoch 106/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0339 - mean_absolute_error: 0.0339\n",
      "Epoch 00106: val_loss improved from 0.03916 to 0.03908, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 662us/sample - loss: 0.0339 - mean_absolute_error: 0.0339 - val_loss: 0.0391 - val_mean_absolute_error: 0.0391 - lr: 0.0010\n",
      "Epoch 107/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0339 - mean_absolute_error: 0.0339\n",
      "Epoch 00107: val_loss improved from 0.03908 to 0.03899, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 649us/sample - loss: 0.0339 - mean_absolute_error: 0.0339 - val_loss: 0.0390 - val_mean_absolute_error: 0.0390 - lr: 0.0010\n",
      "Epoch 108/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0344 - mean_absolute_error: 0.0344\n",
      "Epoch 00108: val_loss improved from 0.03899 to 0.03890, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 715us/sample - loss: 0.0344 - mean_absolute_error: 0.0344 - val_loss: 0.0389 - val_mean_absolute_error: 0.0389 - lr: 0.0010\n",
      "Epoch 109/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0339 - mean_absolute_error: 0.0339\n",
      "Epoch 00109: val_loss improved from 0.03890 to 0.03883, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 709us/sample - loss: 0.0339 - mean_absolute_error: 0.0339 - val_loss: 0.0388 - val_mean_absolute_error: 0.0388 - lr: 0.0010\n",
      "Epoch 110/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0342 - mean_absolute_error: 0.0342\n",
      "Epoch 00110: val_loss improved from 0.03883 to 0.03876, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 683us/sample - loss: 0.0342 - mean_absolute_error: 0.0342 - val_loss: 0.0388 - val_mean_absolute_error: 0.0388 - lr: 0.0010\n",
      "Epoch 111/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0337 - mean_absolute_error: 0.0337\n",
      "Epoch 00111: val_loss improved from 0.03876 to 0.03872, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 623us/sample - loss: 0.0337 - mean_absolute_error: 0.0337 - val_loss: 0.0387 - val_mean_absolute_error: 0.0387 - lr: 0.0010\n",
      "Epoch 112/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0339 - mean_absolute_error: 0.0339\n",
      "Epoch 00112: val_loss improved from 0.03872 to 0.03868, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 664us/sample - loss: 0.0339 - mean_absolute_error: 0.0339 - val_loss: 0.0387 - val_mean_absolute_error: 0.0387 - lr: 0.0010\n",
      "Epoch 113/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0337 - mean_absolute_error: 0.0337\n",
      "Epoch 00113: val_loss improved from 0.03868 to 0.03864, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 655us/sample - loss: 0.0337 - mean_absolute_error: 0.0337 - val_loss: 0.0386 - val_mean_absolute_error: 0.0386 - lr: 0.0010\n",
      "Epoch 114/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0339 - mean_absolute_error: 0.0339\n",
      "Epoch 00114: val_loss improved from 0.03864 to 0.03859, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 646us/sample - loss: 0.0339 - mean_absolute_error: 0.0339 - val_loss: 0.0386 - val_mean_absolute_error: 0.0386 - lr: 0.0010\n",
      "Epoch 115/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0332 - mean_absolute_error: 0.0332\n",
      "Epoch 00115: val_loss improved from 0.03859 to 0.03854, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 659us/sample - loss: 0.0332 - mean_absolute_error: 0.0332 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0322 - mean_absolute_error: 0.0322\n",
      "Epoch 00116: val_loss improved from 0.03854 to 0.03850, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 666us/sample - loss: 0.0322 - mean_absolute_error: 0.0322 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 0.0010\n",
      "Epoch 117/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0334 - mean_absolute_error: 0.0334\n",
      "Epoch 00117: val_loss improved from 0.03850 to 0.03845, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 669us/sample - loss: 0.0334 - mean_absolute_error: 0.0334 - val_loss: 0.0384 - val_mean_absolute_error: 0.0384 - lr: 0.0010\n",
      "Epoch 118/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0336 - mean_absolute_error: 0.0336\n",
      "Epoch 00118: val_loss improved from 0.03845 to 0.03840, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 674us/sample - loss: 0.0336 - mean_absolute_error: 0.0336 - val_loss: 0.0384 - val_mean_absolute_error: 0.0384 - lr: 0.0010\n",
      "Epoch 119/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0332 - mean_absolute_error: 0.0332\n",
      "Epoch 00119: val_loss improved from 0.03840 to 0.03836, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 662us/sample - loss: 0.0332 - mean_absolute_error: 0.0332 - val_loss: 0.0384 - val_mean_absolute_error: 0.0384 - lr: 0.0010\n",
      "Epoch 120/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0342 - mean_absolute_error: 0.0342\n",
      "Epoch 00120: val_loss improved from 0.03836 to 0.03831, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 716us/sample - loss: 0.0342 - mean_absolute_error: 0.0342 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 0.0010\n",
      "Epoch 121/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0327 - mean_absolute_error: 0.0327\n",
      "Epoch 00121: val_loss improved from 0.03831 to 0.03826, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 701us/sample - loss: 0.0327 - mean_absolute_error: 0.0327 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 0.0010\n",
      "Epoch 122/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0333 - mean_absolute_error: 0.0333\n",
      "Epoch 00122: val_loss improved from 0.03826 to 0.03821, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 699us/sample - loss: 0.0333 - mean_absolute_error: 0.0333 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 - lr: 0.0010\n",
      "Epoch 123/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0332 - mean_absolute_error: 0.0332\n",
      "Epoch 00123: val_loss improved from 0.03821 to 0.03816, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 713us/sample - loss: 0.0332 - mean_absolute_error: 0.0332 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 - lr: 0.0010\n",
      "Epoch 124/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0332 - mean_absolute_error: 0.0332\n",
      "Epoch 00124: val_loss improved from 0.03816 to 0.03810, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 703us/sample - loss: 0.0332 - mean_absolute_error: 0.0332 - val_loss: 0.0381 - val_mean_absolute_error: 0.0381 - lr: 0.0010\n",
      "Epoch 125/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0327 - mean_absolute_error: 0.0327\n",
      "Epoch 00125: val_loss improved from 0.03810 to 0.03804, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 701us/sample - loss: 0.0327 - mean_absolute_error: 0.0327 - val_loss: 0.0380 - val_mean_absolute_error: 0.0380 - lr: 0.0010\n",
      "Epoch 126/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0331 - mean_absolute_error: 0.0331\n",
      "Epoch 00126: val_loss improved from 0.03804 to 0.03800, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 723us/sample - loss: 0.0331 - mean_absolute_error: 0.0331 - val_loss: 0.0380 - val_mean_absolute_error: 0.0380 - lr: 0.0010\n",
      "Epoch 127/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0327 - mean_absolute_error: 0.0327\n",
      "Epoch 00127: val_loss improved from 0.03800 to 0.03796, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 690us/sample - loss: 0.0327 - mean_absolute_error: 0.0327 - val_loss: 0.0380 - val_mean_absolute_error: 0.0380 - lr: 0.0010\n",
      "Epoch 128/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0326 - mean_absolute_error: 0.0326\n",
      "Epoch 00128: val_loss improved from 0.03796 to 0.03793, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 686us/sample - loss: 0.0326 - mean_absolute_error: 0.0326 - val_loss: 0.0379 - val_mean_absolute_error: 0.0379 - lr: 0.0010\n",
      "Epoch 129/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0324 - mean_absolute_error: 0.0324\n",
      "Epoch 00129: val_loss improved from 0.03793 to 0.03783, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 697us/sample - loss: 0.0324 - mean_absolute_error: 0.0324 - val_loss: 0.0378 - val_mean_absolute_error: 0.0378 - lr: 0.0010\n",
      "Epoch 130/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0335 - mean_absolute_error: 0.0335\n",
      "Epoch 00130: val_loss improved from 0.03783 to 0.03781, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 715us/sample - loss: 0.0335 - mean_absolute_error: 0.0335 - val_loss: 0.0378 - val_mean_absolute_error: 0.0378 - lr: 0.0010\n",
      "Epoch 131/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0327 - mean_absolute_error: 0.0327\n",
      "Epoch 00131: val_loss improved from 0.03781 to 0.03779, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 722us/sample - loss: 0.0327 - mean_absolute_error: 0.0327 - val_loss: 0.0378 - val_mean_absolute_error: 0.0378 - lr: 0.0010\n",
      "Epoch 132/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 00132: val_loss improved from 0.03779 to 0.03779, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 685us/sample - loss: 0.0316 - mean_absolute_error: 0.0316 - val_loss: 0.0378 - val_mean_absolute_error: 0.0378 - lr: 0.0010\n",
      "Epoch 133/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0328 - mean_absolute_error: 0.0328\n",
      "Epoch 00133: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 632us/sample - loss: 0.0328 - mean_absolute_error: 0.0328 - val_loss: 0.0378 - val_mean_absolute_error: 0.0378 - lr: 0.0010\n",
      "Epoch 134/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0319 - mean_absolute_error: 0.0319\n",
      "Epoch 00134: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00134: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "400/400 [==============================] - 0s 876us/sample - loss: 0.0319 - mean_absolute_error: 0.0319 - val_loss: 0.0378 - val_mean_absolute_error: 0.0378 - lr: 0.0010\n",
      "Epoch 135/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0319 - mean_absolute_error: 0.0319\n",
      "Epoch 00135: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 632us/sample - loss: 0.0319 - mean_absolute_error: 0.0319 - val_loss: 0.0378 - val_mean_absolute_error: 0.0378 - lr: 9.0000e-04\n",
      "Epoch 136/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0326 - mean_absolute_error: 0.0326\n",
      "Epoch 00136: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 662us/sample - loss: 0.0326 - mean_absolute_error: 0.0326 - val_loss: 0.0379 - val_mean_absolute_error: 0.0379 - lr: 9.0000e-04\n",
      "Epoch 137/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0327 - mean_absolute_error: 0.0327\n",
      "Epoch 00137: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 597us/sample - loss: 0.0327 - mean_absolute_error: 0.0327 - val_loss: 0.0379 - val_mean_absolute_error: 0.0379 - lr: 9.0000e-04\n",
      "Epoch 138/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0320 - mean_absolute_error: 0.0320\n",
      "Epoch 00138: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 588us/sample - loss: 0.0320 - mean_absolute_error: 0.0320 - val_loss: 0.0379 - val_mean_absolute_error: 0.0379 - lr: 9.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.0308\n",
      "Epoch 00139: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "400/400 [==============================] - 0s 627us/sample - loss: 0.0308 - mean_absolute_error: 0.0308 - val_loss: 0.0379 - val_mean_absolute_error: 0.0379 - lr: 9.0000e-04\n",
      "Epoch 140/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0331 - mean_absolute_error: 0.0331\n",
      "Epoch 00140: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 543us/sample - loss: 0.0331 - mean_absolute_error: 0.0331 - val_loss: 0.0380 - val_mean_absolute_error: 0.0380 - lr: 8.1000e-04\n",
      "Epoch 141/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0317 - mean_absolute_error: 0.0317\n",
      "Epoch 00141: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 556us/sample - loss: 0.0317 - mean_absolute_error: 0.0317 - val_loss: 0.0380 - val_mean_absolute_error: 0.0380 - lr: 8.1000e-04\n",
      "Epoch 142/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0325 - mean_absolute_error: 0.0325\n",
      "Epoch 00142: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 564us/sample - loss: 0.0325 - mean_absolute_error: 0.0325 - val_loss: 0.0380 - val_mean_absolute_error: 0.0380 - lr: 8.1000e-04\n",
      "Epoch 143/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 00143: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 555us/sample - loss: 0.0316 - mean_absolute_error: 0.0316 - val_loss: 0.0380 - val_mean_absolute_error: 0.0380 - lr: 8.1000e-04\n",
      "Epoch 144/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0328 - mean_absolute_error: 0.0328\n",
      "Epoch 00144: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00144: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
      "400/400 [==============================] - 0s 543us/sample - loss: 0.0328 - mean_absolute_error: 0.0328 - val_loss: 0.0381 - val_mean_absolute_error: 0.0381 - lr: 8.1000e-04\n",
      "Epoch 145/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0331 - mean_absolute_error: 0.0331\n",
      "Epoch 00145: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 549us/sample - loss: 0.0331 - mean_absolute_error: 0.0331 - val_loss: 0.0381 - val_mean_absolute_error: 0.0381 - lr: 7.2900e-04\n",
      "Epoch 146/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.0315\n",
      "Epoch 00146: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 553us/sample - loss: 0.0315 - mean_absolute_error: 0.0315 - val_loss: 0.0381 - val_mean_absolute_error: 0.0381 - lr: 7.2900e-04\n",
      "Epoch 147/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0319 - mean_absolute_error: 0.0319\n",
      "Epoch 00147: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 546us/sample - loss: 0.0319 - mean_absolute_error: 0.0319 - val_loss: 0.0381 - val_mean_absolute_error: 0.0381 - lr: 7.2900e-04\n",
      "Epoch 148/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0320 - mean_absolute_error: 0.0320\n",
      "Epoch 00148: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 622us/sample - loss: 0.0320 - mean_absolute_error: 0.0320 - val_loss: 0.0381 - val_mean_absolute_error: 0.0381 - lr: 7.2900e-04\n",
      "Epoch 149/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0322 - mean_absolute_error: 0.0322\n",
      "Epoch 00149: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00149: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
      "400/400 [==============================] - 0s 581us/sample - loss: 0.0322 - mean_absolute_error: 0.0322 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 - lr: 7.2900e-04\n",
      "Epoch 150/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0324 - mean_absolute_error: 0.0324\n",
      "Epoch 00150: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 553us/sample - loss: 0.0324 - mean_absolute_error: 0.0324 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 - lr: 6.5610e-04\n",
      "Epoch 151/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0319 - mean_absolute_error: 0.0319\n",
      "Epoch 00151: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 568us/sample - loss: 0.0319 - mean_absolute_error: 0.0319 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 - lr: 6.5610e-04\n",
      "Epoch 152/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0312 - mean_absolute_error: 0.0312\n",
      "Epoch 00152: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 581us/sample - loss: 0.0312 - mean_absolute_error: 0.0312 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 - lr: 6.5610e-04\n",
      "Epoch 153/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0325 - mean_absolute_error: 0.0325\n",
      "Epoch 00153: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 583us/sample - loss: 0.0325 - mean_absolute_error: 0.0325 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 - lr: 6.5610e-04\n",
      "Epoch 154/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 00154: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00154: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
      "400/400 [==============================] - 0s 560us/sample - loss: 0.0316 - mean_absolute_error: 0.0316 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 6.5610e-04\n",
      "Epoch 155/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0318\n",
      "Epoch 00155: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 559us/sample - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 5.9049e-04\n",
      "Epoch 156/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0321 - mean_absolute_error: 0.0321\n",
      "Epoch 00156: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 592us/sample - loss: 0.0321 - mean_absolute_error: 0.0321 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 5.9049e-04\n",
      "Epoch 157/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0314 - mean_absolute_error: 0.0314\n",
      "Epoch 00157: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 610us/sample - loss: 0.0314 - mean_absolute_error: 0.0314 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 5.9049e-04\n",
      "Epoch 158/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0320 - mean_absolute_error: 0.0320\n",
      "Epoch 00158: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 743us/sample - loss: 0.0320 - mean_absolute_error: 0.0320 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 5.9049e-04\n",
      "Epoch 159/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0318\n",
      "Epoch 00159: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00159: ReduceLROnPlateau reducing learning rate to 0.0005314410547725857.\n",
      "400/400 [==============================] - 0s 598us/sample - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 5.9049e-04\n",
      "Epoch 160/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0318\n",
      "Epoch 00160: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 593us/sample - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 5.3144e-04\n",
      "Epoch 161/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0320 - mean_absolute_error: 0.0320\n",
      "Epoch 00161: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 596us/sample - loss: 0.0320 - mean_absolute_error: 0.0320 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 5.3144e-04\n",
      "Epoch 162/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - ETA: 0s - loss: 0.0322 - mean_absolute_error: 0.0322\n",
      "Epoch 00162: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 591us/sample - loss: 0.0322 - mean_absolute_error: 0.0322 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 5.3144e-04\n",
      "Epoch 163/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 00163: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 605us/sample - loss: 0.0316 - mean_absolute_error: 0.0316 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 5.3144e-04\n",
      "Epoch 164/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 00164: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00164: ReduceLROnPlateau reducing learning rate to 0.00047829695977270604.\n",
      "400/400 [==============================] - 0s 582us/sample - loss: 0.0316 - mean_absolute_error: 0.0316 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 5.3144e-04\n",
      "Epoch 165/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0310 - mean_absolute_error: 0.0310\n",
      "Epoch 00165: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 607us/sample - loss: 0.0310 - mean_absolute_error: 0.0310 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 4.7830e-04\n",
      "Epoch 166/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0324 - mean_absolute_error: 0.0324\n",
      "Epoch 00166: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 587us/sample - loss: 0.0324 - mean_absolute_error: 0.0324 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 4.7830e-04\n",
      "Epoch 167/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0309 - mean_absolute_error: 0.0309\n",
      "Epoch 00167: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 582us/sample - loss: 0.0309 - mean_absolute_error: 0.0309 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 4.7830e-04\n",
      "Epoch 168/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0320 - mean_absolute_error: 0.0320\n",
      "Epoch 00168: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 580us/sample - loss: 0.0320 - mean_absolute_error: 0.0320 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 4.7830e-04\n",
      "Epoch 169/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0319 - mean_absolute_error: 0.0319\n",
      "Epoch 00169: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00169: ReduceLROnPlateau reducing learning rate to 0.0004304672533180565.\n",
      "400/400 [==============================] - 0s 590us/sample - loss: 0.0319 - mean_absolute_error: 0.0319 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 4.7830e-04\n",
      "Epoch 170/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0320 - mean_absolute_error: 0.0320\n",
      "Epoch 00170: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 579us/sample - loss: 0.0320 - mean_absolute_error: 0.0320 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 4.3047e-04\n",
      "Epoch 171/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0311 - mean_absolute_error: 0.0311\n",
      "Epoch 00171: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 586us/sample - loss: 0.0311 - mean_absolute_error: 0.0311 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 4.3047e-04\n",
      "Epoch 172/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.0315\n",
      "Epoch 00172: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 689us/sample - loss: 0.0315 - mean_absolute_error: 0.0315 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 4.3047e-04\n",
      "Epoch 173/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0318\n",
      "Epoch 00173: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 905us/sample - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 - lr: 4.3047e-04\n",
      "Epoch 174/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0319 - mean_absolute_error: 0.0319\n",
      "Epoch 00174: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00174: ReduceLROnPlateau reducing learning rate to 0.00038742052274756136.\n",
      "400/400 [==============================] - 0s 623us/sample - loss: 0.0319 - mean_absolute_error: 0.0319 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 - lr: 4.3047e-04\n",
      "Epoch 175/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0326 - mean_absolute_error: 0.0326\n",
      "Epoch 00175: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 606us/sample - loss: 0.0326 - mean_absolute_error: 0.0326 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 - lr: 3.8742e-04\n",
      "Epoch 176/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0313 - mean_absolute_error: 0.0313\n",
      "Epoch 00176: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 1s 1ms/sample - loss: 0.0313 - mean_absolute_error: 0.0313 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 - lr: 3.8742e-04\n",
      "Epoch 177/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0310 - mean_absolute_error: 0.0310\n",
      "Epoch 00177: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 750us/sample - loss: 0.0310 - mean_absolute_error: 0.0310 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 - lr: 3.8742e-04\n",
      "Epoch 178/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0311 - mean_absolute_error: 0.0311\n",
      "Epoch 00178: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 586us/sample - loss: 0.0311 - mean_absolute_error: 0.0311 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 - lr: 3.8742e-04\n",
      "Epoch 179/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0317 - mean_absolute_error: 0.0317\n",
      "Epoch 00179: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00179: ReduceLROnPlateau reducing learning rate to 0.0003486784757114947.\n",
      "400/400 [==============================] - 0s 627us/sample - loss: 0.0317 - mean_absolute_error: 0.0317 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 - lr: 3.8742e-04\n",
      "Epoch 180/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.0308\n",
      "Epoch 00180: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 619us/sample - loss: 0.0308 - mean_absolute_error: 0.0308 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 - lr: 3.4868e-04\n",
      "Epoch 181/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.0306\n",
      "Epoch 00181: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 558us/sample - loss: 0.0306 - mean_absolute_error: 0.0306 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 - lr: 3.4868e-04\n",
      "Epoch 182/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0311 - mean_absolute_error: 0.0311\n",
      "Epoch 00182: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 594us/sample - loss: 0.0311 - mean_absolute_error: 0.0311 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 - lr: 3.4868e-04\n",
      "Epoch 183/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0313 - mean_absolute_error: 0.0313\n",
      "Epoch 00183: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 531us/sample - loss: 0.0313 - mean_absolute_error: 0.0313 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 3.4868e-04\n",
      "Epoch 184/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0317 - mean_absolute_error: 0.0317\n",
      "Epoch 00184: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00184: ReduceLROnPlateau reducing learning rate to 0.00031381062290165574.\n",
      "400/400 [==============================] - 0s 570us/sample - loss: 0.0317 - mean_absolute_error: 0.0317 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 3.4868e-04\n",
      "Epoch 185/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - ETA: 0s - loss: 0.0317 - mean_absolute_error: 0.0317\n",
      "Epoch 00185: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 536us/sample - loss: 0.0317 - mean_absolute_error: 0.0317 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 3.1381e-04\n",
      "Epoch 186/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0309 - mean_absolute_error: 0.0309\n",
      "Epoch 00186: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 815us/sample - loss: 0.0309 - mean_absolute_error: 0.0309 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 3.1381e-04\n",
      "Epoch 187/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0323 - mean_absolute_error: 0.0323\n",
      "Epoch 00187: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 935us/sample - loss: 0.0323 - mean_absolute_error: 0.0323 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 3.1381e-04\n",
      "Epoch 188/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0323 - mean_absolute_error: 0.0323\n",
      "Epoch 00188: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 853us/sample - loss: 0.0323 - mean_absolute_error: 0.0323 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383 - lr: 3.1381e-04\n",
      "Epoch 189/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.0315\n",
      "Epoch 00189: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00189: ReduceLROnPlateau reducing learning rate to 0.0002824295632308349.\n",
      "400/400 [==============================] - 0s 754us/sample - loss: 0.0315 - mean_absolute_error: 0.0315 - val_loss: 0.0384 - val_mean_absolute_error: 0.0384 - lr: 3.1381e-04\n",
      "Epoch 190/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0318\n",
      "Epoch 00190: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 700us/sample - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0384 - val_mean_absolute_error: 0.0384 - lr: 2.8243e-04\n",
      "Epoch 191/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0317 - mean_absolute_error: 0.0317\n",
      "Epoch 00191: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 742us/sample - loss: 0.0317 - mean_absolute_error: 0.0317 - val_loss: 0.0384 - val_mean_absolute_error: 0.0384 - lr: 2.8243e-04\n",
      "Epoch 192/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0312 - mean_absolute_error: 0.0312\n",
      "Epoch 00192: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 709us/sample - loss: 0.0312 - mean_absolute_error: 0.0312 - val_loss: 0.0384 - val_mean_absolute_error: 0.0384 - lr: 2.8243e-04\n",
      "Epoch 193/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0313 - mean_absolute_error: 0.0313\n",
      "Epoch 00193: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 641us/sample - loss: 0.0313 - mean_absolute_error: 0.0313 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 2.8243e-04\n",
      "Epoch 194/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 00194: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00194: ReduceLROnPlateau reducing learning rate to 0.00025418660952709616.\n",
      "400/400 [==============================] - 0s 589us/sample - loss: 0.0316 - mean_absolute_error: 0.0316 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 2.8243e-04\n",
      "Epoch 195/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0314 - mean_absolute_error: 0.0314\n",
      "Epoch 00195: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 564us/sample - loss: 0.0314 - mean_absolute_error: 0.0314 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 2.5419e-04\n",
      "Epoch 196/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0310 - mean_absolute_error: 0.0310\n",
      "Epoch 00196: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 572us/sample - loss: 0.0310 - mean_absolute_error: 0.0310 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 2.5419e-04\n",
      "Epoch 197/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0318\n",
      "Epoch 00197: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 551us/sample - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 2.5419e-04\n",
      "Epoch 198/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0322 - mean_absolute_error: 0.0322\n",
      "Epoch 00198: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 579us/sample - loss: 0.0322 - mean_absolute_error: 0.0322 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 2.5419e-04\n",
      "Epoch 199/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0317 - mean_absolute_error: 0.0317\n",
      "Epoch 00199: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00199: ReduceLROnPlateau reducing learning rate to 0.00022876793809700757.\n",
      "400/400 [==============================] - 0s 593us/sample - loss: 0.0317 - mean_absolute_error: 0.0317 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 2.5419e-04\n",
      "Epoch 200/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 00200: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 555us/sample - loss: 0.0316 - mean_absolute_error: 0.0316 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 2.2877e-04\n",
      "Epoch 201/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0311 - mean_absolute_error: 0.0311\n",
      "Epoch 00201: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 532us/sample - loss: 0.0311 - mean_absolute_error: 0.0311 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 2.2877e-04\n",
      "Epoch 202/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.0315\n",
      "Epoch 00202: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 525us/sample - loss: 0.0315 - mean_absolute_error: 0.0315 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 2.2877e-04\n",
      "Epoch 203/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0318\n",
      "Epoch 00203: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 553us/sample - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 2.2877e-04\n",
      "Epoch 204/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0311 - mean_absolute_error: 0.0311\n",
      "Epoch 00204: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00204: ReduceLROnPlateau reducing learning rate to 0.00020589114428730683.\n",
      "400/400 [==============================] - 0s 560us/sample - loss: 0.0311 - mean_absolute_error: 0.0311 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 2.2877e-04\n",
      "Epoch 205/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.0315\n",
      "Epoch 00205: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 541us/sample - loss: 0.0315 - mean_absolute_error: 0.0315 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 2.0589e-04\n",
      "Epoch 206/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0307 - mean_absolute_error: 0.0307\n",
      "Epoch 00206: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 541us/sample - loss: 0.0307 - mean_absolute_error: 0.0307 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 2.0589e-04\n",
      "Epoch 207/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0307 - mean_absolute_error: 0.0307\n",
      "Epoch 00207: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 530us/sample - loss: 0.0307 - mean_absolute_error: 0.0307 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 2.0589e-04\n",
      "Epoch 208/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0317 - mean_absolute_error: 0.0317\n",
      "Epoch 00208: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 527us/sample - loss: 0.0317 - mean_absolute_error: 0.0317 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 2.0589e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0314 - mean_absolute_error: 0.0314\n",
      "Epoch 00209: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00209: ReduceLROnPlateau reducing learning rate to 0.00018530203378759326.\n",
      "400/400 [==============================] - 0s 544us/sample - loss: 0.0314 - mean_absolute_error: 0.0314 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 2.0589e-04\n",
      "Epoch 210/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0320 - mean_absolute_error: 0.0320\n",
      "Epoch 00210: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 565us/sample - loss: 0.0320 - mean_absolute_error: 0.0320 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.8530e-04\n",
      "Epoch 211/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0319 - mean_absolute_error: 0.0319\n",
      "Epoch 00211: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 582us/sample - loss: 0.0319 - mean_absolute_error: 0.0319 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.8530e-04\n",
      "Epoch 212/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0320 - mean_absolute_error: 0.0320\n",
      "Epoch 00212: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 609us/sample - loss: 0.0320 - mean_absolute_error: 0.0320 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.8530e-04\n",
      "Epoch 213/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0312 - mean_absolute_error: 0.0312\n",
      "Epoch 00213: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 614us/sample - loss: 0.0312 - mean_absolute_error: 0.0312 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.8530e-04\n",
      "Epoch 214/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0318\n",
      "Epoch 00214: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00214: ReduceLROnPlateau reducing learning rate to 0.00016677183302817866.\n",
      "400/400 [==============================] - 0s 580us/sample - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.8530e-04\n",
      "Epoch 215/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 00215: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 623us/sample - loss: 0.0316 - mean_absolute_error: 0.0316 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.6677e-04\n",
      "Epoch 216/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0319 - mean_absolute_error: 0.0319\n",
      "Epoch 00216: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 615us/sample - loss: 0.0319 - mean_absolute_error: 0.0319 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.6677e-04\n",
      "Epoch 217/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0318\n",
      "Epoch 00217: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 588us/sample - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.6677e-04\n",
      "Epoch 218/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 00218: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 598us/sample - loss: 0.0316 - mean_absolute_error: 0.0316 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.6677e-04\n",
      "Epoch 219/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0312 - mean_absolute_error: 0.0312\n",
      "Epoch 00219: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00219: ReduceLROnPlateau reducing learning rate to 0.00015009464841568844.\n",
      "400/400 [==============================] - 0s 640us/sample - loss: 0.0312 - mean_absolute_error: 0.0312 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.6677e-04\n",
      "Epoch 220/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0318\n",
      "Epoch 00220: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 613us/sample - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.5009e-04\n",
      "Epoch 221/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0318\n",
      "Epoch 00221: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 592us/sample - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.5009e-04\n",
      "Epoch 222/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.0315\n",
      "Epoch 00222: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 591us/sample - loss: 0.0315 - mean_absolute_error: 0.0315 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.5009e-04\n",
      "Epoch 223/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.0308\n",
      "Epoch 00223: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 608us/sample - loss: 0.0308 - mean_absolute_error: 0.0308 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.5009e-04\n",
      "Epoch 224/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0307 - mean_absolute_error: 0.0307\n",
      "Epoch 00224: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00224: ReduceLROnPlateau reducing learning rate to 0.0001350851875031367.\n",
      "400/400 [==============================] - 0s 578us/sample - loss: 0.0307 - mean_absolute_error: 0.0307 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.5009e-04\n",
      "Epoch 225/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 00225: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 557us/sample - loss: 0.0316 - mean_absolute_error: 0.0316 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.3509e-04\n",
      "Epoch 226/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0314 - mean_absolute_error: 0.0314\n",
      "Epoch 00226: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 566us/sample - loss: 0.0314 - mean_absolute_error: 0.0314 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.3509e-04\n",
      "Epoch 227/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0318\n",
      "Epoch 00227: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 570us/sample - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.3509e-04\n",
      "Epoch 228/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0311 - mean_absolute_error: 0.0311\n",
      "Epoch 00228: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 578us/sample - loss: 0.0311 - mean_absolute_error: 0.0311 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.3509e-04\n",
      "Epoch 229/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.0306\n",
      "Epoch 00229: val_loss did not improve from 0.03779\n",
      "\n",
      "Epoch 00229: ReduceLROnPlateau reducing learning rate to 0.00012157666351413355.\n",
      "400/400 [==============================] - 0s 620us/sample - loss: 0.0306 - mean_absolute_error: 0.0306 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.3509e-04\n",
      "Epoch 230/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0314 - mean_absolute_error: 0.0314\n",
      "Epoch 00230: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 563us/sample - loss: 0.0314 - mean_absolute_error: 0.0314 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.2158e-04\n",
      "Epoch 231/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0314 - mean_absolute_error: 0.0314\n",
      "Epoch 00231: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 569us/sample - loss: 0.0314 - mean_absolute_error: 0.0314 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.2158e-04\n",
      "Epoch 232/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 00232: val_loss did not improve from 0.03779\n",
      "400/400 [==============================] - 0s 556us/sample - loss: 0.0316 - mean_absolute_error: 0.0316 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385 - lr: 1.2158e-04\n",
      "Epoch 00232: early stopping\n"
     ]
    }
   ],
   "source": [
    "print(\"Train DNN Performance Drop Predictor...\")\n",
    "\n",
    "verbose = True\n",
    "\n",
    "drop_predictor.fit(X_train_1, y_train, None, epochs=300, batch_size=400,\n",
    "            validation_split=0.2, verbose=verbose, early_stop_patience=100, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd9cb555a50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = drop_predictor.history\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Error\")\n",
    "\n",
    "ax1.plot(history.history['mean_absolute_error'], label=\"Training Drop Error\")\n",
    "if 'val_mean_absolute_error' in history.history:\n",
    "    ax1.plot(history.history['val_mean_absolute_error'], label=\"Validation Drop Error\")\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0., 0.1)\n",
    "\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "\n",
    "ax2.plot(history.history['loss'], label=\"Training Drop Loss\")\n",
    "if 'val_loss' in history.history:\n",
    "    ax2.plot(history.history['val_loss'], label=\"Validation Drop Loss\")\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate DNN and save results...\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate DNN and save results...\")\n",
    "\n",
    "y_train_drops_pred = drop_predictor.predict(X_train_1)\n",
    "\n",
    "y_test_drops_pred = drop_predictor.predict(X_test_1)\n",
    "\n",
    "y_test_unseen_drops_pred = drop_predictor.predict(X_test_unseen_1)\n",
    "\n",
    "y_test_natural_drops_pred = drop_predictor.predict(X_test_natural_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03233008026605374"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_train, y_train_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.029278633954415212"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_test_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.050669790381567874"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test_unseen, y_test_unseen_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05669665936027703"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test_natural, y_test_natural_drops_pred)"
   ]
  }
 ],
 "metadata": {
  "createdOn": 1620650668671,
  "creator": "smaggio",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "ker_dnn_performance_drop_prediction",
   "language": "python",
   "name": "ker_dnn_performance_drop_prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "modifiedBy": "smaggio",
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
