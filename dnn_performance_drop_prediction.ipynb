{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Performance Drop Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose** Train a DNN-based meta-model to predict a primary model accuracy drop (on various shifted datasets) and beat the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Primary Task** RandomForestClassifier to predict low/high sales of video games records. Accuracy on clean validation set 0.798."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data for the Performance Drop Regressor**\n",
    "- training: 500 datasets (X1), their accuracy drop (y), their meta-features (X2)\n",
    "- validation: take a random split of the previous, if needed.\n",
    "- test data:\n",
    "   1. test: 500 datasets (X1) with same shifts as in the training, but different severity (and their X2 and y).\n",
    "   2. test_unseen: 900 datasets (X1) with other types of shifts, not seen at training time (and their X2 and y).\n",
    "   3. test_natural: 10 datasets (X1) coming from different domains, but same primary task (and their X2 and y).\n",
    "   \n",
    "Each dataset has 475 rows and 9 features (preprocessed already).\n",
    "\n",
    "Each meta-feature vector contains 114 features (will be preprocessed in this notebook to 110 final features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline**\n",
    "\n",
    "Baseline-Meta-Features: RandomForestRegressor trained on meta features only (prediction_percentiles, PAD, RCA, confidence drop, BBSDs KS and BBSDh X2 statistics, KS statistics on individual preprocessed features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/smaggio/workspace/dnn_performance_drop_prediction_under_drift/env_dnn_performance_drop_prediction/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "tf.experimental.output_all_intermediates(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here import of what needed to implement DNNs in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, Layer, Concatenate\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model as KerasModel\n",
    "from tensorflow.random import set_seed as set_random_seed\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data and some simple preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fld = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(fld, 'data.pkl'), 'rb') as f:\n",
    "    out = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, test_unseen, test_natural, ref_task, result_df = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = train.datasets\n",
    "y_train = train.drops\n",
    "train_meta_features_orig = train.meta_features\n",
    "train_drift_types = train.drift_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_1 = test.datasets\n",
    "y_test = test.drops\n",
    "test_meta_features_orig = test.meta_features\n",
    "test_drift_types = test.drift_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_unseen_1 = test_unseen.datasets\n",
    "y_test_unseen = test_unseen.drops\n",
    "test_unseen_meta_features_orig = test_unseen.meta_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_natural_1 = test_natural.datasets\n",
    "y_test_natural = test_natural.drops\n",
    "test_natural_meta_features_orig = test_natural.meta_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 114)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_natural_meta_features_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = Pipeline([('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "                    ('scaler', MinMaxScaler())])\n",
    "\n",
    "X_train_2 = imp.fit_transform(train_meta_features_orig)\n",
    "X_test_2 = imp.transform(test_meta_features_orig)\n",
    "X_test_unseen_2 = imp.transform(test_unseen_meta_features_orig)\n",
    "X_test_natural_2 = imp.transform(test_natural_meta_features_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 475, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 110)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 475, 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 475, 9)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_unseen_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 475, 9)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_natural_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the reference task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ref_task.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7978947368421052"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source validation dataset\n",
    "X_src = ref_task.X_orig\n",
    "y_src = ref_task.y\n",
    "\n",
    "# reference accuracy\n",
    "model.score(ref_task.preprocess.transform(X_src), y_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can extract primary predictions for all datasets, and possibly use them as additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 475, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pred = np.array([model.predict_proba(X) for X in X_train_1])\n",
    "X_train_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 475, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_pred = np.array([model.predict_proba(X) for X in X_test_1])\n",
    "X_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 475, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_unseen_pred = np.array([model.predict_proba(X) for X in X_test_unseen_1])\n",
    "X_test_unseen_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 475, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_natural_pred = np.array([model.predict_proba(X) for X in X_test_natural_1])\n",
    "X_test_natural_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Random Forest Regresson on Meta-Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Drift Features Baseline Performance Drop Predictor...\n",
      "Evaluate Drift Features Baseline and save results...\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Drift Features Baseline Performance Drop Predictor...\")\n",
    "\n",
    "regr = RandomForestRegressor().fit(X_train_2, y_train)\n",
    "\n",
    "print(\"Evaluate Drift Features Baseline and save results...\")\n",
    "\n",
    "y_train_drops_pred = regr.predict(X_train_2)\n",
    "\n",
    "y_test_drops_pred = regr.predict(X_test_2)\n",
    "\n",
    "y_test_unseen_drops_pred = regr.predict(X_test_unseen_2)\n",
    "\n",
    "y_test_natural_drops_pred = regr.predict(X_test_natural_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015347031578947366"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_train, y_train_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04576370526315789"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_test_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06023319298245614"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test_unseen, y_test_unseen_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05888210526315787"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test_natural, y_test_natural_drops_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training DNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN based on MLP or LSTM to represent a dataset and predict performance drop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using datasets only, with no meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define MLP-based DNN Performance Drop Predictor...\n",
      "WARNING:tensorflow:From /Users/smaggio/workspace/dnn_performance_drop_prediction_under_drift/env_dnn_performance_drop_prediction/lib/python3.7/site-packages/keras/initializers/initializers_v1.py:55: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "print(\"Define MLP-based DNN Performance Drop Predictor...\")\n",
    "\n",
    "# define MLP based model\n",
    "\n",
    "n_samples_per_dataset = X_train_1.shape[1]\n",
    "n_features=X_train_1.shape[2]\n",
    "encoded_ds_size=10\n",
    "hidden_size=5\n",
    "\n",
    "input_numeric = Input(shape=(n_samples_per_dataset, n_features), name='dataset')\n",
    "\n",
    "encoded_data = Dense(units=encoded_ds_size, kernel_initializer='normal', \n",
    "                     activation='relu', name='ds_dense')(input_numeric)\n",
    "encoded_data = GlobalAveragePooling1D()(encoded_data)\n",
    "encoded_data = Dense(hidden_size, kernel_initializer='normal', \n",
    "                     activation='relu', name='ds_avg_dense')(encoded_data)\n",
    "\n",
    "encoded_data = Dropout(0.2, name='perf_drop_dropout')(encoded_data)\n",
    "performance_drop = Dense(1, kernel_initializer='normal', name='perf_drop')(encoded_data)\n",
    "\n",
    "drop_predictor = KerasModel(inputs=input_numeric, outputs=performance_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MLP-based DNN Performance Drop Predictor...\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-20 15:56:25.069009: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - ETA: 0s - loss: 12.0173 - mean_absolute_error: 12.0173\n",
      "Epoch 00001: val_loss improved from inf to 4.77347, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 617us/sample - loss: 12.0173 - mean_absolute_error: 12.0173 - val_loss: 4.7735 - val_mean_absolute_error: 4.7735\n",
      "Epoch 2/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 11.5896 - mean_absolute_error: 11.5896\n",
      "Epoch 00002: val_loss improved from 4.77347 to 4.37688, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 75us/sample - loss: 11.5896 - mean_absolute_error: 11.5896 - val_loss: 4.3769 - val_mean_absolute_error: 4.3769\n",
      "Epoch 3/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 9.7919 - mean_absolute_error: 9.7919\n",
      "Epoch 00003: val_loss improved from 4.37688 to 4.00265, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 72us/sample - loss: 9.7919 - mean_absolute_error: 9.7919 - val_loss: 4.0027 - val_mean_absolute_error: 4.0027\n",
      "Epoch 4/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 8.3667 - mean_absolute_error: 8.3667\n",
      "Epoch 00004: val_loss improved from 4.00265 to 3.66796, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 80us/sample - loss: 8.3667 - mean_absolute_error: 8.3667 - val_loss: 3.6680 - val_mean_absolute_error: 3.6680\n",
      "Epoch 5/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 7.0042 - mean_absolute_error: 7.0042"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smaggio/workspace/dnn_performance_drop_prediction_under_drift/env_dnn_performance_drop_prediction/lib/python3.7/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: val_loss improved from 3.66796 to 3.36697, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 76us/sample - loss: 7.0042 - mean_absolute_error: 7.0042 - val_loss: 3.3670 - val_mean_absolute_error: 3.3670\n",
      "Epoch 6/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 7.2709 - mean_absolute_error: 7.2709\n",
      "Epoch 00006: val_loss improved from 3.36697 to 3.08046, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 7.2709 - mean_absolute_error: 7.2709 - val_loss: 3.0805 - val_mean_absolute_error: 3.0805\n",
      "Epoch 7/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 7.1247 - mean_absolute_error: 7.1247\n",
      "Epoch 00007: val_loss improved from 3.08046 to 2.80730, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 191us/sample - loss: 7.1247 - mean_absolute_error: 7.1247 - val_loss: 2.8073 - val_mean_absolute_error: 2.8073\n",
      "Epoch 8/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 6.0412 - mean_absolute_error: 6.0412\n",
      "Epoch 00008: val_loss improved from 2.80730 to 2.54894, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 95us/sample - loss: 6.0412 - mean_absolute_error: 6.0412 - val_loss: 2.5489 - val_mean_absolute_error: 2.5489\n",
      "Epoch 9/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 4.9254 - mean_absolute_error: 4.9254\n",
      "Epoch 00009: val_loss improved from 2.54894 to 2.30664, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 77us/sample - loss: 4.9254 - mean_absolute_error: 4.9254 - val_loss: 2.3066 - val_mean_absolute_error: 2.3066\n",
      "Epoch 10/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 5.6538 - mean_absolute_error: 5.6538\n",
      "Epoch 00010: val_loss improved from 2.30664 to 2.07572, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 117us/sample - loss: 5.6538 - mean_absolute_error: 5.6538 - val_loss: 2.0757 - val_mean_absolute_error: 2.0757\n",
      "Epoch 11/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 3.7122 - mean_absolute_error: 3.7122\n",
      "Epoch 00011: val_loss improved from 2.07572 to 1.86055, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 138us/sample - loss: 3.7122 - mean_absolute_error: 3.7122 - val_loss: 1.8605 - val_mean_absolute_error: 1.8605\n",
      "Epoch 12/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 3.8609 - mean_absolute_error: 3.8609\n",
      "Epoch 00012: val_loss improved from 1.86055 to 1.65746, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 79us/sample - loss: 3.8609 - mean_absolute_error: 3.8609 - val_loss: 1.6575 - val_mean_absolute_error: 1.6575\n",
      "Epoch 13/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 3.4755 - mean_absolute_error: 3.4755\n",
      "Epoch 00013: val_loss improved from 1.65746 to 1.46531, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 77us/sample - loss: 3.4755 - mean_absolute_error: 3.4755 - val_loss: 1.4653 - val_mean_absolute_error: 1.4653\n",
      "Epoch 14/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 3.3683 - mean_absolute_error: 3.3683\n",
      "Epoch 00014: val_loss improved from 1.46531 to 1.28216, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 79us/sample - loss: 3.3683 - mean_absolute_error: 3.3683 - val_loss: 1.2822 - val_mean_absolute_error: 1.2822\n",
      "Epoch 15/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 3.0463 - mean_absolute_error: 3.0463\n",
      "Epoch 00015: val_loss improved from 1.28216 to 1.10685, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 75us/sample - loss: 3.0463 - mean_absolute_error: 3.0463 - val_loss: 1.1068 - val_mean_absolute_error: 1.1068\n",
      "Epoch 16/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 2.4136 - mean_absolute_error: 2.4136\n",
      "Epoch 00016: val_loss improved from 1.10685 to 0.93997, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 76us/sample - loss: 2.4136 - mean_absolute_error: 2.4136 - val_loss: 0.9400 - val_mean_absolute_error: 0.9400\n",
      "Epoch 17/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 2.2513 - mean_absolute_error: 2.2513\n",
      "Epoch 00017: val_loss improved from 0.93997 to 0.77991, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 78us/sample - loss: 2.2513 - mean_absolute_error: 2.2513 - val_loss: 0.7799 - val_mean_absolute_error: 0.7799\n",
      "Epoch 18/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 1.6496 - mean_absolute_error: 1.6496\n",
      "Epoch 00018: val_loss improved from 0.77991 to 0.62753, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 81us/sample - loss: 1.6496 - mean_absolute_error: 1.6496 - val_loss: 0.6275 - val_mean_absolute_error: 0.6275\n",
      "Epoch 19/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 1.2342 - mean_absolute_error: 1.2342\n",
      "Epoch 00019: val_loss improved from 0.62753 to 0.48329, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 85us/sample - loss: 1.2342 - mean_absolute_error: 1.2342 - val_loss: 0.4833 - val_mean_absolute_error: 0.4833\n",
      "Epoch 20/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.9384 - mean_absolute_error: 0.9384\n",
      "Epoch 00020: val_loss improved from 0.48329 to 0.34797, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 85us/sample - loss: 0.9384 - mean_absolute_error: 0.9384 - val_loss: 0.3480 - val_mean_absolute_error: 0.3480\n",
      "Epoch 21/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.7242 - mean_absolute_error: 0.7242\n",
      "Epoch 00021: val_loss improved from 0.34797 to 0.21808, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 86us/sample - loss: 0.7242 - mean_absolute_error: 0.7242 - val_loss: 0.2181 - val_mean_absolute_error: 0.2181\n",
      "Epoch 22/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.3769 - mean_absolute_error: 0.3769\n",
      "Epoch 00022: val_loss improved from 0.21808 to 0.09522, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 95us/sample - loss: 0.3769 - mean_absolute_error: 0.3769 - val_loss: 0.0952 - val_mean_absolute_error: 0.0952\n",
      "Epoch 23/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.1624 - mean_absolute_error: 0.1624\n",
      "Epoch 00023: val_loss improved from 0.09522 to 0.04332, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 81us/sample - loss: 0.1624 - mean_absolute_error: 0.1624 - val_loss: 0.0433 - val_mean_absolute_error: 0.0433\n",
      "Epoch 24/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0394 - mean_absolute_error: 0.0394\n",
      "Epoch 00024: val_loss improved from 0.04332 to 0.04307, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 83us/sample - loss: 0.0394 - mean_absolute_error: 0.0394 - val_loss: 0.0431 - val_mean_absolute_error: 0.0431\n",
      "Epoch 25/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0392 - mean_absolute_error: 0.0392\n",
      "Epoch 00025: val_loss improved from 0.04307 to 0.04279, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 76us/sample - loss: 0.0392 - mean_absolute_error: 0.0392 - val_loss: 0.0428 - val_mean_absolute_error: 0.0428\n",
      "Epoch 26/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0389 - mean_absolute_error: 0.0389\n",
      "Epoch 00026: val_loss improved from 0.04279 to 0.04250, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 76us/sample - loss: 0.0389 - mean_absolute_error: 0.0389 - val_loss: 0.0425 - val_mean_absolute_error: 0.0425\n",
      "Epoch 27/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0385 - mean_absolute_error: 0.0385\n",
      "Epoch 00027: val_loss did not improve from 0.04250\n",
      "400/400 [==============================] - 0s 35us/sample - loss: 0.0385 - mean_absolute_error: 0.0385 - val_loss: 0.0427 - val_mean_absolute_error: 0.0427\n",
      "Epoch 28/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0435 - mean_absolute_error: 0.0435\n",
      "Epoch 00028: val_loss did not improve from 0.04250\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0435 - mean_absolute_error: 0.0435 - val_loss: 0.0434 - val_mean_absolute_error: 0.0434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0478 - mean_absolute_error: 0.0478\n",
      "Epoch 00029: val_loss did not improve from 0.04250\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0478 - mean_absolute_error: 0.0478 - val_loss: 0.0442 - val_mean_absolute_error: 0.0442\n",
      "Epoch 30/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0499 - mean_absolute_error: 0.0499\n",
      "Epoch 00030: val_loss did not improve from 0.04250\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0499 - mean_absolute_error: 0.0499 - val_loss: 0.0436 - val_mean_absolute_error: 0.0436\n",
      "Epoch 31/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0483 - mean_absolute_error: 0.0483\n",
      "Epoch 00031: val_loss improved from 0.04250 to 0.04233, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 71us/sample - loss: 0.0483 - mean_absolute_error: 0.0483 - val_loss: 0.0423 - val_mean_absolute_error: 0.0423\n",
      "Epoch 32/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0436 - mean_absolute_error: 0.0436\n",
      "Epoch 00032: val_loss improved from 0.04233 to 0.04137, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 77us/sample - loss: 0.0436 - mean_absolute_error: 0.0436 - val_loss: 0.0414 - val_mean_absolute_error: 0.0414\n",
      "Epoch 33/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0404 - mean_absolute_error: 0.0404\n",
      "Epoch 00033: val_loss improved from 0.04137 to 0.04095, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 68us/sample - loss: 0.0404 - mean_absolute_error: 0.0404 - val_loss: 0.0410 - val_mean_absolute_error: 0.0410\n",
      "Epoch 34/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0380 - mean_absolute_error: 0.0380\n",
      "Epoch 00034: val_loss improved from 0.04095 to 0.04074, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 73us/sample - loss: 0.0380 - mean_absolute_error: 0.0380 - val_loss: 0.0407 - val_mean_absolute_error: 0.0407\n",
      "Epoch 35/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0366 - mean_absolute_error: 0.0366\n",
      "Epoch 00035: val_loss improved from 0.04074 to 0.04064, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 75us/sample - loss: 0.0366 - mean_absolute_error: 0.0366 - val_loss: 0.0406 - val_mean_absolute_error: 0.0406\n",
      "Epoch 36/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00036: val_loss improved from 0.04064 to 0.04059, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 72us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0406 - val_mean_absolute_error: 0.0406\n",
      "Epoch 37/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0364 - mean_absolute_error: 0.0364\n",
      "Epoch 00037: val_loss improved from 0.04059 to 0.04055, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 72us/sample - loss: 0.0364 - mean_absolute_error: 0.0364 - val_loss: 0.0405 - val_mean_absolute_error: 0.0405\n",
      "Epoch 38/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0364 - mean_absolute_error: 0.0364\n",
      "Epoch 00038: val_loss improved from 0.04055 to 0.04048, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 75us/sample - loss: 0.0364 - mean_absolute_error: 0.0364 - val_loss: 0.0405 - val_mean_absolute_error: 0.0405\n",
      "Epoch 39/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0364 - mean_absolute_error: 0.0364\n",
      "Epoch 00039: val_loss improved from 0.04048 to 0.04046, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 75us/sample - loss: 0.0364 - mean_absolute_error: 0.0364 - val_loss: 0.0405 - val_mean_absolute_error: 0.0405\n",
      "Epoch 40/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0363 - mean_absolute_error: 0.0363\n",
      "Epoch 00040: val_loss did not improve from 0.04046\n",
      "400/400 [==============================] - 0s 26us/sample - loss: 0.0363 - mean_absolute_error: 0.0363 - val_loss: 0.0405 - val_mean_absolute_error: 0.0405\n",
      "Epoch 41/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0363 - mean_absolute_error: 0.0363\n",
      "Epoch 00041: val_loss did not improve from 0.04046\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0363 - mean_absolute_error: 0.0363 - val_loss: 0.0405 - val_mean_absolute_error: 0.0405\n",
      "Epoch 42/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0363 - mean_absolute_error: 0.0363\n",
      "Epoch 00042: val_loss did not improve from 0.04046\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0363 - mean_absolute_error: 0.0363 - val_loss: 0.0405 - val_mean_absolute_error: 0.0405\n",
      "Epoch 43/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0363 - mean_absolute_error: 0.0363\n",
      "Epoch 00043: val_loss improved from 0.04046 to 0.04045, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 71us/sample - loss: 0.0363 - mean_absolute_error: 0.0363 - val_loss: 0.0404 - val_mean_absolute_error: 0.0404\n",
      "Epoch 44/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0363 - mean_absolute_error: 0.0363\n",
      "Epoch 00044: val_loss improved from 0.04045 to 0.04044, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 74us/sample - loss: 0.0363 - mean_absolute_error: 0.0363 - val_loss: 0.0404 - val_mean_absolute_error: 0.0404\n",
      "Epoch 45/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00045: val_loss did not improve from 0.04044\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0404 - val_mean_absolute_error: 0.0404\n",
      "Epoch 46/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0363 - mean_absolute_error: 0.0363\n",
      "Epoch 00046: val_loss improved from 0.04044 to 0.04042, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 71us/sample - loss: 0.0363 - mean_absolute_error: 0.0363 - val_loss: 0.0404 - val_mean_absolute_error: 0.0404\n",
      "Epoch 47/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0363 - mean_absolute_error: 0.0363\n",
      "Epoch 00047: val_loss improved from 0.04042 to 0.04040, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 85us/sample - loss: 0.0363 - mean_absolute_error: 0.0363 - val_loss: 0.0404 - val_mean_absolute_error: 0.0404\n",
      "Epoch 48/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00048: val_loss improved from 0.04040 to 0.04038, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 79us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0404 - val_mean_absolute_error: 0.0404\n",
      "Epoch 49/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00049: val_loss improved from 0.04038 to 0.04034, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 72us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0403 - val_mean_absolute_error: 0.0403\n",
      "Epoch 50/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00050: val_loss improved from 0.04034 to 0.04030, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 71us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0403 - val_mean_absolute_error: 0.0403\n",
      "Epoch 51/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00051: val_loss improved from 0.04030 to 0.04023, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 70us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n",
      "Epoch 52/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00052: val_loss improved from 0.04023 to 0.04017, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 66us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00053: val_loss did not improve from 0.04017\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n",
      "Epoch 54/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0363 - mean_absolute_error: 0.0363\n",
      "Epoch 00054: val_loss did not improve from 0.04017\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0363 - mean_absolute_error: 0.0363 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n",
      "Epoch 55/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00055: val_loss did not improve from 0.04017\n",
      "400/400 [==============================] - 0s 34us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n",
      "Epoch 56/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00056: val_loss did not improve from 0.04017\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n",
      "Epoch 57/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00057: val_loss did not improve from 0.04017\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n",
      "Epoch 58/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00058: val_loss did not improve from 0.04017\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n",
      "Epoch 59/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00059: val_loss did not improve from 0.04017\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n",
      "Epoch 60/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00060: val_loss did not improve from 0.04017\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n",
      "Epoch 61/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00061: val_loss did not improve from 0.04017\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n",
      "Epoch 62/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00062: val_loss improved from 0.04017 to 0.04016, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 65us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n",
      "Epoch 63/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0363 - mean_absolute_error: 0.0363\n",
      "Epoch 00063: val_loss improved from 0.04016 to 0.04010, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 67us/sample - loss: 0.0363 - mean_absolute_error: 0.0363 - val_loss: 0.0401 - val_mean_absolute_error: 0.0401\n",
      "Epoch 64/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00064: val_loss did not improve from 0.04010\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n",
      "Epoch 65/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0361 - mean_absolute_error: 0.0361\n",
      "Epoch 00065: val_loss did not improve from 0.04010\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0361 - mean_absolute_error: 0.0361 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n",
      "Epoch 66/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0361 - mean_absolute_error: 0.0361\n",
      "Epoch 00066: val_loss did not improve from 0.04010\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0361 - mean_absolute_error: 0.0361 - val_loss: 0.0403 - val_mean_absolute_error: 0.0403\n",
      "Epoch 67/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0363 - mean_absolute_error: 0.0363\n",
      "Epoch 00067: val_loss did not improve from 0.04010\n",
      "400/400 [==============================] - 0s 26us/sample - loss: 0.0363 - mean_absolute_error: 0.0363 - val_loss: 0.0403 - val_mean_absolute_error: 0.0403\n",
      "Epoch 68/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0363 - mean_absolute_error: 0.0363\n",
      "Epoch 00068: val_loss did not improve from 0.04010\n",
      "400/400 [==============================] - 0s 25us/sample - loss: 0.0363 - mean_absolute_error: 0.0363 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n",
      "Epoch 69/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00069: val_loss did not improve from 0.04010\n",
      "400/400 [==============================] - 0s 26us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0401 - val_mean_absolute_error: 0.0401\n",
      "Epoch 70/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00070: val_loss improved from 0.04010 to 0.04004, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 79us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0400 - val_mean_absolute_error: 0.0400\n",
      "Epoch 71/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0361 - mean_absolute_error: 0.0361\n",
      "Epoch 00071: val_loss improved from 0.04004 to 0.03999, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 77us/sample - loss: 0.0361 - mean_absolute_error: 0.0361 - val_loss: 0.0400 - val_mean_absolute_error: 0.0400\n",
      "Epoch 72/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00072: val_loss improved from 0.03999 to 0.03995, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 72us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0400 - val_mean_absolute_error: 0.0400\n",
      "Epoch 73/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0361 - mean_absolute_error: 0.0361\n",
      "Epoch 00073: val_loss improved from 0.03995 to 0.03991, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 73us/sample - loss: 0.0361 - mean_absolute_error: 0.0361 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 74/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00074: val_loss improved from 0.03991 to 0.03990, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 69us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 75/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0361 - mean_absolute_error: 0.0361\n",
      "Epoch 00075: val_loss did not improve from 0.03990\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0361 - mean_absolute_error: 0.0361 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 76/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0361 - mean_absolute_error: 0.0361\n",
      "Epoch 00076: val_loss did not improve from 0.03990\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0361 - mean_absolute_error: 0.0361 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 77/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00077: val_loss did not improve from 0.03990\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 78/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0361 - mean_absolute_error: 0.0361\n",
      "Epoch 00078: val_loss did not improve from 0.03990\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0361 - mean_absolute_error: 0.0361 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 79/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00079: val_loss did not improve from 0.03990\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 80/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00080: val_loss did not improve from 0.03990\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 81/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00081: val_loss did not improve from 0.03990\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 82/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00082: val_loss did not improve from 0.03990\n",
      "400/400 [==============================] - 0s 26us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 83/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00083: val_loss improved from 0.03990 to 0.03990, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 71us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 84/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00084: val_loss improved from 0.03990 to 0.03989, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 71us/sample - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 85/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00085: val_loss improved from 0.03989 to 0.03987, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 70us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 86/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00086: val_loss improved from 0.03987 to 0.03986, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 71us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 87/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00087: val_loss improved from 0.03986 to 0.03984, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 72us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0398 - val_mean_absolute_error: 0.0398\n",
      "Epoch 88/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00088: val_loss improved from 0.03984 to 0.03983, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 69us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0398 - val_mean_absolute_error: 0.0398\n",
      "Epoch 89/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00089: val_loss improved from 0.03983 to 0.03981, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 74us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0398 - val_mean_absolute_error: 0.0398\n",
      "Epoch 90/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00090: val_loss improved from 0.03981 to 0.03979, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 72us/sample - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0398 - val_mean_absolute_error: 0.0398\n",
      "Epoch 91/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00091: val_loss improved from 0.03979 to 0.03977, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 72us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0398 - val_mean_absolute_error: 0.0398\n",
      "Epoch 92/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00092: val_loss improved from 0.03977 to 0.03975, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 68us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0398 - val_mean_absolute_error: 0.0398\n",
      "Epoch 93/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00093: val_loss improved from 0.03975 to 0.03973, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 70us/sample - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0397 - val_mean_absolute_error: 0.0397\n",
      "Epoch 94/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00094: val_loss improved from 0.03973 to 0.03971, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 72us/sample - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0397 - val_mean_absolute_error: 0.0397\n",
      "Epoch 95/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00095: val_loss improved from 0.03971 to 0.03968, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 71us/sample - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0397 - val_mean_absolute_error: 0.0397\n",
      "Epoch 96/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00096: val_loss improved from 0.03968 to 0.03966, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 70us/sample - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0397 - val_mean_absolute_error: 0.0397\n",
      "Epoch 97/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00097: val_loss improved from 0.03966 to 0.03966, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0397 - val_mean_absolute_error: 0.0397\n",
      "Epoch 98/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00098: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0397 - val_mean_absolute_error: 0.0397\n",
      "Epoch 99/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00099: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0397 - val_mean_absolute_error: 0.0397\n",
      "Epoch 100/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00100: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0397 - val_mean_absolute_error: 0.0397\n",
      "Epoch 101/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00101: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0397 - val_mean_absolute_error: 0.0397\n",
      "Epoch 102/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00102: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0398 - val_mean_absolute_error: 0.0398\n",
      "Epoch 103/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00103: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0398 - val_mean_absolute_error: 0.0398\n",
      "Epoch 104/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00104: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0398 - val_mean_absolute_error: 0.0398\n",
      "Epoch 105/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00105: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 25us/sample - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0398 - val_mean_absolute_error: 0.0398\n",
      "Epoch 106/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00106: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 107/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00107: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 25us/sample - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 108/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00108: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 109/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00109: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 26us/sample - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 110/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00110: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0400 - val_mean_absolute_error: 0.0400\n",
      "Epoch 111/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0357 - mean_absolute_error: 0.0357\n",
      "Epoch 00111: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0357 - mean_absolute_error: 0.0357 - val_loss: 0.0400 - val_mean_absolute_error: 0.0400\n",
      "Epoch 112/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00112: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0400 - val_mean_absolute_error: 0.0400\n",
      "Epoch 113/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0357 - mean_absolute_error: 0.0357\n",
      "Epoch 00113: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0357 - mean_absolute_error: 0.0357 - val_loss: 0.0400 - val_mean_absolute_error: 0.0400\n",
      "Epoch 114/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00114: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0401 - val_mean_absolute_error: 0.0401\n",
      "Epoch 115/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00115: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 33us/sample - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n",
      "Epoch 116/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00116: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0403 - val_mean_absolute_error: 0.0403\n",
      "Epoch 117/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00117: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0404 - val_mean_absolute_error: 0.0404\n",
      "Epoch 118/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0357 - mean_absolute_error: 0.0357\n",
      "Epoch 00118: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0357 - mean_absolute_error: 0.0357 - val_loss: 0.0405 - val_mean_absolute_error: 0.0405\n",
      "Epoch 119/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00119: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 33us/sample - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0406 - val_mean_absolute_error: 0.0406\n",
      "Epoch 120/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00120: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0408 - val_mean_absolute_error: 0.0408\n",
      "Epoch 121/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00121: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0409 - val_mean_absolute_error: 0.0409\n",
      "Epoch 122/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00122: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0411 - val_mean_absolute_error: 0.0411\n",
      "Epoch 123/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0357 - mean_absolute_error: 0.0357\n",
      "Epoch 00123: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0357 - mean_absolute_error: 0.0357 - val_loss: 0.0413 - val_mean_absolute_error: 0.0413\n",
      "Epoch 124/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00124: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0413 - val_mean_absolute_error: 0.0413\n",
      "Epoch 125/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00125: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0413 - val_mean_absolute_error: 0.0413\n",
      "Epoch 126/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00126: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0413 - val_mean_absolute_error: 0.0413\n",
      "Epoch 127/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0357 - mean_absolute_error: 0.0357\n",
      "Epoch 00127: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 33us/sample - loss: 0.0357 - mean_absolute_error: 0.0357 - val_loss: 0.0413 - val_mean_absolute_error: 0.0413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00128: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0413 - val_mean_absolute_error: 0.0413\n",
      "Epoch 129/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00129: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0414 - val_mean_absolute_error: 0.0414\n",
      "Epoch 130/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0357 - mean_absolute_error: 0.0357\n",
      "Epoch 00130: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 35us/sample - loss: 0.0357 - mean_absolute_error: 0.0357 - val_loss: 0.0415 - val_mean_absolute_error: 0.0415\n",
      "Epoch 131/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0357 - mean_absolute_error: 0.0357\n",
      "Epoch 00131: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 37us/sample - loss: 0.0357 - mean_absolute_error: 0.0357 - val_loss: 0.0416 - val_mean_absolute_error: 0.0416\n",
      "Epoch 132/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0357 - mean_absolute_error: 0.0357\n",
      "Epoch 00132: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0357 - mean_absolute_error: 0.0357 - val_loss: 0.0417 - val_mean_absolute_error: 0.0417\n",
      "Epoch 133/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0357 - mean_absolute_error: 0.0357\n",
      "Epoch 00133: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0357 - mean_absolute_error: 0.0357 - val_loss: 0.0418 - val_mean_absolute_error: 0.0418\n",
      "Epoch 134/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00134: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0419 - val_mean_absolute_error: 0.0419\n",
      "Epoch 135/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00135: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0420 - val_mean_absolute_error: 0.0420\n",
      "Epoch 136/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00136: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0422 - val_mean_absolute_error: 0.0422\n",
      "Epoch 137/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00137: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0424 - val_mean_absolute_error: 0.0424\n",
      "Epoch 138/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00138: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0425 - val_mean_absolute_error: 0.0425\n",
      "Epoch 139/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00139: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0427 - val_mean_absolute_error: 0.0427\n",
      "Epoch 140/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00140: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0429 - val_mean_absolute_error: 0.0429\n",
      "Epoch 141/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00141: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 33us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0429 - val_mean_absolute_error: 0.0429\n",
      "Epoch 142/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00142: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0429 - val_mean_absolute_error: 0.0429\n",
      "Epoch 143/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00143: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0429 - val_mean_absolute_error: 0.0429\n",
      "Epoch 144/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00144: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0430 - val_mean_absolute_error: 0.0430\n",
      "Epoch 145/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00145: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0431 - val_mean_absolute_error: 0.0431\n",
      "Epoch 146/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00146: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0432 - val_mean_absolute_error: 0.0432\n",
      "Epoch 147/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0357 - mean_absolute_error: 0.0357\n",
      "Epoch 00147: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0357 - mean_absolute_error: 0.0357 - val_loss: 0.0433 - val_mean_absolute_error: 0.0433\n",
      "Epoch 148/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00148: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 26us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0433 - val_mean_absolute_error: 0.0433\n",
      "Epoch 149/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00149: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0433 - val_mean_absolute_error: 0.0433\n",
      "Epoch 150/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00150: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0434 - val_mean_absolute_error: 0.0434\n",
      "Epoch 151/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00151: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0435 - val_mean_absolute_error: 0.0435\n",
      "Epoch 152/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00152: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0436 - val_mean_absolute_error: 0.0436\n",
      "Epoch 153/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00153: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0437 - val_mean_absolute_error: 0.0437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00154: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 33us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0439 - val_mean_absolute_error: 0.0439\n",
      "Epoch 155/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00155: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0440 - val_mean_absolute_error: 0.0440\n",
      "Epoch 156/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00156: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 33us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0440 - val_mean_absolute_error: 0.0440\n",
      "Epoch 157/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00157: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0440 - val_mean_absolute_error: 0.0440\n",
      "Epoch 158/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00158: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 38us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0441 - val_mean_absolute_error: 0.0441\n",
      "Epoch 159/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00159: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 33us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0441 - val_mean_absolute_error: 0.0441\n",
      "Epoch 160/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00160: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 37us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0441 - val_mean_absolute_error: 0.0441\n",
      "Epoch 161/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00161: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 39us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0441 - val_mean_absolute_error: 0.0441\n",
      "Epoch 162/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00162: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 35us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0441 - val_mean_absolute_error: 0.0441\n",
      "Epoch 163/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00163: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0441 - val_mean_absolute_error: 0.0441\n",
      "Epoch 164/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00164: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0441 - val_mean_absolute_error: 0.0441\n",
      "Epoch 165/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00165: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 26us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0440 - val_mean_absolute_error: 0.0440\n",
      "Epoch 166/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00166: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0441 - val_mean_absolute_error: 0.0441\n",
      "Epoch 167/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00167: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 26us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0441 - val_mean_absolute_error: 0.0441\n",
      "Epoch 168/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00168: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 24us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0442 - val_mean_absolute_error: 0.0442\n",
      "Epoch 169/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00169: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 25us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0442 - val_mean_absolute_error: 0.0442\n",
      "Epoch 170/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00170: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0443 - val_mean_absolute_error: 0.0443\n",
      "Epoch 171/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00171: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 25us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0444 - val_mean_absolute_error: 0.0444\n",
      "Epoch 172/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0357 - mean_absolute_error: 0.0357\n",
      "Epoch 00172: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 25us/sample - loss: 0.0357 - mean_absolute_error: 0.0357 - val_loss: 0.0444 - val_mean_absolute_error: 0.0444\n",
      "Epoch 173/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00173: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0444 - val_mean_absolute_error: 0.0444\n",
      "Epoch 174/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00174: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0444 - val_mean_absolute_error: 0.0444\n",
      "Epoch 175/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00175: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0444 - val_mean_absolute_error: 0.0444\n",
      "Epoch 176/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00176: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0444 - val_mean_absolute_error: 0.0444\n",
      "Epoch 177/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00177: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0444 - val_mean_absolute_error: 0.0444\n",
      "Epoch 178/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00178: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 25us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0444 - val_mean_absolute_error: 0.0444\n",
      "Epoch 179/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00179: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0445 - val_mean_absolute_error: 0.0445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00180: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0445 - val_mean_absolute_error: 0.0445\n",
      "Epoch 181/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00181: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 26us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0446 - val_mean_absolute_error: 0.0446\n",
      "Epoch 182/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00182: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 25us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0446 - val_mean_absolute_error: 0.0446\n",
      "Epoch 183/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00183: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0446 - val_mean_absolute_error: 0.0446\n",
      "Epoch 184/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00184: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 34us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0447 - val_mean_absolute_error: 0.0447\n",
      "Epoch 185/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00185: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 35us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0447 - val_mean_absolute_error: 0.0447\n",
      "Epoch 186/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00186: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 34us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0447 - val_mean_absolute_error: 0.0447\n",
      "Epoch 187/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00187: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 33us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0447 - val_mean_absolute_error: 0.0447\n",
      "Epoch 188/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00188: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0448 - val_mean_absolute_error: 0.0448\n",
      "Epoch 189/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00189: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0449 - val_mean_absolute_error: 0.0449\n",
      "Epoch 190/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00190: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0449 - val_mean_absolute_error: 0.0449\n",
      "Epoch 191/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00191: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 33us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0449 - val_mean_absolute_error: 0.0449\n",
      "Epoch 192/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00192: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 34us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0449 - val_mean_absolute_error: 0.0449\n",
      "Epoch 193/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00193: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0449 - val_mean_absolute_error: 0.0449\n",
      "Epoch 194/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00194: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0450 - val_mean_absolute_error: 0.0450\n",
      "Epoch 195/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00195: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 25us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0450 - val_mean_absolute_error: 0.0450\n",
      "Epoch 196/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00196: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0451 - val_mean_absolute_error: 0.0451\n",
      "Epoch 197/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00197: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 25us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0453 - val_mean_absolute_error: 0.0453\n",
      "Epoch 198/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00198: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0453 - val_mean_absolute_error: 0.0453\n",
      "Epoch 199/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00199: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 26us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0453 - val_mean_absolute_error: 0.0453\n",
      "Epoch 200/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00200: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0453 - val_mean_absolute_error: 0.0453\n",
      "Epoch 201/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00201: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0453 - val_mean_absolute_error: 0.0453\n",
      "Epoch 202/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00202: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 26us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0453 - val_mean_absolute_error: 0.0453\n",
      "Epoch 203/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00203: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0453 - val_mean_absolute_error: 0.0453\n",
      "Epoch 204/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00204: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0453 - val_mean_absolute_error: 0.0453\n",
      "Epoch 205/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00205: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0453 - val_mean_absolute_error: 0.0453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 206/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0350 - mean_absolute_error: 0.0350\n",
      "Epoch 00206: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0350 - mean_absolute_error: 0.0350 - val_loss: 0.0453 - val_mean_absolute_error: 0.0453\n",
      "Epoch 207/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00207: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0454 - val_mean_absolute_error: 0.0454\n",
      "Epoch 208/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00208: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0455 - val_mean_absolute_error: 0.0455\n",
      "Epoch 209/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00209: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0455 - val_mean_absolute_error: 0.0455\n",
      "Epoch 210/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00210: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456\n",
      "Epoch 211/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00211: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456\n",
      "Epoch 212/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00212: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456\n",
      "Epoch 213/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00213: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0455 - val_mean_absolute_error: 0.0455\n",
      "Epoch 214/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00214: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456\n",
      "Epoch 215/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00215: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456\n",
      "Epoch 216/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00216: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 33us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0457 - val_mean_absolute_error: 0.0457\n",
      "Epoch 217/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00217: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 33us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0458 - val_mean_absolute_error: 0.0458\n",
      "Epoch 218/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00218: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0458 - val_mean_absolute_error: 0.0458\n",
      "Epoch 219/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00219: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 33us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0459 - val_mean_absolute_error: 0.0459\n",
      "Epoch 220/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00220: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0459 - val_mean_absolute_error: 0.0459\n",
      "Epoch 221/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00221: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 35us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0459 - val_mean_absolute_error: 0.0459\n",
      "Epoch 222/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0349 - mean_absolute_error: 0.0349\n",
      "Epoch 00222: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0349 - mean_absolute_error: 0.0349 - val_loss: 0.0459 - val_mean_absolute_error: 0.0459\n",
      "Epoch 223/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00223: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 36us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0459 - val_mean_absolute_error: 0.0459\n",
      "Epoch 224/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00224: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 34us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0460 - val_mean_absolute_error: 0.0460\n",
      "Epoch 225/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00225: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0460 - val_mean_absolute_error: 0.0460\n",
      "Epoch 226/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00226: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0461 - val_mean_absolute_error: 0.0461\n",
      "Epoch 227/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00227: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 34us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0461 - val_mean_absolute_error: 0.0461\n",
      "Epoch 228/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00228: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0462 - val_mean_absolute_error: 0.0462\n",
      "Epoch 229/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00229: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0462 - val_mean_absolute_error: 0.0462\n",
      "Epoch 230/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00230: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0462 - val_mean_absolute_error: 0.0462\n",
      "Epoch 231/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00231: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0463 - val_mean_absolute_error: 0.0463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00232: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0463 - val_mean_absolute_error: 0.0463\n",
      "Epoch 233/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00233: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 34us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0464 - val_mean_absolute_error: 0.0464\n",
      "Epoch 234/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00234: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0464 - val_mean_absolute_error: 0.0464\n",
      "Epoch 235/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00235: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0464 - val_mean_absolute_error: 0.0464\n",
      "Epoch 236/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00236: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0465 - val_mean_absolute_error: 0.0465\n",
      "Epoch 237/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00237: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 34us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0466 - val_mean_absolute_error: 0.0466\n",
      "Epoch 238/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00238: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 33us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0467 - val_mean_absolute_error: 0.0467\n",
      "Epoch 239/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00239: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 33us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0468 - val_mean_absolute_error: 0.0468\n",
      "Epoch 240/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00240: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0468 - val_mean_absolute_error: 0.0468\n",
      "Epoch 241/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00241: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0468 - val_mean_absolute_error: 0.0468\n",
      "Epoch 242/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00242: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0467 - val_mean_absolute_error: 0.0467\n",
      "Epoch 243/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00243: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0468 - val_mean_absolute_error: 0.0468\n",
      "Epoch 244/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00244: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0468 - val_mean_absolute_error: 0.0468\n",
      "Epoch 245/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00245: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0469 - val_mean_absolute_error: 0.0469\n",
      "Epoch 246/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0350 - mean_absolute_error: 0.0350\n",
      "Epoch 00246: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0350 - mean_absolute_error: 0.0350 - val_loss: 0.0469 - val_mean_absolute_error: 0.0469\n",
      "Epoch 247/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0350 - mean_absolute_error: 0.0350\n",
      "Epoch 00247: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0350 - mean_absolute_error: 0.0350 - val_loss: 0.0469 - val_mean_absolute_error: 0.0469\n",
      "Epoch 248/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00248: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 25us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0469 - val_mean_absolute_error: 0.0469\n",
      "Epoch 249/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00249: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0469 - val_mean_absolute_error: 0.0469\n",
      "Epoch 250/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00250: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 26us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0470 - val_mean_absolute_error: 0.0470\n",
      "Epoch 251/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00251: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0470 - val_mean_absolute_error: 0.0470\n",
      "Epoch 252/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00252: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 26us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0471 - val_mean_absolute_error: 0.0471\n",
      "Epoch 253/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00253: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0472 - val_mean_absolute_error: 0.0472\n",
      "Epoch 254/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00254: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 26us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0473 - val_mean_absolute_error: 0.0473\n",
      "Epoch 255/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00255: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 26us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0473 - val_mean_absolute_error: 0.0473\n",
      "Epoch 256/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00256: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0473 - val_mean_absolute_error: 0.0473\n",
      "Epoch 257/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0350 - mean_absolute_error: 0.0350\n",
      "Epoch 00257: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 25us/sample - loss: 0.0350 - mean_absolute_error: 0.0350 - val_loss: 0.0473 - val_mean_absolute_error: 0.0473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 258/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00258: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0472 - val_mean_absolute_error: 0.0472\n",
      "Epoch 259/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0373 - mean_absolute_error: 0.0373\n",
      "Epoch 00259: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0373 - mean_absolute_error: 0.0373 - val_loss: 0.0468 - val_mean_absolute_error: 0.0468\n",
      "Epoch 260/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0350 - mean_absolute_error: 0.0350\n",
      "Epoch 00260: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0350 - mean_absolute_error: 0.0350 - val_loss: 0.0465 - val_mean_absolute_error: 0.0465\n",
      "Epoch 261/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00261: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 26us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0462 - val_mean_absolute_error: 0.0462\n",
      "Epoch 262/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00262: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 25us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0460 - val_mean_absolute_error: 0.0460\n",
      "Epoch 263/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00263: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0459 - val_mean_absolute_error: 0.0459\n",
      "Epoch 264/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00264: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 24us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0457 - val_mean_absolute_error: 0.0457\n",
      "Epoch 265/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00265: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456\n",
      "Epoch 266/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00266: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456\n",
      "Epoch 267/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00267: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 27us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0455 - val_mean_absolute_error: 0.0455\n",
      "Epoch 268/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00268: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0455 - val_mean_absolute_error: 0.0455\n",
      "Epoch 269/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00269: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 35us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0455 - val_mean_absolute_error: 0.0455\n",
      "Epoch 270/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00270: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 35us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0455 - val_mean_absolute_error: 0.0455\n",
      "Epoch 271/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00271: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 35us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0455 - val_mean_absolute_error: 0.0455\n",
      "Epoch 272/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00272: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456\n",
      "Epoch 273/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00273: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 33us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456\n",
      "Epoch 274/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00274: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456\n",
      "Epoch 275/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00275: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 32us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456\n",
      "Epoch 276/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00276: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 36us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456\n",
      "Epoch 277/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00277: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456\n",
      "Epoch 278/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00278: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456\n",
      "Epoch 279/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00279: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0457 - val_mean_absolute_error: 0.0457\n",
      "Epoch 280/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00280: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0458 - val_mean_absolute_error: 0.0458\n",
      "Epoch 281/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00281: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0458 - val_mean_absolute_error: 0.0458\n",
      "Epoch 282/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00282: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0458 - val_mean_absolute_error: 0.0458\n",
      "Epoch 283/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00283: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 28us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0458 - val_mean_absolute_error: 0.0458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00284: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 29us/sample - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0458 - val_mean_absolute_error: 0.0458\n",
      "Epoch 285/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00285: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0458 - val_mean_absolute_error: 0.0458\n",
      "Epoch 286/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00286: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0458 - val_mean_absolute_error: 0.0458\n",
      "Epoch 287/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00287: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 41us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0459 - val_mean_absolute_error: 0.0459\n",
      "Epoch 288/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00288: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 34us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0460 - val_mean_absolute_error: 0.0460\n",
      "Epoch 289/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00289: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 42us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0461 - val_mean_absolute_error: 0.0461\n",
      "Epoch 290/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00290: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 37us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0461 - val_mean_absolute_error: 0.0461\n",
      "Epoch 291/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00291: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 33us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0461 - val_mean_absolute_error: 0.0461\n",
      "Epoch 292/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00292: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 36us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0461 - val_mean_absolute_error: 0.0461\n",
      "Epoch 293/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00293: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 38us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0461 - val_mean_absolute_error: 0.0461\n",
      "Epoch 294/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00294: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 35us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0461 - val_mean_absolute_error: 0.0461\n",
      "Epoch 295/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00295: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 35us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0461 - val_mean_absolute_error: 0.0461\n",
      "Epoch 296/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00296: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 35us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0460 - val_mean_absolute_error: 0.0460\n",
      "Epoch 297/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00297: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 35us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0460 - val_mean_absolute_error: 0.0460\n",
      "Epoch 298/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00298: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 30us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0460 - val_mean_absolute_error: 0.0460\n",
      "Epoch 299/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00299: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 31us/sample - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0459 - val_mean_absolute_error: 0.0459\n",
      "Epoch 300/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00300: val_loss did not improve from 0.03966\n",
      "400/400 [==============================] - 0s 41us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0459 - val_mean_absolute_error: 0.0459\n",
      "WARNING:tensorflow:From /Users/smaggio/workspace/dnn_performance_drop_prediction_under_drift/env_dnn_performance_drop_prediction/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "print(\"Train MLP-based DNN Performance Drop Predictor...\")\n",
    "\n",
    "# fit the model\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "epochs=300\n",
    "batch_size=400\n",
    "validation_split=0.2\n",
    "verbose=True\n",
    "early_stop_patience=300\n",
    "lr=0.001\n",
    "    \n",
    "np.random.seed(random_state)\n",
    "set_random_seed(random_state)\n",
    "\n",
    "if os.path.exists('./mdc_net.h5'):\n",
    "            os.remove('./mdc_net.h5')\n",
    "\n",
    "drop_predictor.compile(loss='mean_absolute_error',\n",
    "              optimizer=Adam(lr),\n",
    "              metrics=['mean_absolute_error'])\n",
    "    \n",
    "callbacks = [ModelCheckpoint(filepath='./mdc_net.h5', monitor='val_loss', verbose=verbose, \n",
    "                             save_best_only=True, mode='min'),\n",
    "             EarlyStopping(monitor='val_loss', mode='min', verbose=verbose, \n",
    "                           patience=early_stop_patience)]\n",
    "\n",
    "history = drop_predictor.fit(\n",
    "    X_train_1, \n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=validation_split,\n",
    "    callbacks=callbacks,\n",
    "    verbose=verbose\n",
    ")\n",
    "\n",
    "drop_predictor = load_model('./mdc_net.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dataset (InputLayer)         [(None, 475, 9)]          0         \n",
      "_________________________________________________________________\n",
      "ds_dense (Dense)             (None, 475, 10)           100       \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "ds_avg_dense (Dense)         (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "perf_drop_dropout (Dropout)  (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "perf_drop (Dense)            (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 161\n",
      "Trainable params: 161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "drop_predictor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f899a682990>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure(figsize=(8, 8))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "\n",
    "plt.plot(history.history['mean_absolute_error'], label=\"Training Drop Error\")\n",
    "plt.plot(history.history['val_mean_absolute_error'], label=\"Validation Drop Error\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate MLP-based DNN and save results...\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate MLP-based DNN and save results...\")\n",
    "\n",
    "y_train_drops_pred = drop_predictor.predict(X_train_1)\n",
    "\n",
    "y_test_drops_pred = drop_predictor.predict(X_test_1)\n",
    "\n",
    "y_test_unseen_drops_pred = drop_predictor.predict(X_test_unseen_1)\n",
    "\n",
    "y_test_natural_drops_pred = drop_predictor.predict(X_test_natural_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.036598700372913955"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_train, y_train_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024681114791380736"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_test_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.049063117394173704"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test_unseen, y_test_unseen_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05585996493344244"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test_natural, y_test_natural_drops_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define LSTM-based DNN Performance Drop Predictor...\n",
      "WARNING:tensorflow:From /Users/smaggio/workspace/dnn_performance_drop_prediction_under_drift/env_dnn_performance_drop_prediction/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/smaggio/workspace/dnn_performance_drop_prediction_under_drift/env_dnn_performance_drop_prediction/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "print(\"Define LSTM-based DNN Performance Drop Predictor...\")\n",
    "\n",
    "# define LSTM based model\n",
    "\n",
    "n_samples_per_dataset = X_train_1.shape[1]\n",
    "n_features=X_train_1.shape[2]\n",
    "encoded_ds_size=10\n",
    "hidden_size=5\n",
    "\n",
    "input_shape = (n_samples_per_dataset, n_features)\n",
    "input_numeric = Input(shape=input_shape, name='dataset')\n",
    "\n",
    "encoded_data = Bidirectional(LSTM(units=encoded_ds_size, return_sequences=False,\n",
    "                             input_shape=input_shape), name='encoded_dataset')(input_numeric)\n",
    "\n",
    "encoded_data = Dropout(0.2, name='perf_drop_dropout')(encoded_data)\n",
    "performance_drop = Dense(1, kernel_initializer='normal', name='perf_drop')(encoded_data)\n",
    "\n",
    "drop_predictor = KerasModel(inputs=input_numeric, outputs=performance_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train LSTM-based DNN Performance Drop Predictor...\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.1078 - mean_absolute_error: 0.1078\n",
      "Epoch 00001: val_loss improved from inf to 0.10080, saving model to ./mdc_net.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smaggio/workspace/dnn_performance_drop_prediction_under_drift/env_dnn_performance_drop_prediction/lib/python3.7/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 2ms/sample - loss: 0.1078 - mean_absolute_error: 0.1078 - val_loss: 0.1008 - val_mean_absolute_error: 0.1008\n",
      "Epoch 2/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.1036 - mean_absolute_error: 0.1036\n",
      "Epoch 00002: val_loss improved from 0.10080 to 0.09627, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 651us/sample - loss: 0.1036 - mean_absolute_error: 0.1036 - val_loss: 0.0963 - val_mean_absolute_error: 0.0963\n",
      "Epoch 3/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0993 - mean_absolute_error: 0.0993\n",
      "Epoch 00003: val_loss improved from 0.09627 to 0.09202, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 627us/sample - loss: 0.0993 - mean_absolute_error: 0.0993 - val_loss: 0.0920 - val_mean_absolute_error: 0.0920\n",
      "Epoch 4/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0979 - mean_absolute_error: 0.0979\n",
      "Epoch 00004: val_loss improved from 0.09202 to 0.08806, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 604us/sample - loss: 0.0979 - mean_absolute_error: 0.0979 - val_loss: 0.0881 - val_mean_absolute_error: 0.0881\n",
      "Epoch 5/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0909 - mean_absolute_error: 0.0909\n",
      "Epoch 00005: val_loss improved from 0.08806 to 0.08423, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 622us/sample - loss: 0.0909 - mean_absolute_error: 0.0909 - val_loss: 0.0842 - val_mean_absolute_error: 0.0842\n",
      "Epoch 6/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0852 - mean_absolute_error: 0.0852\n",
      "Epoch 00006: val_loss improved from 0.08423 to 0.08060, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 685us/sample - loss: 0.0852 - mean_absolute_error: 0.0852 - val_loss: 0.0806 - val_mean_absolute_error: 0.0806\n",
      "Epoch 7/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0839 - mean_absolute_error: 0.0839\n",
      "Epoch 00007: val_loss improved from 0.08060 to 0.07699, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 665us/sample - loss: 0.0839 - mean_absolute_error: 0.0839 - val_loss: 0.0770 - val_mean_absolute_error: 0.0770\n",
      "Epoch 8/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0788 - mean_absolute_error: 0.0788\n",
      "Epoch 00008: val_loss improved from 0.07699 to 0.07340, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 725us/sample - loss: 0.0788 - mean_absolute_error: 0.0788 - val_loss: 0.0734 - val_mean_absolute_error: 0.0734\n",
      "Epoch 9/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0776 - mean_absolute_error: 0.0776\n",
      "Epoch 00009: val_loss improved from 0.07340 to 0.06993, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 736us/sample - loss: 0.0776 - mean_absolute_error: 0.0776 - val_loss: 0.0699 - val_mean_absolute_error: 0.0699\n",
      "Epoch 10/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0737 - mean_absolute_error: 0.0737\n",
      "Epoch 00010: val_loss improved from 0.06993 to 0.06667, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 740us/sample - loss: 0.0737 - mean_absolute_error: 0.0737 - val_loss: 0.0667 - val_mean_absolute_error: 0.0667\n",
      "Epoch 11/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0694 - mean_absolute_error: 0.0694\n",
      "Epoch 00011: val_loss improved from 0.06667 to 0.06378, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 746us/sample - loss: 0.0694 - mean_absolute_error: 0.0694 - val_loss: 0.0638 - val_mean_absolute_error: 0.0638\n",
      "Epoch 12/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0678 - mean_absolute_error: 0.0678\n",
      "Epoch 00012: val_loss improved from 0.06378 to 0.06132, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 753us/sample - loss: 0.0678 - mean_absolute_error: 0.0678 - val_loss: 0.0613 - val_mean_absolute_error: 0.0613\n",
      "Epoch 13/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0667 - mean_absolute_error: 0.0667\n",
      "Epoch 00013: val_loss improved from 0.06132 to 0.05906, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 760us/sample - loss: 0.0667 - mean_absolute_error: 0.0667 - val_loss: 0.0591 - val_mean_absolute_error: 0.0591\n",
      "Epoch 14/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0644 - mean_absolute_error: 0.0644\n",
      "Epoch 00014: val_loss improved from 0.05906 to 0.05717, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 760us/sample - loss: 0.0644 - mean_absolute_error: 0.0644 - val_loss: 0.0572 - val_mean_absolute_error: 0.0572\n",
      "Epoch 15/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0648 - mean_absolute_error: 0.0648\n",
      "Epoch 00015: val_loss improved from 0.05717 to 0.05571, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 749us/sample - loss: 0.0648 - mean_absolute_error: 0.0648 - val_loss: 0.0557 - val_mean_absolute_error: 0.0557\n",
      "Epoch 16/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0608 - mean_absolute_error: 0.0608\n",
      "Epoch 00016: val_loss improved from 0.05571 to 0.05456, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 746us/sample - loss: 0.0608 - mean_absolute_error: 0.0608 - val_loss: 0.0546 - val_mean_absolute_error: 0.0546\n",
      "Epoch 17/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0603 - mean_absolute_error: 0.0603\n",
      "Epoch 00017: val_loss improved from 0.05456 to 0.05349, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 761us/sample - loss: 0.0603 - mean_absolute_error: 0.0603 - val_loss: 0.0535 - val_mean_absolute_error: 0.0535\n",
      "Epoch 18/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0571 - mean_absolute_error: 0.0571\n",
      "Epoch 00018: val_loss improved from 0.05349 to 0.05260, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 737us/sample - loss: 0.0571 - mean_absolute_error: 0.0571 - val_loss: 0.0526 - val_mean_absolute_error: 0.0526\n",
      "Epoch 19/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0556 - mean_absolute_error: 0.0556\n",
      "Epoch 00019: val_loss improved from 0.05260 to 0.05180, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 740us/sample - loss: 0.0556 - mean_absolute_error: 0.0556 - val_loss: 0.0518 - val_mean_absolute_error: 0.0518\n",
      "Epoch 20/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0581 - mean_absolute_error: 0.0581\n",
      "Epoch 00020: val_loss improved from 0.05180 to 0.05104, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 770us/sample - loss: 0.0581 - mean_absolute_error: 0.0581 - val_loss: 0.0510 - val_mean_absolute_error: 0.0510\n",
      "Epoch 21/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0564 - mean_absolute_error: 0.0564\n",
      "Epoch 00021: val_loss improved from 0.05104 to 0.05041, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 747us/sample - loss: 0.0564 - mean_absolute_error: 0.0564 - val_loss: 0.0504 - val_mean_absolute_error: 0.0504\n",
      "Epoch 22/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0529 - mean_absolute_error: 0.0529\n",
      "Epoch 00022: val_loss improved from 0.05041 to 0.04995, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 700us/sample - loss: 0.0529 - mean_absolute_error: 0.0529 - val_loss: 0.0499 - val_mean_absolute_error: 0.0499\n",
      "Epoch 23/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0513 - mean_absolute_error: 0.0513\n",
      "Epoch 00023: val_loss improved from 0.04995 to 0.04951, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 726us/sample - loss: 0.0513 - mean_absolute_error: 0.0513 - val_loss: 0.0495 - val_mean_absolute_error: 0.0495\n",
      "Epoch 24/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0542 - mean_absolute_error: 0.0542\n",
      "Epoch 00024: val_loss improved from 0.04951 to 0.04906, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 684us/sample - loss: 0.0542 - mean_absolute_error: 0.0542 - val_loss: 0.0491 - val_mean_absolute_error: 0.0491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0501 - mean_absolute_error: 0.0501\n",
      "Epoch 00025: val_loss improved from 0.04906 to 0.04868, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 688us/sample - loss: 0.0501 - mean_absolute_error: 0.0501 - val_loss: 0.0487 - val_mean_absolute_error: 0.0487\n",
      "Epoch 26/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0521 - mean_absolute_error: 0.0521\n",
      "Epoch 00026: val_loss improved from 0.04868 to 0.04831, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 706us/sample - loss: 0.0521 - mean_absolute_error: 0.0521 - val_loss: 0.0483 - val_mean_absolute_error: 0.0483\n",
      "Epoch 27/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0487 - mean_absolute_error: 0.0487\n",
      "Epoch 00027: val_loss improved from 0.04831 to 0.04796, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 679us/sample - loss: 0.0487 - mean_absolute_error: 0.0487 - val_loss: 0.0480 - val_mean_absolute_error: 0.0480\n",
      "Epoch 28/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0508 - mean_absolute_error: 0.0508\n",
      "Epoch 00028: val_loss improved from 0.04796 to 0.04763, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 689us/sample - loss: 0.0508 - mean_absolute_error: 0.0508 - val_loss: 0.0476 - val_mean_absolute_error: 0.0476\n",
      "Epoch 29/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0484 - mean_absolute_error: 0.0484\n",
      "Epoch 00029: val_loss improved from 0.04763 to 0.04732, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 755us/sample - loss: 0.0484 - mean_absolute_error: 0.0484 - val_loss: 0.0473 - val_mean_absolute_error: 0.0473\n",
      "Epoch 30/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0483 - mean_absolute_error: 0.0483\n",
      "Epoch 00030: val_loss improved from 0.04732 to 0.04704, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 673us/sample - loss: 0.0483 - mean_absolute_error: 0.0483 - val_loss: 0.0470 - val_mean_absolute_error: 0.0470\n",
      "Epoch 31/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0495 - mean_absolute_error: 0.0495\n",
      "Epoch 00031: val_loss improved from 0.04704 to 0.04696, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 719us/sample - loss: 0.0495 - mean_absolute_error: 0.0495 - val_loss: 0.0470 - val_mean_absolute_error: 0.0470\n",
      "Epoch 32/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0455 - mean_absolute_error: 0.0455\n",
      "Epoch 00032: val_loss improved from 0.04696 to 0.04687, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 655us/sample - loss: 0.0455 - mean_absolute_error: 0.0455 - val_loss: 0.0469 - val_mean_absolute_error: 0.0469\n",
      "Epoch 33/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0473 - mean_absolute_error: 0.0473\n",
      "Epoch 00033: val_loss improved from 0.04687 to 0.04640, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 655us/sample - loss: 0.0473 - mean_absolute_error: 0.0473 - val_loss: 0.0464 - val_mean_absolute_error: 0.0464\n",
      "Epoch 34/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0450 - mean_absolute_error: 0.0450\n",
      "Epoch 00034: val_loss improved from 0.04640 to 0.04592, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 648us/sample - loss: 0.0450 - mean_absolute_error: 0.0450 - val_loss: 0.0459 - val_mean_absolute_error: 0.0459\n",
      "Epoch 35/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0461 - mean_absolute_error: 0.0461\n",
      "Epoch 00035: val_loss improved from 0.04592 to 0.04557, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 674us/sample - loss: 0.0461 - mean_absolute_error: 0.0461 - val_loss: 0.0456 - val_mean_absolute_error: 0.0456\n",
      "Epoch 36/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0445 - mean_absolute_error: 0.0445\n",
      "Epoch 00036: val_loss improved from 0.04557 to 0.04537, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 664us/sample - loss: 0.0445 - mean_absolute_error: 0.0445 - val_loss: 0.0454 - val_mean_absolute_error: 0.0454\n",
      "Epoch 37/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0458 - mean_absolute_error: 0.0458\n",
      "Epoch 00037: val_loss improved from 0.04537 to 0.04528, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 678us/sample - loss: 0.0458 - mean_absolute_error: 0.0458 - val_loss: 0.0453 - val_mean_absolute_error: 0.0453\n",
      "Epoch 38/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0451 - mean_absolute_error: 0.0451\n",
      "Epoch 00038: val_loss improved from 0.04528 to 0.04521, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 678us/sample - loss: 0.0451 - mean_absolute_error: 0.0451 - val_loss: 0.0452 - val_mean_absolute_error: 0.0452\n",
      "Epoch 39/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0444 - mean_absolute_error: 0.0444\n",
      "Epoch 00039: val_loss did not improve from 0.04521\n",
      "400/400 [==============================] - 0s 628us/sample - loss: 0.0444 - mean_absolute_error: 0.0444 - val_loss: 0.0452 - val_mean_absolute_error: 0.0452\n",
      "Epoch 40/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0449 - mean_absolute_error: 0.0449\n",
      "Epoch 00040: val_loss improved from 0.04521 to 0.04513, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 718us/sample - loss: 0.0449 - mean_absolute_error: 0.0449 - val_loss: 0.0451 - val_mean_absolute_error: 0.0451\n",
      "Epoch 41/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0451 - mean_absolute_error: 0.0451\n",
      "Epoch 00041: val_loss improved from 0.04513 to 0.04485, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 711us/sample - loss: 0.0451 - mean_absolute_error: 0.0451 - val_loss: 0.0449 - val_mean_absolute_error: 0.0449\n",
      "Epoch 42/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0425 - mean_absolute_error: 0.0425\n",
      "Epoch 00042: val_loss improved from 0.04485 to 0.04457, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 665us/sample - loss: 0.0425 - mean_absolute_error: 0.0425 - val_loss: 0.0446 - val_mean_absolute_error: 0.0446\n",
      "Epoch 43/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0450 - mean_absolute_error: 0.0450\n",
      "Epoch 00043: val_loss improved from 0.04457 to 0.04434, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 687us/sample - loss: 0.0450 - mean_absolute_error: 0.0450 - val_loss: 0.0443 - val_mean_absolute_error: 0.0443\n",
      "Epoch 44/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0410 - mean_absolute_error: 0.0410\n",
      "Epoch 00044: val_loss improved from 0.04434 to 0.04412, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 726us/sample - loss: 0.0410 - mean_absolute_error: 0.0410 - val_loss: 0.0441 - val_mean_absolute_error: 0.0441\n",
      "Epoch 45/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0421 - mean_absolute_error: 0.0421\n",
      "Epoch 00045: val_loss improved from 0.04412 to 0.04388, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 720us/sample - loss: 0.0421 - mean_absolute_error: 0.0421 - val_loss: 0.0439 - val_mean_absolute_error: 0.0439\n",
      "Epoch 46/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0433 - mean_absolute_error: 0.0433\n",
      "Epoch 00046: val_loss improved from 0.04388 to 0.04361, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 716us/sample - loss: 0.0433 - mean_absolute_error: 0.0433 - val_loss: 0.0436 - val_mean_absolute_error: 0.0436\n",
      "Epoch 47/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0430 - mean_absolute_error: 0.0430\n",
      "Epoch 00047: val_loss improved from 0.04361 to 0.04322, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 741us/sample - loss: 0.0430 - mean_absolute_error: 0.0430 - val_loss: 0.0432 - val_mean_absolute_error: 0.0432\n",
      "Epoch 48/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0432 - mean_absolute_error: 0.0432\n",
      "Epoch 00048: val_loss improved from 0.04322 to 0.04283, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 722us/sample - loss: 0.0432 - mean_absolute_error: 0.0432 - val_loss: 0.0428 - val_mean_absolute_error: 0.0428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0413 - mean_absolute_error: 0.0413\n",
      "Epoch 00049: val_loss improved from 0.04283 to 0.04246, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 767us/sample - loss: 0.0413 - mean_absolute_error: 0.0413 - val_loss: 0.0425 - val_mean_absolute_error: 0.0425\n",
      "Epoch 50/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0386 - mean_absolute_error: 0.0386\n",
      "Epoch 00050: val_loss improved from 0.04246 to 0.04208, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 699us/sample - loss: 0.0386 - mean_absolute_error: 0.0386 - val_loss: 0.0421 - val_mean_absolute_error: 0.0421\n",
      "Epoch 51/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0398 - mean_absolute_error: 0.0398\n",
      "Epoch 00051: val_loss improved from 0.04208 to 0.04191, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 720us/sample - loss: 0.0398 - mean_absolute_error: 0.0398 - val_loss: 0.0419 - val_mean_absolute_error: 0.0419\n",
      "Epoch 52/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0426 - mean_absolute_error: 0.0426\n",
      "Epoch 00052: val_loss improved from 0.04191 to 0.04184, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 727us/sample - loss: 0.0426 - mean_absolute_error: 0.0426 - val_loss: 0.0418 - val_mean_absolute_error: 0.0418\n",
      "Epoch 53/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0400 - mean_absolute_error: 0.0400\n",
      "Epoch 00053: val_loss improved from 0.04184 to 0.04180, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 702us/sample - loss: 0.0400 - mean_absolute_error: 0.0400 - val_loss: 0.0418 - val_mean_absolute_error: 0.0418\n",
      "Epoch 54/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0389 - mean_absolute_error: 0.0389\n",
      "Epoch 00054: val_loss did not improve from 0.04180\n",
      "400/400 [==============================] - 0s 626us/sample - loss: 0.0389 - mean_absolute_error: 0.0389 - val_loss: 0.0418 - val_mean_absolute_error: 0.0418\n",
      "Epoch 55/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0381 - mean_absolute_error: 0.0381\n",
      "Epoch 00055: val_loss did not improve from 0.04180\n",
      "400/400 [==============================] - 0s 638us/sample - loss: 0.0381 - mean_absolute_error: 0.0381 - val_loss: 0.0418 - val_mean_absolute_error: 0.0418\n",
      "Epoch 56/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0394 - mean_absolute_error: 0.0394\n",
      "Epoch 00056: val_loss did not improve from 0.04180\n",
      "400/400 [==============================] - 0s 652us/sample - loss: 0.0394 - mean_absolute_error: 0.0394 - val_loss: 0.0418 - val_mean_absolute_error: 0.0418\n",
      "Epoch 57/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0387 - mean_absolute_error: 0.0387\n",
      "Epoch 00057: val_loss did not improve from 0.04180\n",
      "400/400 [==============================] - 0s 661us/sample - loss: 0.0387 - mean_absolute_error: 0.0387 - val_loss: 0.0418 - val_mean_absolute_error: 0.0418\n",
      "Epoch 58/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0390 - mean_absolute_error: 0.0390\n",
      "Epoch 00058: val_loss did not improve from 0.04180\n",
      "400/400 [==============================] - 0s 625us/sample - loss: 0.0390 - mean_absolute_error: 0.0390 - val_loss: 0.0418 - val_mean_absolute_error: 0.0418\n",
      "Epoch 59/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0381 - mean_absolute_error: 0.0381\n",
      "Epoch 00059: val_loss did not improve from 0.04180\n",
      "400/400 [==============================] - 0s 653us/sample - loss: 0.0381 - mean_absolute_error: 0.0381 - val_loss: 0.0418 - val_mean_absolute_error: 0.0418\n",
      "Epoch 60/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0386 - mean_absolute_error: 0.0386\n",
      "Epoch 00060: val_loss did not improve from 0.04180\n",
      "400/400 [==============================] - 0s 630us/sample - loss: 0.0386 - mean_absolute_error: 0.0386 - val_loss: 0.0418 - val_mean_absolute_error: 0.0418\n",
      "Epoch 61/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00061: val_loss did not improve from 0.04180\n",
      "400/400 [==============================] - 0s 601us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0418 - val_mean_absolute_error: 0.0418\n",
      "Epoch 62/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00062: val_loss did not improve from 0.04180\n",
      "400/400 [==============================] - 0s 587us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0418 - val_mean_absolute_error: 0.0418\n",
      "Epoch 63/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00063: val_loss did not improve from 0.04180\n",
      "400/400 [==============================] - 0s 590us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0418 - val_mean_absolute_error: 0.0418\n",
      "Epoch 64/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0390 - mean_absolute_error: 0.0390\n",
      "Epoch 00064: val_loss improved from 0.04180 to 0.04178, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 665us/sample - loss: 0.0390 - mean_absolute_error: 0.0390 - val_loss: 0.0418 - val_mean_absolute_error: 0.0418\n",
      "Epoch 65/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00065: val_loss improved from 0.04178 to 0.04175, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 668us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0417 - val_mean_absolute_error: 0.0417\n",
      "Epoch 66/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0365 - mean_absolute_error: 0.0365\n",
      "Epoch 00066: val_loss improved from 0.04175 to 0.04171, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 667us/sample - loss: 0.0365 - mean_absolute_error: 0.0365 - val_loss: 0.0417 - val_mean_absolute_error: 0.0417\n",
      "Epoch 67/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0386 - mean_absolute_error: 0.0386\n",
      "Epoch 00067: val_loss improved from 0.04171 to 0.04167, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 666us/sample - loss: 0.0386 - mean_absolute_error: 0.0386 - val_loss: 0.0417 - val_mean_absolute_error: 0.0417\n",
      "Epoch 68/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00068: val_loss improved from 0.04167 to 0.04163, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 685us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0416 - val_mean_absolute_error: 0.0416\n",
      "Epoch 69/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0378 - mean_absolute_error: 0.0378\n",
      "Epoch 00069: val_loss improved from 0.04163 to 0.04159, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 676us/sample - loss: 0.0378 - mean_absolute_error: 0.0378 - val_loss: 0.0416 - val_mean_absolute_error: 0.0416\n",
      "Epoch 70/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0374 - mean_absolute_error: 0.0374\n",
      "Epoch 00070: val_loss improved from 0.04159 to 0.04156, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 665us/sample - loss: 0.0374 - mean_absolute_error: 0.0374 - val_loss: 0.0416 - val_mean_absolute_error: 0.0416\n",
      "Epoch 71/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0349 - mean_absolute_error: 0.0349\n",
      "Epoch 00071: val_loss improved from 0.04156 to 0.04152, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 673us/sample - loss: 0.0349 - mean_absolute_error: 0.0349 - val_loss: 0.0415 - val_mean_absolute_error: 0.0415\n",
      "Epoch 72/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0381 - mean_absolute_error: 0.0381\n",
      "Epoch 00072: val_loss improved from 0.04152 to 0.04149, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 692us/sample - loss: 0.0381 - mean_absolute_error: 0.0381 - val_loss: 0.0415 - val_mean_absolute_error: 0.0415\n",
      "Epoch 73/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00073: val_loss improved from 0.04149 to 0.04147, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 697us/sample - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0415 - val_mean_absolute_error: 0.0415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0357 - mean_absolute_error: 0.0357\n",
      "Epoch 00074: val_loss improved from 0.04147 to 0.04146, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 709us/sample - loss: 0.0357 - mean_absolute_error: 0.0357 - val_loss: 0.0415 - val_mean_absolute_error: 0.0415\n",
      "Epoch 75/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0364 - mean_absolute_error: 0.0364\n",
      "Epoch 00075: val_loss improved from 0.04146 to 0.04143, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 695us/sample - loss: 0.0364 - mean_absolute_error: 0.0364 - val_loss: 0.0414 - val_mean_absolute_error: 0.0414\n",
      "Epoch 76/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00076: val_loss improved from 0.04143 to 0.04140, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 692us/sample - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0414 - val_mean_absolute_error: 0.0414\n",
      "Epoch 77/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00077: val_loss improved from 0.04140 to 0.04136, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 766us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0414 - val_mean_absolute_error: 0.0414\n",
      "Epoch 78/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00078: val_loss improved from 0.04136 to 0.04132, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 679us/sample - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0413 - val_mean_absolute_error: 0.0413\n",
      "Epoch 79/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0366 - mean_absolute_error: 0.0366\n",
      "Epoch 00079: val_loss improved from 0.04132 to 0.04128, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 695us/sample - loss: 0.0366 - mean_absolute_error: 0.0366 - val_loss: 0.0413 - val_mean_absolute_error: 0.0413\n",
      "Epoch 80/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00080: val_loss improved from 0.04128 to 0.04123, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 758us/sample - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0412 - val_mean_absolute_error: 0.0412\n",
      "Epoch 81/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0343 - mean_absolute_error: 0.0343\n",
      "Epoch 00081: val_loss improved from 0.04123 to 0.04117, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 672us/sample - loss: 0.0343 - mean_absolute_error: 0.0343 - val_loss: 0.0412 - val_mean_absolute_error: 0.0412\n",
      "Epoch 82/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0339 - mean_absolute_error: 0.0339\n",
      "Epoch 00082: val_loss improved from 0.04117 to 0.04110, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 664us/sample - loss: 0.0339 - mean_absolute_error: 0.0339 - val_loss: 0.0411 - val_mean_absolute_error: 0.0411\n",
      "Epoch 83/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0344 - mean_absolute_error: 0.0344\n",
      "Epoch 00083: val_loss improved from 0.04110 to 0.04104, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 741us/sample - loss: 0.0344 - mean_absolute_error: 0.0344 - val_loss: 0.0410 - val_mean_absolute_error: 0.0410\n",
      "Epoch 84/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0360 - mean_absolute_error: 0.0360\n",
      "Epoch 00084: val_loss improved from 0.04104 to 0.04098, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 698us/sample - loss: 0.0360 - mean_absolute_error: 0.0360 - val_loss: 0.0410 - val_mean_absolute_error: 0.0410\n",
      "Epoch 85/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00085: val_loss improved from 0.04098 to 0.04091, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 698us/sample - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0409 - val_mean_absolute_error: 0.0409\n",
      "Epoch 86/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0331 - mean_absolute_error: 0.0331\n",
      "Epoch 00086: val_loss improved from 0.04091 to 0.04083, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 676us/sample - loss: 0.0331 - mean_absolute_error: 0.0331 - val_loss: 0.0408 - val_mean_absolute_error: 0.0408\n",
      "Epoch 87/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0335 - mean_absolute_error: 0.0335\n",
      "Epoch 00087: val_loss improved from 0.04083 to 0.04075, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 689us/sample - loss: 0.0335 - mean_absolute_error: 0.0335 - val_loss: 0.0408 - val_mean_absolute_error: 0.0408\n",
      "Epoch 88/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0341 - mean_absolute_error: 0.0341\n",
      "Epoch 00088: val_loss improved from 0.04075 to 0.04068, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 663us/sample - loss: 0.0341 - mean_absolute_error: 0.0341 - val_loss: 0.0407 - val_mean_absolute_error: 0.0407\n",
      "Epoch 89/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0350 - mean_absolute_error: 0.0350\n",
      "Epoch 00089: val_loss improved from 0.04068 to 0.04061, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 656us/sample - loss: 0.0350 - mean_absolute_error: 0.0350 - val_loss: 0.0406 - val_mean_absolute_error: 0.0406\n",
      "Epoch 90/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0340 - mean_absolute_error: 0.0340\n",
      "Epoch 00090: val_loss improved from 0.04061 to 0.04053, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 686us/sample - loss: 0.0340 - mean_absolute_error: 0.0340 - val_loss: 0.0405 - val_mean_absolute_error: 0.0405\n",
      "Epoch 91/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0357 - mean_absolute_error: 0.0357\n",
      "Epoch 00091: val_loss improved from 0.04053 to 0.04045, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 637us/sample - loss: 0.0357 - mean_absolute_error: 0.0357 - val_loss: 0.0404 - val_mean_absolute_error: 0.0404\n",
      "Epoch 92/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0361 - mean_absolute_error: 0.0361\n",
      "Epoch 00092: val_loss improved from 0.04045 to 0.04037, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 640us/sample - loss: 0.0361 - mean_absolute_error: 0.0361 - val_loss: 0.0404 - val_mean_absolute_error: 0.0404\n",
      "Epoch 93/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0333 - mean_absolute_error: 0.0333\n",
      "Epoch 00093: val_loss improved from 0.04037 to 0.04030, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 690us/sample - loss: 0.0333 - mean_absolute_error: 0.0333 - val_loss: 0.0403 - val_mean_absolute_error: 0.0403\n",
      "Epoch 94/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0333 - mean_absolute_error: 0.0333\n",
      "Epoch 00094: val_loss improved from 0.04030 to 0.04021, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 676us/sample - loss: 0.0333 - mean_absolute_error: 0.0333 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n",
      "Epoch 95/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0328 - mean_absolute_error: 0.0328\n",
      "Epoch 00095: val_loss improved from 0.04021 to 0.04015, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 646us/sample - loss: 0.0328 - mean_absolute_error: 0.0328 - val_loss: 0.0402 - val_mean_absolute_error: 0.0402\n",
      "Epoch 96/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0333 - mean_absolute_error: 0.0333\n",
      "Epoch 00096: val_loss improved from 0.04015 to 0.04009, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 631us/sample - loss: 0.0333 - mean_absolute_error: 0.0333 - val_loss: 0.0401 - val_mean_absolute_error: 0.0401\n",
      "Epoch 97/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0339 - mean_absolute_error: 0.0339\n",
      "Epoch 00097: val_loss improved from 0.04009 to 0.04002, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 643us/sample - loss: 0.0339 - mean_absolute_error: 0.0339 - val_loss: 0.0400 - val_mean_absolute_error: 0.0400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0341 - mean_absolute_error: 0.0341\n",
      "Epoch 00098: val_loss improved from 0.04002 to 0.03995, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 653us/sample - loss: 0.0341 - mean_absolute_error: 0.0341 - val_loss: 0.0400 - val_mean_absolute_error: 0.0400\n",
      "Epoch 99/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0328 - mean_absolute_error: 0.0328\n",
      "Epoch 00099: val_loss improved from 0.03995 to 0.03986, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 665us/sample - loss: 0.0328 - mean_absolute_error: 0.0328 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
      "Epoch 100/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0341 - mean_absolute_error: 0.0341\n",
      "Epoch 00100: val_loss improved from 0.03986 to 0.03978, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 691us/sample - loss: 0.0341 - mean_absolute_error: 0.0341 - val_loss: 0.0398 - val_mean_absolute_error: 0.0398\n",
      "Epoch 101/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0321 - mean_absolute_error: 0.0321\n",
      "Epoch 00101: val_loss improved from 0.03978 to 0.03970, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 650us/sample - loss: 0.0321 - mean_absolute_error: 0.0321 - val_loss: 0.0397 - val_mean_absolute_error: 0.0397\n",
      "Epoch 102/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0328 - mean_absolute_error: 0.0328\n",
      "Epoch 00102: val_loss improved from 0.03970 to 0.03962, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 664us/sample - loss: 0.0328 - mean_absolute_error: 0.0328 - val_loss: 0.0396 - val_mean_absolute_error: 0.0396\n",
      "Epoch 103/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0337 - mean_absolute_error: 0.0337\n",
      "Epoch 00103: val_loss improved from 0.03962 to 0.03957, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 673us/sample - loss: 0.0337 - mean_absolute_error: 0.0337 - val_loss: 0.0396 - val_mean_absolute_error: 0.0396\n",
      "Epoch 104/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0336 - mean_absolute_error: 0.0336\n",
      "Epoch 00104: val_loss improved from 0.03957 to 0.03951, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 674us/sample - loss: 0.0336 - mean_absolute_error: 0.0336 - val_loss: 0.0395 - val_mean_absolute_error: 0.0395\n",
      "Epoch 105/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0332 - mean_absolute_error: 0.0332\n",
      "Epoch 00105: val_loss improved from 0.03951 to 0.03946, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 709us/sample - loss: 0.0332 - mean_absolute_error: 0.0332 - val_loss: 0.0395 - val_mean_absolute_error: 0.0395\n",
      "Epoch 106/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0325 - mean_absolute_error: 0.0325\n",
      "Epoch 00106: val_loss improved from 0.03946 to 0.03942, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 719us/sample - loss: 0.0325 - mean_absolute_error: 0.0325 - val_loss: 0.0394 - val_mean_absolute_error: 0.0394\n",
      "Epoch 107/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0325 - mean_absolute_error: 0.0325\n",
      "Epoch 00107: val_loss improved from 0.03942 to 0.03940, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 725us/sample - loss: 0.0325 - mean_absolute_error: 0.0325 - val_loss: 0.0394 - val_mean_absolute_error: 0.0394\n",
      "Epoch 108/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0328 - mean_absolute_error: 0.0328\n",
      "Epoch 00108: val_loss improved from 0.03940 to 0.03938, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 723us/sample - loss: 0.0328 - mean_absolute_error: 0.0328 - val_loss: 0.0394 - val_mean_absolute_error: 0.0394\n",
      "Epoch 109/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0326 - mean_absolute_error: 0.0326\n",
      "Epoch 00109: val_loss improved from 0.03938 to 0.03937, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 718us/sample - loss: 0.0326 - mean_absolute_error: 0.0326 - val_loss: 0.0394 - val_mean_absolute_error: 0.0394\n",
      "Epoch 110/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0329 - mean_absolute_error: 0.0329\n",
      "Epoch 00110: val_loss improved from 0.03937 to 0.03935, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 697us/sample - loss: 0.0329 - mean_absolute_error: 0.0329 - val_loss: 0.0393 - val_mean_absolute_error: 0.0393\n",
      "Epoch 111/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0318\n",
      "Epoch 00111: val_loss improved from 0.03935 to 0.03931, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 726us/sample - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0393 - val_mean_absolute_error: 0.0393\n",
      "Epoch 112/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0325 - mean_absolute_error: 0.0325\n",
      "Epoch 00112: val_loss improved from 0.03931 to 0.03928, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 733us/sample - loss: 0.0325 - mean_absolute_error: 0.0325 - val_loss: 0.0393 - val_mean_absolute_error: 0.0393\n",
      "Epoch 113/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0322 - mean_absolute_error: 0.0322\n",
      "Epoch 00113: val_loss improved from 0.03928 to 0.03922, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 770us/sample - loss: 0.0322 - mean_absolute_error: 0.0322 - val_loss: 0.0392 - val_mean_absolute_error: 0.0392\n",
      "Epoch 114/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0318\n",
      "Epoch 00114: val_loss improved from 0.03922 to 0.03917, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 709us/sample - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0392 - val_mean_absolute_error: 0.0392\n",
      "Epoch 115/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0319 - mean_absolute_error: 0.0319\n",
      "Epoch 00115: val_loss improved from 0.03917 to 0.03910, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 707us/sample - loss: 0.0319 - mean_absolute_error: 0.0319 - val_loss: 0.0391 - val_mean_absolute_error: 0.0391\n",
      "Epoch 116/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0328 - mean_absolute_error: 0.0328\n",
      "Epoch 00116: val_loss improved from 0.03910 to 0.03902, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 717us/sample - loss: 0.0328 - mean_absolute_error: 0.0328 - val_loss: 0.0390 - val_mean_absolute_error: 0.0390\n",
      "Epoch 117/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0333 - mean_absolute_error: 0.0333\n",
      "Epoch 00117: val_loss improved from 0.03902 to 0.03895, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 714us/sample - loss: 0.0333 - mean_absolute_error: 0.0333 - val_loss: 0.0390 - val_mean_absolute_error: 0.0390\n",
      "Epoch 118/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0318\n",
      "Epoch 00118: val_loss improved from 0.03895 to 0.03888, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 678us/sample - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0389 - val_mean_absolute_error: 0.0389\n",
      "Epoch 119/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0321 - mean_absolute_error: 0.0321\n",
      "Epoch 00119: val_loss improved from 0.03888 to 0.03879, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 677us/sample - loss: 0.0321 - mean_absolute_error: 0.0321 - val_loss: 0.0388 - val_mean_absolute_error: 0.0388\n",
      "Epoch 120/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0326 - mean_absolute_error: 0.0326\n",
      "Epoch 00120: val_loss improved from 0.03879 to 0.03869, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 650us/sample - loss: 0.0326 - mean_absolute_error: 0.0326 - val_loss: 0.0387 - val_mean_absolute_error: 0.0387\n",
      "Epoch 121/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0326 - mean_absolute_error: 0.0326\n",
      "Epoch 00121: val_loss improved from 0.03869 to 0.03859, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 686us/sample - loss: 0.0326 - mean_absolute_error: 0.0326 - val_loss: 0.0386 - val_mean_absolute_error: 0.0386\n",
      "Epoch 122/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 00122: val_loss improved from 0.03859 to 0.03851, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 649us/sample - loss: 0.0316 - mean_absolute_error: 0.0316 - val_loss: 0.0385 - val_mean_absolute_error: 0.0385\n",
      "Epoch 123/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.0306\n",
      "Epoch 00123: val_loss improved from 0.03851 to 0.03844, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 771us/sample - loss: 0.0306 - mean_absolute_error: 0.0306 - val_loss: 0.0384 - val_mean_absolute_error: 0.0384\n",
      "Epoch 124/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.0306\n",
      "Epoch 00124: val_loss improved from 0.03844 to 0.03838, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 703us/sample - loss: 0.0306 - mean_absolute_error: 0.0306 - val_loss: 0.0384 - val_mean_absolute_error: 0.0384\n",
      "Epoch 125/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0331 - mean_absolute_error: 0.0331\n",
      "Epoch 00125: val_loss improved from 0.03838 to 0.03833, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 692us/sample - loss: 0.0331 - mean_absolute_error: 0.0331 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383\n",
      "Epoch 126/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0322 - mean_absolute_error: 0.0322\n",
      "Epoch 00126: val_loss improved from 0.03833 to 0.03831, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 747us/sample - loss: 0.0322 - mean_absolute_error: 0.0322 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383\n",
      "Epoch 127/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0328 - mean_absolute_error: 0.0328\n",
      "Epoch 00127: val_loss improved from 0.03831 to 0.03829, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 711us/sample - loss: 0.0328 - mean_absolute_error: 0.0328 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383\n",
      "Epoch 128/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0311 - mean_absolute_error: 0.0311\n",
      "Epoch 00128: val_loss improved from 0.03829 to 0.03827, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 695us/sample - loss: 0.0311 - mean_absolute_error: 0.0311 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383\n",
      "Epoch 129/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 00129: val_loss improved from 0.03827 to 0.03825, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 695us/sample - loss: 0.0316 - mean_absolute_error: 0.0316 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383\n",
      "Epoch 130/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0321 - mean_absolute_error: 0.0321\n",
      "Epoch 00130: val_loss improved from 0.03825 to 0.03822, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 709us/sample - loss: 0.0321 - mean_absolute_error: 0.0321 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382\n",
      "Epoch 131/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.0306\n",
      "Epoch 00131: val_loss improved from 0.03822 to 0.03817, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 682us/sample - loss: 0.0306 - mean_absolute_error: 0.0306 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382\n",
      "Epoch 132/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0321 - mean_absolute_error: 0.0321\n",
      "Epoch 00132: val_loss improved from 0.03817 to 0.03811, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 767us/sample - loss: 0.0321 - mean_absolute_error: 0.0321 - val_loss: 0.0381 - val_mean_absolute_error: 0.0381\n",
      "Epoch 133/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0326 - mean_absolute_error: 0.0326\n",
      "Epoch 00133: val_loss improved from 0.03811 to 0.03804, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 674us/sample - loss: 0.0326 - mean_absolute_error: 0.0326 - val_loss: 0.0380 - val_mean_absolute_error: 0.0380\n",
      "Epoch 134/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0314 - mean_absolute_error: 0.0314\n",
      "Epoch 00134: val_loss improved from 0.03804 to 0.03796, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 699us/sample - loss: 0.0314 - mean_absolute_error: 0.0314 - val_loss: 0.0380 - val_mean_absolute_error: 0.0380\n",
      "Epoch 135/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.0308\n",
      "Epoch 00135: val_loss improved from 0.03796 to 0.03789, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 670us/sample - loss: 0.0308 - mean_absolute_error: 0.0308 - val_loss: 0.0379 - val_mean_absolute_error: 0.0379\n",
      "Epoch 136/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0317 - mean_absolute_error: 0.0317\n",
      "Epoch 00136: val_loss improved from 0.03789 to 0.03783, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 671us/sample - loss: 0.0317 - mean_absolute_error: 0.0317 - val_loss: 0.0378 - val_mean_absolute_error: 0.0378\n",
      "Epoch 137/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0319 - mean_absolute_error: 0.0319\n",
      "Epoch 00137: val_loss improved from 0.03783 to 0.03778, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 720us/sample - loss: 0.0319 - mean_absolute_error: 0.0319 - val_loss: 0.0378 - val_mean_absolute_error: 0.0378\n",
      "Epoch 138/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0330 - mean_absolute_error: 0.0330\n",
      "Epoch 00138: val_loss improved from 0.03778 to 0.03774, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 724us/sample - loss: 0.0330 - mean_absolute_error: 0.0330 - val_loss: 0.0377 - val_mean_absolute_error: 0.0377\n",
      "Epoch 139/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 00139: val_loss improved from 0.03774 to 0.03769, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 730us/sample - loss: 0.0316 - mean_absolute_error: 0.0316 - val_loss: 0.0377 - val_mean_absolute_error: 0.0377\n",
      "Epoch 140/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0311 - mean_absolute_error: 0.0311\n",
      "Epoch 00140: val_loss improved from 0.03769 to 0.03765, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 725us/sample - loss: 0.0311 - mean_absolute_error: 0.0311 - val_loss: 0.0377 - val_mean_absolute_error: 0.0377\n",
      "Epoch 141/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0299 - mean_absolute_error: 0.0299\n",
      "Epoch 00141: val_loss improved from 0.03765 to 0.03763, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 731us/sample - loss: 0.0299 - mean_absolute_error: 0.0299 - val_loss: 0.0376 - val_mean_absolute_error: 0.0376\n",
      "Epoch 142/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0317 - mean_absolute_error: 0.0317\n",
      "Epoch 00142: val_loss improved from 0.03763 to 0.03758, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 741us/sample - loss: 0.0317 - mean_absolute_error: 0.0317 - val_loss: 0.0376 - val_mean_absolute_error: 0.0376\n",
      "Epoch 143/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0302 - mean_absolute_error: 0.0302\n",
      "Epoch 00143: val_loss improved from 0.03758 to 0.03755, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 759us/sample - loss: 0.0302 - mean_absolute_error: 0.0302 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 144/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0329 - mean_absolute_error: 0.0329\n",
      "Epoch 00144: val_loss improved from 0.03755 to 0.03751, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 722us/sample - loss: 0.0329 - mean_absolute_error: 0.0329 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0309 - mean_absolute_error: 0.0309\n",
      "Epoch 00145: val_loss improved from 0.03751 to 0.03747, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 931us/sample - loss: 0.0309 - mean_absolute_error: 0.0309 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 146/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0325 - mean_absolute_error: 0.0325\n",
      "Epoch 00146: val_loss improved from 0.03747 to 0.03743, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 709us/sample - loss: 0.0325 - mean_absolute_error: 0.0325 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 147/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.0306\n",
      "Epoch 00147: val_loss improved from 0.03743 to 0.03741, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 713us/sample - loss: 0.0306 - mean_absolute_error: 0.0306 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 148/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0307 - mean_absolute_error: 0.0307\n",
      "Epoch 00148: val_loss improved from 0.03741 to 0.03740, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 756us/sample - loss: 0.0307 - mean_absolute_error: 0.0307 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 149/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0303 - mean_absolute_error: 0.0303\n",
      "Epoch 00149: val_loss improved from 0.03740 to 0.03740, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 736us/sample - loss: 0.0303 - mean_absolute_error: 0.0303 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 150/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0302 - mean_absolute_error: 0.0302\n",
      "Epoch 00150: val_loss did not improve from 0.03740\n",
      "400/400 [==============================] - 0s 706us/sample - loss: 0.0302 - mean_absolute_error: 0.0302 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 151/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0310 - mean_absolute_error: 0.0310\n",
      "Epoch 00151: val_loss did not improve from 0.03740\n",
      "400/400 [==============================] - 0s 684us/sample - loss: 0.0310 - mean_absolute_error: 0.0310 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 152/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0301 - mean_absolute_error: 0.0301\n",
      "Epoch 00152: val_loss did not improve from 0.03740\n",
      "400/400 [==============================] - 0s 664us/sample - loss: 0.0301 - mean_absolute_error: 0.0301 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 153/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0302 - mean_absolute_error: 0.0302\n",
      "Epoch 00153: val_loss did not improve from 0.03740\n",
      "400/400 [==============================] - 0s 698us/sample - loss: 0.0302 - mean_absolute_error: 0.0302 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 154/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0309 - mean_absolute_error: 0.0309\n",
      "Epoch 00154: val_loss did not improve from 0.03740\n",
      "400/400 [==============================] - 0s 673us/sample - loss: 0.0309 - mean_absolute_error: 0.0309 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 155/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0312 - mean_absolute_error: 0.0312\n",
      "Epoch 00155: val_loss did not improve from 0.03740\n",
      "400/400 [==============================] - 0s 685us/sample - loss: 0.0312 - mean_absolute_error: 0.0312 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 156/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0317 - mean_absolute_error: 0.0317\n",
      "Epoch 00156: val_loss improved from 0.03740 to 0.03740, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 824us/sample - loss: 0.0317 - mean_absolute_error: 0.0317 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 157/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0302 - mean_absolute_error: 0.0302\n",
      "Epoch 00157: val_loss improved from 0.03740 to 0.03739, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 809us/sample - loss: 0.0302 - mean_absolute_error: 0.0302 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 158/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0295 - mean_absolute_error: 0.0295\n",
      "Epoch 00158: val_loss did not improve from 0.03739\n",
      "400/400 [==============================] - 0s 708us/sample - loss: 0.0295 - mean_absolute_error: 0.0295 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 159/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0303 - mean_absolute_error: 0.0303\n",
      "Epoch 00159: val_loss did not improve from 0.03739\n",
      "400/400 [==============================] - 0s 727us/sample - loss: 0.0303 - mean_absolute_error: 0.0303 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 160/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0302 - mean_absolute_error: 0.0302\n",
      "Epoch 00160: val_loss did not improve from 0.03739\n",
      "400/400 [==============================] - 0s 748us/sample - loss: 0.0302 - mean_absolute_error: 0.0302 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 161/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0322 - mean_absolute_error: 0.0322\n",
      "Epoch 00161: val_loss did not improve from 0.03739\n",
      "400/400 [==============================] - 0s 739us/sample - loss: 0.0322 - mean_absolute_error: 0.0322 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 162/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.0306\n",
      "Epoch 00162: val_loss did not improve from 0.03739\n",
      "400/400 [==============================] - 0s 704us/sample - loss: 0.0306 - mean_absolute_error: 0.0306 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 163/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 00163: val_loss did not improve from 0.03739\n",
      "400/400 [==============================] - 0s 745us/sample - loss: 0.0316 - mean_absolute_error: 0.0316 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 164/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0299 - mean_absolute_error: 0.0299\n",
      "Epoch 00164: val_loss did not improve from 0.03739\n",
      "400/400 [==============================] - 0s 755us/sample - loss: 0.0299 - mean_absolute_error: 0.0299 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 165/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0305 - mean_absolute_error: 0.0305\n",
      "Epoch 00165: val_loss did not improve from 0.03739\n",
      "400/400 [==============================] - 0s 825us/sample - loss: 0.0305 - mean_absolute_error: 0.0305 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 166/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.0308\n",
      "Epoch 00166: val_loss did not improve from 0.03739\n",
      "400/400 [==============================] - 0s 747us/sample - loss: 0.0308 - mean_absolute_error: 0.0308 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 167/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.0306\n",
      "Epoch 00167: val_loss did not improve from 0.03739\n",
      "400/400 [==============================] - 0s 748us/sample - loss: 0.0306 - mean_absolute_error: 0.0306 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 168/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0305 - mean_absolute_error: 0.0305\n",
      "Epoch 00168: val_loss did not improve from 0.03739\n",
      "400/400 [==============================] - 0s 745us/sample - loss: 0.0305 - mean_absolute_error: 0.0305 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 169/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0311 - mean_absolute_error: 0.0311\n",
      "Epoch 00169: val_loss did not improve from 0.03739\n",
      "400/400 [==============================] - 0s 809us/sample - loss: 0.0311 - mean_absolute_error: 0.0311 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 170/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - ETA: 0s - loss: 0.0296 - mean_absolute_error: 0.0296\n",
      "Epoch 00170: val_loss did not improve from 0.03739\n",
      "400/400 [==============================] - 0s 710us/sample - loss: 0.0296 - mean_absolute_error: 0.0296 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 171/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0309 - mean_absolute_error: 0.0309\n",
      "Epoch 00171: val_loss did not improve from 0.03739\n",
      "400/400 [==============================] - 0s 726us/sample - loss: 0.0309 - mean_absolute_error: 0.0309 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 172/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0305 - mean_absolute_error: 0.0305\n",
      "Epoch 00172: val_loss did not improve from 0.03739\n",
      "400/400 [==============================] - 0s 708us/sample - loss: 0.0305 - mean_absolute_error: 0.0305 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 173/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0298 - mean_absolute_error: 0.0298\n",
      "Epoch 00173: val_loss did not improve from 0.03739\n",
      "400/400 [==============================] - 0s 770us/sample - loss: 0.0298 - mean_absolute_error: 0.0298 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 174/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.0306\n",
      "Epoch 00174: val_loss improved from 0.03739 to 0.03738, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 785us/sample - loss: 0.0306 - mean_absolute_error: 0.0306 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 175/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0297 - mean_absolute_error: 0.0297\n",
      "Epoch 00175: val_loss improved from 0.03738 to 0.03735, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 797us/sample - loss: 0.0297 - mean_absolute_error: 0.0297 - val_loss: 0.0373 - val_mean_absolute_error: 0.0373\n",
      "Epoch 176/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0297 - mean_absolute_error: 0.0297\n",
      "Epoch 00176: val_loss improved from 0.03735 to 0.03731, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 781us/sample - loss: 0.0297 - mean_absolute_error: 0.0297 - val_loss: 0.0373 - val_mean_absolute_error: 0.0373\n",
      "Epoch 177/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0301 - mean_absolute_error: 0.0301\n",
      "Epoch 00177: val_loss improved from 0.03731 to 0.03728, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 829us/sample - loss: 0.0301 - mean_absolute_error: 0.0301 - val_loss: 0.0373 - val_mean_absolute_error: 0.0373\n",
      "Epoch 178/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0296 - mean_absolute_error: 0.0296\n",
      "Epoch 00178: val_loss improved from 0.03728 to 0.03726, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 805us/sample - loss: 0.0296 - mean_absolute_error: 0.0296 - val_loss: 0.0373 - val_mean_absolute_error: 0.0373\n",
      "Epoch 179/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.0308\n",
      "Epoch 00179: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 776us/sample - loss: 0.0308 - mean_absolute_error: 0.0308 - val_loss: 0.0373 - val_mean_absolute_error: 0.0373\n",
      "Epoch 180/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0299 - mean_absolute_error: 0.0299\n",
      "Epoch 00180: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 727us/sample - loss: 0.0299 - mean_absolute_error: 0.0299 - val_loss: 0.0373 - val_mean_absolute_error: 0.0373\n",
      "Epoch 181/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0302 - mean_absolute_error: 0.0302\n",
      "Epoch 00181: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 724us/sample - loss: 0.0302 - mean_absolute_error: 0.0302 - val_loss: 0.0373 - val_mean_absolute_error: 0.0373\n",
      "Epoch 182/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0299 - mean_absolute_error: 0.0299\n",
      "Epoch 00182: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 727us/sample - loss: 0.0299 - mean_absolute_error: 0.0299 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 183/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.0308\n",
      "Epoch 00183: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 701us/sample - loss: 0.0308 - mean_absolute_error: 0.0308 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 184/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0297 - mean_absolute_error: 0.0297\n",
      "Epoch 00184: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 725us/sample - loss: 0.0297 - mean_absolute_error: 0.0297 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 185/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0288\n",
      "Epoch 00185: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 698us/sample - loss: 0.0288 - mean_absolute_error: 0.0288 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 186/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0295 - mean_absolute_error: 0.0295\n",
      "Epoch 00186: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 975us/sample - loss: 0.0295 - mean_absolute_error: 0.0295 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 187/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0307 - mean_absolute_error: 0.0307\n",
      "Epoch 00187: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 740us/sample - loss: 0.0307 - mean_absolute_error: 0.0307 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 188/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0296 - mean_absolute_error: 0.0296\n",
      "Epoch 00188: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 704us/sample - loss: 0.0296 - mean_absolute_error: 0.0296 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 189/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0290 - mean_absolute_error: 0.0290\n",
      "Epoch 00189: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 717us/sample - loss: 0.0290 - mean_absolute_error: 0.0290 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 190/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0299 - mean_absolute_error: 0.0299\n",
      "Epoch 00190: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 759us/sample - loss: 0.0299 - mean_absolute_error: 0.0299 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 191/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0295 - mean_absolute_error: 0.0295\n",
      "Epoch 00191: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 705us/sample - loss: 0.0295 - mean_absolute_error: 0.0295 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 192/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0293 - mean_absolute_error: 0.0293\n",
      "Epoch 00192: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 717us/sample - loss: 0.0293 - mean_absolute_error: 0.0293 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 193/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0297 - mean_absolute_error: 0.0297\n",
      "Epoch 00193: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 760us/sample - loss: 0.0297 - mean_absolute_error: 0.0297 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 194/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0303 - mean_absolute_error: 0.0303\n",
      "Epoch 00194: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 731us/sample - loss: 0.0303 - mean_absolute_error: 0.0303 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 195/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0303 - mean_absolute_error: 0.0303\n",
      "Epoch 00195: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 898us/sample - loss: 0.0303 - mean_absolute_error: 0.0303 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 196/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0301 - mean_absolute_error: 0.0301\n",
      "Epoch 00196: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 841us/sample - loss: 0.0301 - mean_absolute_error: 0.0301 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 197/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0292 - mean_absolute_error: 0.0292\n",
      "Epoch 00197: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 784us/sample - loss: 0.0292 - mean_absolute_error: 0.0292 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 198/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0294 - mean_absolute_error: 0.0294\n",
      "Epoch 00198: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 744us/sample - loss: 0.0294 - mean_absolute_error: 0.0294 - val_loss: 0.0377 - val_mean_absolute_error: 0.0377\n",
      "Epoch 199/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0291 - mean_absolute_error: 0.0291\n",
      "Epoch 00199: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 904us/sample - loss: 0.0291 - mean_absolute_error: 0.0291 - val_loss: 0.0379 - val_mean_absolute_error: 0.0379\n",
      "Epoch 200/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0294 - mean_absolute_error: 0.0294\n",
      "Epoch 00200: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 721us/sample - loss: 0.0294 - mean_absolute_error: 0.0294 - val_loss: 0.0379 - val_mean_absolute_error: 0.0379\n",
      "Epoch 201/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0291 - mean_absolute_error: 0.0291\n",
      "Epoch 00201: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 830us/sample - loss: 0.0291 - mean_absolute_error: 0.0291 - val_loss: 0.0379 - val_mean_absolute_error: 0.0379\n",
      "Epoch 202/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0307 - mean_absolute_error: 0.0307\n",
      "Epoch 00202: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 824us/sample - loss: 0.0307 - mean_absolute_error: 0.0307 - val_loss: 0.0379 - val_mean_absolute_error: 0.0379\n",
      "Epoch 203/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0293 - mean_absolute_error: 0.0293\n",
      "Epoch 00203: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 831us/sample - loss: 0.0293 - mean_absolute_error: 0.0293 - val_loss: 0.0379 - val_mean_absolute_error: 0.0379\n",
      "Epoch 204/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0289 - mean_absolute_error: 0.0289\n",
      "Epoch 00204: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 792us/sample - loss: 0.0289 - mean_absolute_error: 0.0289 - val_loss: 0.0378 - val_mean_absolute_error: 0.0378\n",
      "Epoch 205/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0303 - mean_absolute_error: 0.0303\n",
      "Epoch 00205: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 840us/sample - loss: 0.0303 - mean_absolute_error: 0.0303 - val_loss: 0.0377 - val_mean_absolute_error: 0.0377\n",
      "Epoch 206/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0293 - mean_absolute_error: 0.0293\n",
      "Epoch 00206: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 952us/sample - loss: 0.0293 - mean_absolute_error: 0.0293 - val_loss: 0.0377 - val_mean_absolute_error: 0.0377\n",
      "Epoch 207/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0305 - mean_absolute_error: 0.0305\n",
      "Epoch 00207: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 761us/sample - loss: 0.0305 - mean_absolute_error: 0.0305 - val_loss: 0.0377 - val_mean_absolute_error: 0.0377\n",
      "Epoch 208/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0292 - mean_absolute_error: 0.0292\n",
      "Epoch 00208: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 885us/sample - loss: 0.0292 - mean_absolute_error: 0.0292 - val_loss: 0.0377 - val_mean_absolute_error: 0.0377\n",
      "Epoch 209/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0300 - mean_absolute_error: 0.0300\n",
      "Epoch 00209: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 748us/sample - loss: 0.0300 - mean_absolute_error: 0.0300 - val_loss: 0.0376 - val_mean_absolute_error: 0.0376\n",
      "Epoch 210/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0298 - mean_absolute_error: 0.0298\n",
      "Epoch 00210: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 855us/sample - loss: 0.0298 - mean_absolute_error: 0.0298 - val_loss: 0.0377 - val_mean_absolute_error: 0.0377\n",
      "Epoch 211/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0302 - mean_absolute_error: 0.0302\n",
      "Epoch 00211: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 770us/sample - loss: 0.0302 - mean_absolute_error: 0.0302 - val_loss: 0.0377 - val_mean_absolute_error: 0.0377\n",
      "Epoch 212/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0295 - mean_absolute_error: 0.0295\n",
      "Epoch 00212: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 752us/sample - loss: 0.0295 - mean_absolute_error: 0.0295 - val_loss: 0.0376 - val_mean_absolute_error: 0.0376\n",
      "Epoch 213/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0293 - mean_absolute_error: 0.0293\n",
      "Epoch 00213: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 793us/sample - loss: 0.0293 - mean_absolute_error: 0.0293 - val_loss: 0.0376 - val_mean_absolute_error: 0.0376\n",
      "Epoch 214/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0292 - mean_absolute_error: 0.0292\n",
      "Epoch 00214: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 752us/sample - loss: 0.0292 - mean_absolute_error: 0.0292 - val_loss: 0.0376 - val_mean_absolute_error: 0.0376\n",
      "Epoch 215/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0298 - mean_absolute_error: 0.0298\n",
      "Epoch 00215: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 769us/sample - loss: 0.0298 - mean_absolute_error: 0.0298 - val_loss: 0.0376 - val_mean_absolute_error: 0.0376\n",
      "Epoch 216/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0299 - mean_absolute_error: 0.0299\n",
      "Epoch 00216: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 735us/sample - loss: 0.0299 - mean_absolute_error: 0.0299 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 217/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0300 - mean_absolute_error: 0.0300\n",
      "Epoch 00217: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 764us/sample - loss: 0.0300 - mean_absolute_error: 0.0300 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 218/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0298 - mean_absolute_error: 0.0298\n",
      "Epoch 00218: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 753us/sample - loss: 0.0298 - mean_absolute_error: 0.0298 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 219/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0292 - mean_absolute_error: 0.0292\n",
      "Epoch 00219: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 702us/sample - loss: 0.0292 - mean_absolute_error: 0.0292 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 220/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0292 - mean_absolute_error: 0.0292\n",
      "Epoch 00220: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 703us/sample - loss: 0.0292 - mean_absolute_error: 0.0292 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0293 - mean_absolute_error: 0.0293\n",
      "Epoch 00221: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 782us/sample - loss: 0.0293 - mean_absolute_error: 0.0293 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 222/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0294 - mean_absolute_error: 0.0294\n",
      "Epoch 00222: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 841us/sample - loss: 0.0294 - mean_absolute_error: 0.0294 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 223/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0287\n",
      "Epoch 00223: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 751us/sample - loss: 0.0287 - mean_absolute_error: 0.0287 - val_loss: 0.0376 - val_mean_absolute_error: 0.0376\n",
      "Epoch 224/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0288\n",
      "Epoch 00224: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 733us/sample - loss: 0.0288 - mean_absolute_error: 0.0288 - val_loss: 0.0376 - val_mean_absolute_error: 0.0376\n",
      "Epoch 225/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0297 - mean_absolute_error: 0.0297\n",
      "Epoch 00225: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 767us/sample - loss: 0.0297 - mean_absolute_error: 0.0297 - val_loss: 0.0376 - val_mean_absolute_error: 0.0376\n",
      "Epoch 226/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0298 - mean_absolute_error: 0.0298\n",
      "Epoch 00226: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 741us/sample - loss: 0.0298 - mean_absolute_error: 0.0298 - val_loss: 0.0376 - val_mean_absolute_error: 0.0376\n",
      "Epoch 227/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0286 - mean_absolute_error: 0.0286\n",
      "Epoch 00227: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 772us/sample - loss: 0.0286 - mean_absolute_error: 0.0286 - val_loss: 0.0376 - val_mean_absolute_error: 0.0376\n",
      "Epoch 228/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0289 - mean_absolute_error: 0.0289\n",
      "Epoch 00228: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 709us/sample - loss: 0.0289 - mean_absolute_error: 0.0289 - val_loss: 0.0376 - val_mean_absolute_error: 0.0376\n",
      "Epoch 229/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0301 - mean_absolute_error: 0.0301\n",
      "Epoch 00229: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 719us/sample - loss: 0.0301 - mean_absolute_error: 0.0301 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 230/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0294 - mean_absolute_error: 0.0294\n",
      "Epoch 00230: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 786us/sample - loss: 0.0294 - mean_absolute_error: 0.0294 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 231/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0294 - mean_absolute_error: 0.0294\n",
      "Epoch 00231: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 735us/sample - loss: 0.0294 - mean_absolute_error: 0.0294 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 232/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.0306\n",
      "Epoch 00232: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 726us/sample - loss: 0.0306 - mean_absolute_error: 0.0306 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 233/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0287\n",
      "Epoch 00233: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 843us/sample - loss: 0.0287 - mean_absolute_error: 0.0287 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 234/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0293 - mean_absolute_error: 0.0293\n",
      "Epoch 00234: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 765us/sample - loss: 0.0293 - mean_absolute_error: 0.0293 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 235/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0296 - mean_absolute_error: 0.0296\n",
      "Epoch 00235: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 757us/sample - loss: 0.0296 - mean_absolute_error: 0.0296 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 236/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0290 - mean_absolute_error: 0.0290\n",
      "Epoch 00236: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 694us/sample - loss: 0.0290 - mean_absolute_error: 0.0290 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 237/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0302 - mean_absolute_error: 0.0302\n",
      "Epoch 00237: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 697us/sample - loss: 0.0302 - mean_absolute_error: 0.0302 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 238/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.0304\n",
      "Epoch 00238: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 692us/sample - loss: 0.0304 - mean_absolute_error: 0.0304 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 239/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0299 - mean_absolute_error: 0.0299\n",
      "Epoch 00239: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 678us/sample - loss: 0.0299 - mean_absolute_error: 0.0299 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 240/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0290 - mean_absolute_error: 0.0290\n",
      "Epoch 00240: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 674us/sample - loss: 0.0290 - mean_absolute_error: 0.0290 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 241/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0298 - mean_absolute_error: 0.0298\n",
      "Epoch 00241: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 666us/sample - loss: 0.0298 - mean_absolute_error: 0.0298 - val_loss: 0.0374 - val_mean_absolute_error: 0.0374\n",
      "Epoch 242/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0282 - mean_absolute_error: 0.0282\n",
      "Epoch 00242: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 689us/sample - loss: 0.0282 - mean_absolute_error: 0.0282 - val_loss: 0.0373 - val_mean_absolute_error: 0.0373\n",
      "Epoch 243/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0291 - mean_absolute_error: 0.0291\n",
      "Epoch 00243: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 711us/sample - loss: 0.0291 - mean_absolute_error: 0.0291 - val_loss: 0.0373 - val_mean_absolute_error: 0.0373\n",
      "Epoch 244/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0288\n",
      "Epoch 00244: val_loss did not improve from 0.03726\n",
      "400/400 [==============================] - 0s 691us/sample - loss: 0.0288 - mean_absolute_error: 0.0288 - val_loss: 0.0373 - val_mean_absolute_error: 0.0373\n",
      "Epoch 245/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0296 - mean_absolute_error: 0.0296\n",
      "Epoch 00245: val_loss improved from 0.03726 to 0.03724, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 750us/sample - loss: 0.0296 - mean_absolute_error: 0.0296 - val_loss: 0.0372 - val_mean_absolute_error: 0.0372\n",
      "Epoch 246/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0296 - mean_absolute_error: 0.0296\n",
      "Epoch 00246: val_loss improved from 0.03724 to 0.03719, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 757us/sample - loss: 0.0296 - mean_absolute_error: 0.0296 - val_loss: 0.0372 - val_mean_absolute_error: 0.0372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 247/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0300 - mean_absolute_error: 0.0300\n",
      "Epoch 00247: val_loss improved from 0.03719 to 0.03715, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 760us/sample - loss: 0.0300 - mean_absolute_error: 0.0300 - val_loss: 0.0372 - val_mean_absolute_error: 0.0372\n",
      "Epoch 248/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0303 - mean_absolute_error: 0.0303\n",
      "Epoch 00248: val_loss improved from 0.03715 to 0.03711, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 755us/sample - loss: 0.0303 - mean_absolute_error: 0.0303 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 249/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0295 - mean_absolute_error: 0.0295\n",
      "Epoch 00249: val_loss improved from 0.03711 to 0.03709, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 752us/sample - loss: 0.0295 - mean_absolute_error: 0.0295 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 250/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0307 - mean_absolute_error: 0.0307\n",
      "Epoch 00250: val_loss did not improve from 0.03709\n",
      "400/400 [==============================] - 0s 662us/sample - loss: 0.0307 - mean_absolute_error: 0.0307 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 251/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0290 - mean_absolute_error: 0.0290\n",
      "Epoch 00251: val_loss did not improve from 0.03709\n",
      "400/400 [==============================] - 0s 675us/sample - loss: 0.0290 - mean_absolute_error: 0.0290 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 252/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0292 - mean_absolute_error: 0.0292\n",
      "Epoch 00252: val_loss did not improve from 0.03709\n",
      "400/400 [==============================] - 0s 703us/sample - loss: 0.0292 - mean_absolute_error: 0.0292 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 253/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0290 - mean_absolute_error: 0.0290\n",
      "Epoch 00253: val_loss did not improve from 0.03709\n",
      "400/400 [==============================] - 0s 749us/sample - loss: 0.0290 - mean_absolute_error: 0.0290 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 254/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0284 - mean_absolute_error: 0.0284\n",
      "Epoch 00254: val_loss did not improve from 0.03709\n",
      "400/400 [==============================] - 0s 673us/sample - loss: 0.0284 - mean_absolute_error: 0.0284 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 255/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0291 - mean_absolute_error: 0.0291\n",
      "Epoch 00255: val_loss did not improve from 0.03709\n",
      "400/400 [==============================] - 0s 715us/sample - loss: 0.0291 - mean_absolute_error: 0.0291 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 256/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0285 - mean_absolute_error: 0.0285\n",
      "Epoch 00256: val_loss did not improve from 0.03709\n",
      "400/400 [==============================] - 0s 708us/sample - loss: 0.0285 - mean_absolute_error: 0.0285 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 257/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0293 - mean_absolute_error: 0.0293\n",
      "Epoch 00257: val_loss did not improve from 0.03709\n",
      "400/400 [==============================] - 0s 697us/sample - loss: 0.0293 - mean_absolute_error: 0.0293 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 258/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0287\n",
      "Epoch 00258: val_loss did not improve from 0.03709\n",
      "400/400 [==============================] - 0s 670us/sample - loss: 0.0287 - mean_absolute_error: 0.0287 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 259/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0288\n",
      "Epoch 00259: val_loss did not improve from 0.03709\n",
      "400/400 [==============================] - 0s 690us/sample - loss: 0.0288 - mean_absolute_error: 0.0288 - val_loss: 0.0372 - val_mean_absolute_error: 0.0372\n",
      "Epoch 260/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0290 - mean_absolute_error: 0.0290\n",
      "Epoch 00260: val_loss did not improve from 0.03709\n",
      "400/400 [==============================] - 0s 666us/sample - loss: 0.0290 - mean_absolute_error: 0.0290 - val_loss: 0.0372 - val_mean_absolute_error: 0.0372\n",
      "Epoch 261/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0287 - mean_absolute_error: 0.0287\n",
      "Epoch 00261: val_loss did not improve from 0.03709\n",
      "400/400 [==============================] - 0s 715us/sample - loss: 0.0287 - mean_absolute_error: 0.0287 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 262/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0290 - mean_absolute_error: 0.0290\n",
      "Epoch 00262: val_loss improved from 0.03709 to 0.03708, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 749us/sample - loss: 0.0290 - mean_absolute_error: 0.0290 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 263/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0282 - mean_absolute_error: 0.0282\n",
      "Epoch 00263: val_loss improved from 0.03708 to 0.03708, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 770us/sample - loss: 0.0282 - mean_absolute_error: 0.0282 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 264/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0289 - mean_absolute_error: 0.0289\n",
      "Epoch 00264: val_loss improved from 0.03708 to 0.03708, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 769us/sample - loss: 0.0289 - mean_absolute_error: 0.0289 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 265/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0288\n",
      "Epoch 00265: val_loss improved from 0.03708 to 0.03704, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 746us/sample - loss: 0.0288 - mean_absolute_error: 0.0288 - val_loss: 0.0370 - val_mean_absolute_error: 0.0370\n",
      "Epoch 266/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0291 - mean_absolute_error: 0.0291\n",
      "Epoch 00266: val_loss improved from 0.03704 to 0.03702, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 800us/sample - loss: 0.0291 - mean_absolute_error: 0.0291 - val_loss: 0.0370 - val_mean_absolute_error: 0.0370\n",
      "Epoch 267/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0292 - mean_absolute_error: 0.0292\n",
      "Epoch 00267: val_loss improved from 0.03702 to 0.03699, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 881us/sample - loss: 0.0292 - mean_absolute_error: 0.0292 - val_loss: 0.0370 - val_mean_absolute_error: 0.0370\n",
      "Epoch 268/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0297 - mean_absolute_error: 0.0297\n",
      "Epoch 00268: val_loss improved from 0.03699 to 0.03695, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 790us/sample - loss: 0.0297 - mean_absolute_error: 0.0297 - val_loss: 0.0370 - val_mean_absolute_error: 0.0370\n",
      "Epoch 269/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0291 - mean_absolute_error: 0.0291\n",
      "Epoch 00269: val_loss did not improve from 0.03695\n",
      "400/400 [==============================] - 0s 748us/sample - loss: 0.0291 - mean_absolute_error: 0.0291 - val_loss: 0.0370 - val_mean_absolute_error: 0.0370\n",
      "Epoch 270/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.0304\n",
      "Epoch 00270: val_loss did not improve from 0.03695\n",
      "400/400 [==============================] - 0s 725us/sample - loss: 0.0304 - mean_absolute_error: 0.0304 - val_loss: 0.0370 - val_mean_absolute_error: 0.0370\n",
      "Epoch 271/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0290 - mean_absolute_error: 0.0290\n",
      "Epoch 00271: val_loss did not improve from 0.03695\n",
      "400/400 [==============================] - 0s 706us/sample - loss: 0.0290 - mean_absolute_error: 0.0290 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 272/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0297 - mean_absolute_error: 0.0297\n",
      "Epoch 00272: val_loss did not improve from 0.03695\n",
      "400/400 [==============================] - 0s 819us/sample - loss: 0.0297 - mean_absolute_error: 0.0297 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 273/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0290 - mean_absolute_error: 0.0290\n",
      "Epoch 00273: val_loss did not improve from 0.03695\n",
      "400/400 [==============================] - 0s 717us/sample - loss: 0.0290 - mean_absolute_error: 0.0290 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 274/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0280 - mean_absolute_error: 0.0280\n",
      "Epoch 00274: val_loss did not improve from 0.03695\n",
      "400/400 [==============================] - 0s 699us/sample - loss: 0.0280 - mean_absolute_error: 0.0280 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 275/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0288\n",
      "Epoch 00275: val_loss did not improve from 0.03695\n",
      "400/400 [==============================] - 0s 689us/sample - loss: 0.0288 - mean_absolute_error: 0.0288 - val_loss: 0.0372 - val_mean_absolute_error: 0.0372\n",
      "Epoch 276/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0284 - mean_absolute_error: 0.0284\n",
      "Epoch 00276: val_loss did not improve from 0.03695\n",
      "400/400 [==============================] - 0s 698us/sample - loss: 0.0284 - mean_absolute_error: 0.0284 - val_loss: 0.0372 - val_mean_absolute_error: 0.0372\n",
      "Epoch 277/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0284 - mean_absolute_error: 0.0284\n",
      "Epoch 00277: val_loss did not improve from 0.03695\n",
      "400/400 [==============================] - 0s 699us/sample - loss: 0.0284 - mean_absolute_error: 0.0284 - val_loss: 0.0372 - val_mean_absolute_error: 0.0372\n",
      "Epoch 278/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0288\n",
      "Epoch 00278: val_loss did not improve from 0.03695\n",
      "400/400 [==============================] - 0s 696us/sample - loss: 0.0288 - mean_absolute_error: 0.0288 - val_loss: 0.0372 - val_mean_absolute_error: 0.0372\n",
      "Epoch 279/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0290 - mean_absolute_error: 0.0290\n",
      "Epoch 00279: val_loss did not improve from 0.03695\n",
      "400/400 [==============================] - 0s 703us/sample - loss: 0.0290 - mean_absolute_error: 0.0290 - val_loss: 0.0372 - val_mean_absolute_error: 0.0372\n",
      "Epoch 280/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0282 - mean_absolute_error: 0.0282\n",
      "Epoch 00280: val_loss did not improve from 0.03695\n",
      "400/400 [==============================] - 0s 713us/sample - loss: 0.0282 - mean_absolute_error: 0.0282 - val_loss: 0.0371 - val_mean_absolute_error: 0.0371\n",
      "Epoch 281/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0295 - mean_absolute_error: 0.0295\n",
      "Epoch 00281: val_loss did not improve from 0.03695\n",
      "400/400 [==============================] - 0s 755us/sample - loss: 0.0295 - mean_absolute_error: 0.0295 - val_loss: 0.0370 - val_mean_absolute_error: 0.0370\n",
      "Epoch 282/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0282 - mean_absolute_error: 0.0282\n",
      "Epoch 00282: val_loss improved from 0.03695 to 0.03691, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 797us/sample - loss: 0.0282 - mean_absolute_error: 0.0282 - val_loss: 0.0369 - val_mean_absolute_error: 0.0369\n",
      "Epoch 283/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0291 - mean_absolute_error: 0.0291\n",
      "Epoch 00283: val_loss improved from 0.03691 to 0.03687, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 777us/sample - loss: 0.0291 - mean_absolute_error: 0.0291 - val_loss: 0.0369 - val_mean_absolute_error: 0.0369\n",
      "Epoch 284/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0292 - mean_absolute_error: 0.0292\n",
      "Epoch 00284: val_loss improved from 0.03687 to 0.03685, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 814us/sample - loss: 0.0292 - mean_absolute_error: 0.0292 - val_loss: 0.0368 - val_mean_absolute_error: 0.0368\n",
      "Epoch 285/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0290 - mean_absolute_error: 0.0290\n",
      "Epoch 00285: val_loss did not improve from 0.03685\n",
      "400/400 [==============================] - 0s 743us/sample - loss: 0.0290 - mean_absolute_error: 0.0290 - val_loss: 0.0369 - val_mean_absolute_error: 0.0369\n",
      "Epoch 286/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0292 - mean_absolute_error: 0.0292\n",
      "Epoch 00286: val_loss did not improve from 0.03685\n",
      "400/400 [==============================] - 0s 798us/sample - loss: 0.0292 - mean_absolute_error: 0.0292 - val_loss: 0.0369 - val_mean_absolute_error: 0.0369\n",
      "Epoch 287/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0289 - mean_absolute_error: 0.0289\n",
      "Epoch 00287: val_loss did not improve from 0.03685\n",
      "400/400 [==============================] - 0s 844us/sample - loss: 0.0289 - mean_absolute_error: 0.0289 - val_loss: 0.0369 - val_mean_absolute_error: 0.0369\n",
      "Epoch 288/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0289 - mean_absolute_error: 0.0289\n",
      "Epoch 00288: val_loss did not improve from 0.03685\n",
      "400/400 [==============================] - 0s 730us/sample - loss: 0.0289 - mean_absolute_error: 0.0289 - val_loss: 0.0369 - val_mean_absolute_error: 0.0369\n",
      "Epoch 289/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0295 - mean_absolute_error: 0.0295\n",
      "Epoch 00289: val_loss did not improve from 0.03685\n",
      "400/400 [==============================] - 0s 768us/sample - loss: 0.0295 - mean_absolute_error: 0.0295 - val_loss: 0.0370 - val_mean_absolute_error: 0.0370\n",
      "Epoch 290/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0288 - mean_absolute_error: 0.0288\n",
      "Epoch 00290: val_loss did not improve from 0.03685\n",
      "400/400 [==============================] - 0s 753us/sample - loss: 0.0288 - mean_absolute_error: 0.0288 - val_loss: 0.0370 - val_mean_absolute_error: 0.0370\n",
      "Epoch 291/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0292 - mean_absolute_error: 0.0292\n",
      "Epoch 00291: val_loss did not improve from 0.03685\n",
      "400/400 [==============================] - 0s 782us/sample - loss: 0.0292 - mean_absolute_error: 0.0292 - val_loss: 0.0370 - val_mean_absolute_error: 0.0370\n",
      "Epoch 292/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0284 - mean_absolute_error: 0.0284\n",
      "Epoch 00292: val_loss did not improve from 0.03685\n",
      "400/400 [==============================] - 0s 752us/sample - loss: 0.0284 - mean_absolute_error: 0.0284 - val_loss: 0.0370 - val_mean_absolute_error: 0.0370\n",
      "Epoch 293/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0293 - mean_absolute_error: 0.0293\n",
      "Epoch 00293: val_loss did not improve from 0.03685\n",
      "400/400 [==============================] - 0s 686us/sample - loss: 0.0293 - mean_absolute_error: 0.0293 - val_loss: 0.0370 - val_mean_absolute_error: 0.0370\n",
      "Epoch 294/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0293 - mean_absolute_error: 0.0293\n",
      "Epoch 00294: val_loss did not improve from 0.03685\n",
      "400/400 [==============================] - 0s 691us/sample - loss: 0.0293 - mean_absolute_error: 0.0293 - val_loss: 0.0370 - val_mean_absolute_error: 0.0370\n",
      "Epoch 295/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0285 - mean_absolute_error: 0.0285\n",
      "Epoch 00295: val_loss did not improve from 0.03685\n",
      "400/400 [==============================] - 0s 697us/sample - loss: 0.0285 - mean_absolute_error: 0.0285 - val_loss: 0.0370 - val_mean_absolute_error: 0.0370\n",
      "Epoch 296/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0283 - mean_absolute_error: 0.0283\n",
      "Epoch 00296: val_loss did not improve from 0.03685\n",
      "400/400 [==============================] - 0s 760us/sample - loss: 0.0283 - mean_absolute_error: 0.0283 - val_loss: 0.0369 - val_mean_absolute_error: 0.0369\n",
      "Epoch 297/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0280 - mean_absolute_error: 0.0280\n",
      "Epoch 00297: val_loss improved from 0.03685 to 0.03682, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 794us/sample - loss: 0.0280 - mean_absolute_error: 0.0280 - val_loss: 0.0368 - val_mean_absolute_error: 0.0368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0281 - mean_absolute_error: 0.0281\n",
      "Epoch 00298: val_loss improved from 0.03682 to 0.03677, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 861us/sample - loss: 0.0281 - mean_absolute_error: 0.0281 - val_loss: 0.0368 - val_mean_absolute_error: 0.0368\n",
      "Epoch 299/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0280 - mean_absolute_error: 0.0280\n",
      "Epoch 00299: val_loss improved from 0.03677 to 0.03673, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 792us/sample - loss: 0.0280 - mean_absolute_error: 0.0280 - val_loss: 0.0367 - val_mean_absolute_error: 0.0367\n",
      "Epoch 300/300\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0291 - mean_absolute_error: 0.0291\n",
      "Epoch 00300: val_loss improved from 0.03673 to 0.03669, saving model to ./mdc_net.h5\n",
      "400/400 [==============================] - 0s 823us/sample - loss: 0.0291 - mean_absolute_error: 0.0291 - val_loss: 0.0367 - val_mean_absolute_error: 0.0367\n"
     ]
    }
   ],
   "source": [
    "print(\"Train LSTM-based DNN Performance Drop Predictor...\")\n",
    "\n",
    "# fit the model\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "epochs=300\n",
    "batch_size=400\n",
    "validation_split=0.2\n",
    "verbose=True\n",
    "early_stop_patience=300\n",
    "lr=0.001\n",
    "    \n",
    "np.random.seed(random_state)\n",
    "set_random_seed(random_state)\n",
    "\n",
    "if os.path.exists('./mdc_net.h5'):\n",
    "    os.remove('./mdc_net.h5')\n",
    "        \n",
    "drop_predictor.compile(loss='mean_absolute_error',\n",
    "              optimizer=Adam(lr),\n",
    "              metrics=['mean_absolute_error'])\n",
    "    \n",
    "callbacks = [ModelCheckpoint(filepath='./mdc_net.h5', monitor='val_loss', verbose=verbose, \n",
    "                             save_best_only=True, mode='min'),\n",
    "             EarlyStopping(monitor='val_loss', mode='min', verbose=verbose, \n",
    "                           patience=early_stop_patience)]\n",
    "\n",
    "history = drop_predictor.fit(\n",
    "    X_train_1, \n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=validation_split,\n",
    "    callbacks=callbacks,\n",
    "    verbose=verbose\n",
    ")\n",
    "\n",
    "drop_predictor = load_model('./mdc_net.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f89a08844d0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure(figsize=(8, 8))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "\n",
    "plt.plot(history.history['mean_absolute_error'], label=\"Training Drop Error\")\n",
    "plt.plot(history.history['val_mean_absolute_error'], label=\"Validation Drop Error\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate LSTM-based DNN and save results...\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate LSTM-based DNN and save results...\")\n",
    "\n",
    "y_train_drops_pred = drop_predictor.predict(X_train_1)\n",
    "\n",
    "y_test_drops_pred = drop_predictor.predict(X_test_1)\n",
    "\n",
    "y_test_unseen_drops_pred = drop_predictor.predict(X_test_unseen_1)\n",
    "\n",
    "y_test_natural_drops_pred = drop_predictor.predict(X_test_natural_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028964305240100154"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_train, y_train_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022716608752984757"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_test_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.052004002735010005"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test_unseen, y_test_unseen_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06264404282091479"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test_natural, y_test_natural_drops_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "createdOn": 1620650668671,
  "creator": "smaggio",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "ker_dnn_performance_drop_prediction",
   "language": "python",
   "name": "ker_dnn_performance_drop_prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "modifiedBy": "smaggio",
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
